<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<meta name="theme-color" content="#222">


  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "258f1ebb"
    });
  daovoice('update');
  </script>









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP,Pytorch,">










<meta name="description" content="由Yif翻译，仅供学习严禁任何商业用途 Preface本书旨在为新人提供自然语言处理（NLP）和深度学习，以涵盖这两个领域的重要主题。这两个主题领域都呈指数级增长。对于一本介绍深度学习和强调实施的NLP的书，本书占据了重要的中间地带。在写这本书时，我们不得不对哪些材料遗漏做出艰难的，有时甚至是不舒服的选择。对于初学者，我们希望本书能够为基础知识提供强有力的基础，并可以瞥见可能的内容。特别是机器学习">
<meta name="keywords" content="NLP,Pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="Natural-Language-Processing-with-PyTorch（一）">
<meta property="og:url" content="http://yifdu.github.io/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/index.html">
<meta property="og:site_name" content="深度智障">
<meta property="og:description" content="由Yif翻译，仅供学习严禁任何商业用途 Preface本书旨在为新人提供自然语言处理（NLP）和深度学习，以涵盖这两个领域的重要主题。这两个主题领域都呈指数级增长。对于一本介绍深度学习和强调实施的NLP的书，本书占据了重要的中间地带。在写这本书时，我们不得不对哪些材料遗漏做出艰难的，有时甚至是不舒服的选择。对于初学者，我们希望本书能够为基础知识提供强有力的基础，并可以瞥见可能的内容。特别是机器学习">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1545023935255&di=c2469985994fb00217b96200714f5f27&imgtype=0&src=http%3A%2F%2Fimage.tupian114.com%2F20120504%2F2012050420332255.jpg">
<meta property="og:updated_time" content="2019-02-16T13:54:22.732Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Natural-Language-Processing-with-PyTorch（一）">
<meta name="twitter:description" content="由Yif翻译，仅供学习严禁任何商业用途 Preface本书旨在为新人提供自然语言处理（NLP）和深度学习，以涵盖这两个领域的重要主题。这两个主题领域都呈指数级增长。对于一本介绍深度学习和强调实施的NLP的书，本书占据了重要的中间地带。在写这本书时，我们不得不对哪些材料遗漏做出艰难的，有时甚至是不舒服的选择。对于初学者，我们希望本书能够为基础知识提供强有力的基础，并可以瞥见可能的内容。特别是机器学习">
<meta name="twitter:image" content="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1545023935255&di=c2469985994fb00217b96200714f5f27&imgtype=0&src=http%3A%2F%2Fimage.tupian114.com%2F20120504%2F2012050420332255.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yifdu.github.io/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/">





  <title>Natural-Language-Processing-with-PyTorch（一） | 深度智障</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">深度智障</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yifdu.github.io/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yif Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/xuanyi.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深度智障">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Natural-Language-Processing-with-PyTorch（一）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-17T09:54:38+08:00">
                2018-12-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Pytorch/" itemprop="url" rel="index">
                    <span itemprop="name">Pytorch</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Pytorch/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读数
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9.3k 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  37 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
        <div class="post-gallery" itemscope="" itemtype="http://schema.org/ImageGallery">
          
          
            <div class="post-gallery-row">
              <a class="post-gallery-img fancybox" href="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1545023935255&di=c2469985994fb00217b96200714f5f27&imgtype=0&src=http%3A%2F%2Fimage.tupian114.com%2F20120504%2F2012050420332255.jpg" rel="gallery_cjse8ihp100c0rswax0h81eae" itemscope="" itemtype="http://schema.org/ImageObject" itemprop="url">
                <img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1545023935255&di=c2469985994fb00217b96200714f5f27&imgtype=0&src=http%3A%2F%2Fimage.tupian114.com%2F20120504%2F2012050420332255.jpg" itemprop="contentUrl">
              </a>
            
          

          
          </div>
        </div>
      

      
        <p><strong>由Yif翻译，仅供学习严禁任何商业用途</strong></p>
<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>本书旨在为新人提供自然语言处理（NLP）和深度学习，以涵盖这两个领域的重要主题。这两个主题领域都呈指数级增长。对于一本介绍深度学习和强调实施的NLP的书，本书占据了重要的中间地带。在写这本书时，我们不得不对哪些材料遗漏做出艰难的，有时甚至是不舒服的选择。对于初学者，我们希望本书能够为基础知识提供强有力的基础，并可以瞥见可能的内容。特别是机器学习和深度学习是一种经验学科，而不是智力科学。我们希望每章中慷慨的端到端代码示例邀请您参与这一经历。当我们开始编写本书时，我们从PyTorch 0.2开始。每个PyTorch更新从0.2到0.4修改了示例。 PyTorch 1.0将于本书出版时发布。本书中的代码示例符合PyTorch 0.4，它应该与即将发布的PyTorch 1.0版本一样工作.1关于本书风格的注释。我们在大多数地方都故意避免使用数学;并不是因为深度学习数学特别困难（事实并非如此），而是因为它在许多情况下分散了本书主要目标的注意力——增强初学者的能力。在许多情况下，无论是在代码还是文本方面，我们都有类似的动机，我们倾向于对简洁性进行阐述。高级读者和有经验的程序员可以找到方法来收紧代码等等，但我们的选择是尽可能明确，以便覆盖我们想要达到的大多数受众。</p>
<h1 id="Chapter-1-Introduction"><a href="#Chapter-1-Introduction" class="headerlink" title="Chapter 1.Introduction"></a>Chapter 1.Introduction</h1><p>像Echo (Alexa)、Siri和谷歌Translate这样的家喻户晓的产品名称至少有一个共同点。它们都是自然语言处理(NLP)应用的产物，NLP是本书的两个主要主题之一。NLP是一套运用统计方法的技术，无论是否有语言学的洞见，为了解决现实世界的任务而理解文本。这种对文本的“理解”主要是通过将文本转换为可用的计算表示，这些计算表示是离散或连续的组合结构，如向量或张量、图形和树。</p>
<p>从数据(本例中为文本)中学习适合于任务的表示形式是机器学习的主题。应用机器学习的文本数据有超过三十年的历史,但最近(2008年至2010年开始)$^1$一组机器学习技术,被称为深度学习,继续发展和证明非常有效的各种人工智能(AI)在NLP中的任务,演讲,和计算机视觉。深度学习是我们要讲的另一个主题;因此，本书是关于NLP和深度学习的研究。</p>
<p>简单地说，深度学习使人们能够使用一种称为计算图和数字优化技术的抽象概念有效地从数据中学习表示。这就是深度学习和计算图的成功之处，像谷歌、Facebook和Amazon这样的大型技术公司已经发布了基于它们的计算图形框架和库的实现，以捕捉研究人员和工程师的思维。在本书中，我们考虑PyTorch，一个越来越流行的基于python的计算图框架库来实现深度学习算法。在本章中，我们将解释什么是计算图，以及我们选择使用PyTorch作为框架。机器学习和深度学习的领域是广阔的。在这一章，在本书的大部分时间里，我们主要考虑的是所谓的监督学习;也就是说，使用标记的训练示例进行学习。我们解释了监督学习范式，这将成为本书的基础。如果到目前为止您还不熟悉其中的许多术语，那么您是对的。这一章，以及未来的章节，不仅澄清了这一点，而且深入研究了它们。如果您已经熟悉这里提到的一些术语和概念，我们仍然鼓励您遵循以下两个原因:为本书其余部分建立一个共享的词汇表，以及填补理解未来章节所需的任何空白。</p>
<p>本章的目标是:</p>
<ul>
<li>发展对监督学习范式的清晰理解，理解术语，并发展一个概念框架来处理未来章节的学习任务</li>
<li>学习如何为学习任务的输入编码</li>
<li>理解什么是计算图</li>
<li>掌握PyTorch的基本知识</li>
</ul>
<p>让我们开始吧！</p>
<h2 id="The-Supervised-Learning-Paradigm"><a href="#The-Supervised-Learning-Paradigm" class="headerlink" title="The Supervised Learning Paradigm"></a>The Supervised Learning Paradigm</h2><p>机器学习中的监督，或者简单的监督学习，是指将目标(被预测的内容)的ground truth用于观察(输入)的情况。例如，在文档分类中，目标是一个分类标签，观察(输入)是一个文档。例如，在机器翻译中，观察(输入)是一种语言的句子，目标是另一种语言的句子。通过对输入数据的理解，我们在图1-1中演示了监督学习范式。<br><img src="/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/nlpp_0101.png" alt="nlpp_0101" title="图1-1 监督学习范式，一种从标记输入数据中学习的概念框架。"></p>
<p>我们可以将监督学习范式分解为六个主要概念，如图1-1所示:<br>Observations：<br>观察是我们想要预测的东西。我们用x表示观察值。我们有时把观察值称为“输入”。<br>Targets:<br>目标是与观察相对应的标签。它通常是被预言的事情。按照机器学习/深度学习中的标准符号，我们用y表示这些。有时，这被称为ground truth 。<br>Model:<br>模型是一个数学表达式或函数，它接受一个观察值x，并预测其目标标签的值。<br>Parameters:<br>有时也称为权重，这些参数化模型。标准使用的符号w(权重)或ŵ。<br>Predictions:<br>预测，也称为估计，是模型在给定观测值的情况下所猜测目标的值。我们用一个“hat”表示这些。所以,目标y的预测用ŷ来表示。<br>Loss function:<br>损失函数是比较预测与训练数据中观测目标之间的距离的函数。给定一个目标及其预测，损失函数将分配一个称为损失的标量实值。损失值越低，模型对目标的预测效果越好。我们用L表示损失函数。</p>
<p>虽然在NLP /深度学习建模或编写本书时，这在数学上并不是正式有效，但我们将正式重述监督学习范例，以便为该领域的新读者提供标准术语，以便他们拥有熟悉arXiv研究论文中的符号和写作风格。</p>
<p>考虑一个数据集D=$\{X_i,y_i\}_{i=1}^n$，有n个例子。给定这个数据集，我们想要学习一个由权值w参数化的函数(模型)f，也就是说，我们对f的结构做一个假设，给定这个结构，权值w的学习值将充分表征模型。对于一个给定的输入X,模型预测ŷ作为目标:<br>ŷ =f(X;W)<br>在监督学习中，对于训练例子，我们知道观察的真正目标y。这个实例的损失将为L(y,ŷ)。然后，监督学习就变成了一个寻找最优参数/权值w的过程，从而使所有n个例子的累积损失最小化。</p>
<hr>
<p>利用(随机)梯度下降法进行训练<br>监督学习的目标是为给定的数据集选择参数值，使损失函数最小化。换句话说，这等价于在方程中求根。我们知道梯度下降法是一种常见的求方程根的方法。回忆一下，在传统的梯度下降法中，我们对根(参数)的一些初值进行猜测，并迭代更新这些参数，直到目标函数(损失函数)的计算值低于可接受阈值(即收敛准则)。对于大型数据集，由于内存限制，在整个数据集上实现传统的梯度下降通常是不可能的，而且由于计算开销，速度非常慢。相反，通常采用一种近似的梯度下降称为随机梯度下降(SGD)。在随机情况下，数据点或数据点的子集是随机选择的，并计算该子集的梯度。当使用单个数据点时，这种方法称为纯SGD，当使用(多个)数据点的子集时，我们将其称为小型批处理SGD。通常情况下，“纯”和“小型批处理”这两个词在根据上下文变得清晰时就会被删除。在实际应用中，很少使用纯SGD，因为它会由于有噪声的更新而导致非常慢的收敛。一般SGD算法有不同的变体，都是为了更快的收敛。在后面的章节中，我们将探讨这些变体中的一些，以及如何使用渐变来更新参数。这种迭代更新参数的过程称为反向传播。反向传播的每个步骤(又名epoch)由向前传递和向后传递组成。向前传递用参数的当前值计算输入并计算损失函数。反向传递使用损失梯度更新参数。</p>
<hr>
<p>请注意，到目前为止，这里没有什么是专门针对深度学习或神经网络的。图1-1中箭头的方向表示训练系统时数据的“流”。关于训练和“计算图”中“流”的概念，我们还有更多要说的，但首先，让我们看看如何用数字表示NLP问题中的输入和目标，这样我们就可以训练模型并预测结果。</p>
<h2 id="Observation-and-Target-Encoding"><a href="#Observation-and-Target-Encoding" class="headerlink" title="Observation and Target Encoding"></a>Observation and Target Encoding</h2><p>我们需要用数字表示观测值(文本)，以便与机器学习算法一起使用。图1-2给出了一个可视化的描述。<br><img src="/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/nlpp_0102.png" alt="nlpp_0102" title="图1-2 观察和目标编码:注意图1-1中的目标和观察是如何用向量或张量表示的。这被统称为输入“编码”。"></p>
<p>表示文本的一种简单方法是用数字向量表示。有无数种方法可以执行这种映射/表示。事实上，本书的大部分内容都致力于从数据中学习此类任务表示。然而，我们从基于启发式的一些简单的基于计数的表示开始。虽然简单，但是它们非常强大，或者可以作为更丰富的表示学习的起点。所有这些基于计数的表示都是从一个固定维数的向量开始的。</p>
<h3 id="One-Hot-Representation"><a href="#One-Hot-Representation" class="headerlink" title="One-Hot Representation"></a>One-Hot Representation</h3><p>顾名思义，one-hot表示从一个零向量开始，如果单词出现在句子或文档中，则将向量中的相应条目设置为1。考虑下面两句话。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Time flies like an arrow.</span><br><span class="line">Fruit flies like a banana.</span><br></pre></td></tr></table></figure></p>
<p>对句子进行标记，忽略标点符号，并将所有的单词都用小写字母表示，就会得到一个大小为8的词汇表:{time, fruit, flies, like, a, an, arrow, banana}。所以，我们可以用一个八维的one-hot向量来表示每个单词。在本书中，我们使用$1_w$表示令牌/单词w的one-hot表示。</p>
<p>对于短语、句子或文档，压缩的one-hot表示仅仅是其组成词的逻辑或的one-hot表示。使用图1-3所示的编码，短语“like a banana”的one-hot表示将是一个3×8矩阵，其中的列是8维的one-hot向量。通常还会看到“折叠”或二进制编码，其中文本/短语由词汇表长度的向量表示，用0和1表示单词的缺失或存在。“like a banana”的二进制编码是:[0,0,0,1,1,0,0,1]。</p>
<p><img src="/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/nlpp_0103.png" alt="nlpp_0103" title="图1-3 “Time flies like an arrow”和“Fruit flies like a banana.”这两个句子的one-hot表示形式。"></p>
<hr>
<p>NOTE<br>在这一点上，如果你觉得我们把“flies”的两种不同的意思(或感觉)搞混了，恭喜你，聪明的读者!语言充满了歧义，但是我们仍然可以通过极其简化的假设来构建有用的解决方案。学习特定于意义的表示是可能的，但是我们现在做得有些超前了。</p>
<hr>
<p>尽管对于本书中的输入，我们很少使用除了one-hot表示之外的其他表示，但是由于NLP中受欢迎、历史原因和完成目的，我们现在介绍术语频率(TF)和术语频率反转文档频率(TF-idf)表示。这些表示在信息检索(IR)中有着悠久的历史，甚至在今天的生产NLP系统中也得到了广泛的应用。(翻译有不足)</p>
<h3 id="TF-Representation"><a href="#TF-Representation" class="headerlink" title="TF Representation"></a>TF Representation</h3><p>短语、句子或文档的TF表示仅仅是构成词的one-hot的总和。为了继续我们愚蠢的示例，使用前面提到的one-hot编码，“Fruit flies like time flies a fruit”这句话具有以下TF表示:[1,2,2,1,1,1,0,0]。注意，每个条目是句子(语料库)中出现相应单词的次数的计数。我们用TF(w)表示一个单词的TF。</p>
<p>Example 1-1. Generating a “collapsed” one-hot or binary representation using scikit-learn<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">corpus = [<span class="string">'Time flies flies like an arrow.'</span>,</span><br><span class="line">          <span class="string">'Fruit flies like a banana.'</span>]</span><br><span class="line">one_hot_vectorizer = CountVectorizer(binary=<span class="keyword">True</span>)</span><br><span class="line">one_hot = one_hot_vectorizer.fit_transform(corpus).toarray()</span><br><span class="line">sns.heatmap(one_hot, annot=<span class="keyword">True</span>,</span><br><span class="line">            cbar=<span class="keyword">False</span>, xticklabels=vocab,</span><br><span class="line">            yticklabels=[<span class="string">'Sentence 1'</span>, <span class="string">'Sentence 2'</span>])</span><br></pre></td></tr></table></figure></p>
<p><img src="/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/nlpp_0104.png" alt="nlpp_0104" title="图1-4 由示例1-1生成的折叠one-hot表示。"></p>
<p>折叠的onehot是一个向量中有多个1的onehot</p>
<h3 id="TF-IDF-Representation"><a href="#TF-IDF-Representation" class="headerlink" title="TF-IDF Representation"></a>TF-IDF Representation</h3><p>考虑一组专利文件。您可能希望它们中的大多数都有诸如claim、system、method、procedure等单词，并且经常重复多次。TF表示对更频繁的单词进行加权。然而，像“claim”这样的常用词并不能增加我们对具体专利的理解。相反，如果“tetrafluoroethylene”这样罕见的词出现的频率较低，但很可能表明专利文件的性质，我们希望在我们的表述中赋予它更大的权重。反文档频率(IDF)是一种启发式算法，可以精确地做到这一点。</p>
<p><strong>IDF表示惩罚常见的符号，并奖励向量表示中的罕见符号。</strong> 符号w的IDF(w)对语料库的定义为</p>
<script type="math/tex; mode=display">IDF(w)=log\frac{N}{n_w}</script><p>其中$n_w$是包含单词w的文档数量，N是文档总数。TF-IDF分数就是TF(w) * IDF(w)的乘积。首先，请注意在所有文档(例如， $n_w$ = N)， IDF(w)为0,TF-IDF得分为0，完全惩罚了这一项。其次，如果一个术语很少出现(可能只出现在一个文档中)，那么IDF就是log n的最大值。</p>
<p>Example 1-2. Generating TF-IDF representation using scikit-learn<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">tfidf_vectorizer = TfidfVectorizer()</span><br><span class="line">tfidf = tfidf_vectorizer.fit_transform(corpus).toarray()</span><br><span class="line">sns.heatmap(tfidf, annot=<span class="keyword">True</span>, cbar=<span class="keyword">False</span>, xticklabels=vocab,</span><br><span class="line">            yticklabels= [<span class="string">'Sentence 1'</span>, <span class="string">'Sentence 2'</span>])</span><br></pre></td></tr></table></figure></p>
<p><img src="/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/nlpp_01_tfidf.png" alt="nlpp_01_tfidf" title="图 1-5"><br>在深度学习中，很少看到使用像TF-IDF这样的启发式表示对输入进行编码，因为目标是学习一种表示。通常，我们从一个使用整数索引的one-hot编码和一个特殊的“embedding lookup”层开始构建神经网络的输入。在后面的章节中，我们将给出几个这样做的例子。</p>
<h3 id="Target-Encoding"><a href="#Target-Encoding" class="headerlink" title="Target Encoding"></a>Target Encoding</h3><p>正如“监督学习范式”所指出的，目标变量的确切性质取决于所解决的NLP任务。例如，在机器翻译、摘要和回答问题的情况下，目标也是文本，并使用前面描述的one-hot编码方法进行编码。</p>
<p>许多NLP任务实际上使用分类标签，其中模型必须预测一组固定标签中的一个。对此进行编码的一种常见方法是对每个标签使用惟一索引。当输出标签的数量太大时，这种简单的表示可能会出现问题。这方面的一个例子是语言建模问题，在这个问题中，任务是预测下一个单词，给定过去看到的单词。标签空间是一种语言的全部词汇，它可以很容易地增长到几十万，包括特殊字符、名称等等。我们将在后面的章节中重新讨论这个问题以及如何解决这个问题。</p>
<p>一些NLP问题涉及从给定文本中预测一个数值。例如，给定一篇英语文章，我们可能需要分配一个数字评分或可读性评分。给定一个餐馆评论片段，我们可能需要预测直到小数点后第一位的星级。给定用户的推文，我们可能需要预测用户的年龄群。有几种方法可以对数字目标进行编码，但是将目标简单地绑定到分类“容器”中(例如，“0-18”、“19-25”、“25-30”等等)，并将其视为有序分类问题是一种合理的方法。 binning可以是均匀的，也可以是非均匀的，数据驱动的。虽然关于这一点的详细讨论超出了本书的范围，但是我们提请您注意这些问题，因为在这种情况下，目标编码会显著影响性能，我们鼓励您参阅Dougherty等人(1995)及其引用。</p>
<h2 id="Computational-Graphs"><a href="#Computational-Graphs" class="headerlink" title="Computational Graphs"></a>Computational Graphs</h2><p>图1-1将监督学习(训练)范式概括为数据流架构，模型(数学表达式)对输入进行转换以获得预测，损失函数(另一个表达式)提供反馈信号来调整模型的参数。利用计算图数据结构可以方便地实现该数据流。从技术上讲，计算图是对数学表达式建模的抽象。在深度学习的上下文中，计算图的实现(如Theano、TensorFlow和PyTorch)进行了额外的记录(bookkeeping)，以实现在监督学习范式中训练期间获取参数梯度所需的自动微分。我们将在“PyTorch基础知识”中进一步探讨这一点。推理(或预测)就是简单的表达式求值(计算图上的正向流)。让我们看看计算图如何建模表达式。考虑表达式:y=wx+b</p>
<p>这可以写成两个子表达式z = wx和y = z + b，然后我们可以用一个有向无环图(DAG)表示原始表达式，其中的节点是乘法和加法等数学运算。操作的输入是节点的传入边，操作的输出是传出边。因此，对于表达式y = wx + b，计算图如图1-6所示。在下一节中，我们将看到PyTorch如何让我们以一种直观的方式创建计算图形，以及它如何让我们计算梯度，而无需考虑任何记录(bookkeeping)。</p>
<p><img src="/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/nlpp_0105.png" alt="nlpp_0105" title="图1-6 用计算图表示y = wx + b。"></p>
<h2 id="PyTorch-Basics"><a href="#PyTorch-Basics" class="headerlink" title="PyTorch Basics"></a>PyTorch Basics</h2><p>在本书中，我们广泛地使用PyTorch来实现我们的深度学习模型。PyTorch是一个开源、社区驱动的深度学习框架。与Theano、Caffe和TensorFlow不同，PyTorch实现了一种“tape-based automatic differentiation”方法，允许我们动态定义和执行计算图形。这对于调试和用最少的努力构建复杂的模型非常有帮助。</p>
<hr>
<p>动态 VS 静态计算图<br>像Theano、Caffe和TensorFlow这样的静态框架需要首先声明、编译和执行计算图。虽然这会导致非常高效的实现(在生产和移动设置中非常有用)，但在研究和开发过程中可能会变得非常麻烦。像Chainer、DyNet和PyTorch这样的现代框架实现了动态计算图，从而支持更灵活的命令式开发风格，而不需要在每次执行之前编译模型。动态计算图在建模NLP任务时特别有用，每个输入可能导致不同的图结构。</p>
<hr>
<p>PyTorch是一个优化的张量操作库，它提供了一系列用于深度学习的包。这个库的核心是张量，它是一个包含一些多维数据的数学对象。0阶张量就是一个数字，或者标量。一阶张量(一阶张量)是一个数字数组，或者说是一个向量。类似地，二阶张量是一个向量数组，或者说是一个矩阵。因此，张量可以推广为标量的n维数组，如图1-7所示</p>
<p><strong>图1-7 还未给出</strong></p>
<p>在以下部分中，我们将使用PyTorch学习以下内容:</p>
<ul>
<li>创建张量</li>
<li>操作与张量</li>
<li>索引、切片和与张量连接</li>
<li>用张量计算梯度</li>
<li>使用带有gpu的CUDA张量</li>
</ul>
<p>在本节的其余部分中，我们将首先使用PyTorch来熟悉各种PyTorch操作。我们建议您现在已经安装了PyTorch并准备好了Python 3.5+笔记本，并按照本节中的示例进行操作。我们还建议您完成本节后面的练习。</p>
<h3 id="Installing-PyTorch"><a href="#Installing-PyTorch" class="headerlink" title="Installing PyTorch"></a>Installing PyTorch</h3><p>第一步是通过在pytorch.org上选择您的系统首选项在您的机器上安装PyTorch。选择您的操作系统，然后选择包管理器(我们推荐conda/pip)，然后选择您正在使用的Python版本(我们推荐3.5+)。这将生成命令，以便您执行安装PyTorch。在撰写本文时，conda环境的安装命令如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision -c pytorch</span><br></pre></td></tr></table></figure></p>
<hr>
<p>NOTE<br>如果您有一个支持CUDA的图形处理器单元(GPU)，您还应该选择合适的CUDA版本。要了解更多细节，请参考pytorch.org上的安装说明。</p>
<hr>
<h3 id="Creating-Tensors"><a href="#Creating-Tensors" class="headerlink" title="Creating Tensors"></a>Creating Tensors</h3><p>首先，我们定义一个辅助函数，描述(x)，它总结了张量x的各种性质，例如张量的类型、张量的维数和张量的内容:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe</span><span class="params">(x)</span>:</span></span><br><span class="line">  print(<span class="string">"Type: &#123;&#125;"</span>.format(x.type()))</span><br><span class="line">  print(<span class="string">"Shape/size: &#123;&#125;"</span>.format(x.shape))</span><br><span class="line">  print(<span class="string">"Values: \n&#123;&#125;"</span>.format(x))</span><br></pre></td></tr></table></figure></p>
<p>PyTorch允许我们使用torch包以许多不同的方式创建张量。创建张量的一种方法是通过指定一个随机张量的维数来初始化它，如例1-3所示。</p>
<p>Example 1-3. Creating a tensor in PyTorch with torch.Tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">describe(torch.Tensor(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">3.2018e-05</span>,  <span class="number">4.5747e-41</span>,  <span class="number">2.5058e+25</span>],</span><br><span class="line">        [ <span class="number">3.0813e-41</span>,  <span class="number">4.4842e-44</span>,  <span class="number">0.0000e+00</span>]])</span><br></pre></td></tr></table></figure></p>
<p>我们还可以创建一个张量通过随机初始化值区间上的均匀分布(0,1)或标准正态分布(从均匀分布随机初始化张量,说,是很重要的,正如您将看到的在第三章和第四章),见示例1-4。<br>Example 1-4. Creating a randomly initialized tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]：</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">describe(torch.rand(<span class="number">2</span>, <span class="number">3</span>))   <span class="comment"># uniform random</span></span><br><span class="line">describe(torch.randn(<span class="number">2</span>, <span class="number">3</span>))  <span class="comment"># random normal</span></span><br><span class="line">Output[<span class="number">0</span>]：</span><br><span class="line">Type:  torch.FloatTensor</span><br><span class="line">Shape/size:  torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line"> tensor([[ <span class="number">0.0242</span>,  <span class="number">0.6630</span>,  <span class="number">0.9787</span>],</span><br><span class="line">        [ <span class="number">0.1037</span>,  <span class="number">0.3920</span>,  <span class="number">0.6084</span>]])</span><br><span class="line"></span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[<span class="number">-0.1330</span>, <span class="number">-2.9222</span>, <span class="number">-1.3649</span>],</span><br><span class="line">        [ <span class="number">2.3648</span>,  <span class="number">1.1561</span>,  <span class="number">1.5042</span>]])</span><br></pre></td></tr></table></figure></p>
<p>我们还可以创建张量，所有张量都用相同的标量填充。对于创建0或1张量，我们有内置函数，对于填充特定值，我们可以使用fill_()方法。任何带有下划线( _ )的PyTorch方法都是指就地(in place)操作;也就是说，它在不创建新对象的情况下就地修改内容，如示例1-5所示。</p>
<p>Example 1-5. Creating a filled tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">describe(torch.zeros(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">describe(x)</span><br><span class="line">x.fill_(<span class="number">5</span>)</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">5.</span>,  <span class="number">5.</span>,  <span class="number">5.</span>],</span><br><span class="line">        [ <span class="number">5.</span>,  <span class="number">5.</span>,  <span class="number">5.</span>]])</span><br></pre></td></tr></table></figure></p>
<p>示例1-6演示了如何通过使用Python列表以声明的方式创建张量。<br>Example 1-6. Creating and initializing a tensor from lists<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line">x = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],  </span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure></p>
<p>值可以来自列表(如前面的示例)，也可以来自NumPy数组。当然，我们也可以从PyTorch张量变换到NumPy数组。注意，这个张量的类型是一个double张量，而不是默认的FloatTensor。这对应于NumPy随机矩阵的数据类型float64，如示例1-7所示。</p>
<p>Example 1-7. Creating and initializing a tensor from NumPy<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">npy = np.random.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">describe(torch.from_numpy(npy))</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.DoubleTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.8360</span>,  <span class="number">0.8836</span>,  <span class="number">0.0545</span>],</span><br><span class="line">        [ <span class="number">0.6928</span>,  <span class="number">0.2333</span>,  <span class="number">0.7984</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<p>在处理使用Numpy格式数值的遗留库(legacy libraries)时，在NumPy和PyTorch张量之间切换的能力变得非常重要。</p>
<h3 id="Tensor-Types-and-Size"><a href="#Tensor-Types-and-Size" class="headerlink" title="Tensor Types and Size"></a>Tensor Types and Size</h3><p>每个张量都有一个相关的类型和大小。使用torch时的默认张量类型。张量构造函数是torch.FloatTensor。但是，可以在初始化时指定张量，也可以在以后使用类型转换方法将张量转换为另一种类型(float、long、double等)。有两种方法可以指定初始化类型，一种是直接调用特定张量类型(如FloatTensor和LongTensor)的构造函数，另一种是使用特殊的方法torch。张量，并提供dtype，如例1-8所示。</p>
<p>Example 1-8. Tensor properties<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],  </span><br><span class="line">                       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>]])</span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line">x = x.long()</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">Type: torch.LongTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>]])</span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.int64)</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.LongTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>]])</span><br><span class="line">Input[<span class="number">3</span>]:</span><br><span class="line">x = x.float()</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">3</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure></p>
<p>我们利用张量对象的形状特性和尺寸方法来获取其尺寸的测量值。访问这些度量的两种方法基本上是相同的。在调试PyTorch代码时，检查张量的形状成为必不可少的工具。</p>
<h3 id="Tensor-Operations"><a href="#Tensor-Operations" class="headerlink" title="Tensor Operations"></a>Tensor Operations</h3><p>在创建了张量之后，可以像处理传统编程语言类型(如“+”、“-”、“* ”和“/”)那样对它们进行操作。除了操作符，我们还可以使用.add()之类的函数，如示例1-9所示，这些函数对应于符号操作符。</p>
<p>Example 1-9. Tensor operations: addition<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.0461</span>,  <span class="number">0.4024</span>, <span class="number">-1.0115</span>],</span><br><span class="line">        [ <span class="number">0.2167</span>, <span class="number">-0.6123</span>,  <span class="number">0.5036</span>]])</span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line">describe(torch.add(x, x))</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.0923</span>,  <span class="number">0.8048</span>, <span class="number">-2.0231</span>],</span><br><span class="line">        [ <span class="number">0.4335</span>, <span class="number">-1.2245</span>,  <span class="number">1.0072</span>]])</span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">describe(x + x)</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.0923</span>,  <span class="number">0.8048</span>, <span class="number">-2.0231</span>],</span><br><span class="line">        [ <span class="number">0.4335</span>, <span class="number">-1.2245</span>,  <span class="number">1.0072</span>]])</span><br></pre></td></tr></table></figure></p>
<p>还有一些运算可以应用到张量的特定维数上。正如您可能已经注意到的，对于2D张量，我们将行表示为维度0，列表示为维度1，如示例1-10所示。<br>Example 1-10. Dimension-based tensor operations<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">6</span>)</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">6</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>])</span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line">x = x.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]])</span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">describe(torch.sum(x, dim=<span class="number">0</span>))</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([ <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">7.</span>])</span><br><span class="line">Input[<span class="number">3</span>]:</span><br><span class="line">describe(torch.sum(x, dim=<span class="number">1</span>))</span><br><span class="line">Output[<span class="number">3</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([  <span class="number">3.</span>,  <span class="number">12.</span>])</span><br><span class="line">Input[<span class="number">4</span>]:</span><br><span class="line">describe(torch.transpose(x, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">Output[<span class="number">4</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">5.</span>]])</span><br></pre></td></tr></table></figure></p>
<p>通常，我们需要执行更复杂的操作，包括索引、切片、连接和突变(indexing,slicing,joining and mutation)的组合。与NumPy和其他数字库一样，PyTorch也有内置函数，可以使此类张量操作非常简单。</p>
<h3 id="Indexing-slicing-and-joining"><a href="#Indexing-slicing-and-joining" class="headerlink" title="Indexing, slicing, and joining"></a>Indexing, slicing, and joining</h3><p>如果您是一个NumPy用户，那么您可能非常熟悉示例1-11中所示的PyTorch的索引和切片方案。</p>
<p>Example 1-11. Slicing and indexing a tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]])</span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line">describe(x[:<span class="number">1</span>, :<span class="number">2</span>])</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>]])</span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">describe(x[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([])</span><br><span class="line">Values:</span><br><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure></p>
<p>示例1-12演示了PyTorch还具有用于复杂索引和切片操作的函数，您可能对有效地访问张量的非连续位置感兴趣。</p>
<p>Example 1-12. Complex indexing: noncontiguous indexing of a tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line">indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">describe(torch.index_select(x, dim=<span class="number">1</span>, index=indices))</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">5.</span>]])</span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line">indices = torch.LongTensor([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">describe(torch.index_select(x, dim=<span class="number">0</span>, index=indices))</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>]])</span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">row_indices = torch.arange(<span class="number">2</span>).long()</span><br><span class="line">col_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">describe(x[row_indices, col_indices])</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">4.</span>])</span><br></pre></td></tr></table></figure></p>
<p>注意指标(indices)是一个长张量;这是使用PyTorch函数进行索引的要求。我们还可以使用内置的连接函数连接张量，如示例1-13所示，通过指定张量和维度。</p>
<p>Example 1-13. Concatenating tensors<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">6</span>).view(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]])</span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line">describe(torch.cat([x, x], dim=<span class="number">0</span>))</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]])</span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">describe(torch.cat([x, x], dim=<span class="number">1</span>))</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]])</span><br><span class="line">Input[<span class="number">3</span>]:</span><br><span class="line">describe(torch.stack([x, x]))</span><br><span class="line">Output[<span class="number">3</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]]])</span><br></pre></td></tr></table></figure></p>
<p>PyTorch还在张量上实现了高效的线性代数操作，如乘法、逆和trace，如示例1-14所示。</p>
<p>Example 1-14. Linear algebra on tensors: multiplication</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x1 = torch.arange(<span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">describe(x1)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]])</span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line">x2 = torch.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">x2[:, <span class="number">1</span>] += <span class="number">1</span></span><br><span class="line">describe(x2)</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">2.</span>]])</span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">describe(torch.mm(x1, x2))</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[  <span class="number">3.</span>,   <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">12.</span>,  <span class="number">24.</span>]])</span><br></pre></td></tr></table></figure>
<p>到目前为止，我们已经研究了创建和操作恒定PyTorch张量对象的方法。就像编程语言(如Python)变量封装一块数据,关于数据的额外信息(如内存地址存储,例如),PyTorch张量处理构建计算图时所需的记账(bookkeeping)所需构建计算图对机器学习只是在实例化时通过启用一个布尔标志。</p>
<h3 id="Tensors-and-Computational-Graphs"><a href="#Tensors-and-Computational-Graphs" class="headerlink" title="Tensors and Computational Graphs"></a>Tensors and Computational Graphs</h3><p>PyTorch张量类封装了数据(张量本身)和一系列操作，如代数操作、索引操作和整形操作。然而,1-15所示的例子,当requires_grad布尔标志被设置为True的张量,记账操作启用,可以追踪的梯度张量以及梯度函数,这两个需要基于促进梯度学习讨论“监督学习范式”。</p>
<p>Example 1-15. Creating tensors for gradient bookkeeping<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">describe(x)</span><br><span class="line">print(x.grad <span class="keyword">is</span> <span class="keyword">None</span>)</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line">y = (x + <span class="number">2</span>) * (x + <span class="number">5</span>) + <span class="number">3</span></span><br><span class="line">describe(y)</span><br><span class="line">print(x.grad <span class="keyword">is</span> <span class="keyword">None</span>)</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">21.</span>,  <span class="number">21.</span>],</span><br><span class="line">        [ <span class="number">21.</span>,  <span class="number">21.</span>]])</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">z = y.mean()</span><br><span class="line">describe(z)</span><br><span class="line">z.backward()</span><br><span class="line">print(x.grad <span class="keyword">is</span> <span class="keyword">None</span>)</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.FloatTensor</span><br><span class="line">Shape/size: torch.Size([])</span><br><span class="line">Values:</span><br><span class="line"><span class="number">21.0</span></span><br><span class="line"><span class="keyword">False</span></span><br></pre></td></tr></table></figure></p>
<p>当您使用requires_grad=True创建张量时，您需要PyTorch来管理计算梯度的bookkeeping信息。首先，PyTorch将跟踪向前传递的值。然后，在计算结束时，使用单个标量来计算向后传递。反向传递是通过对一个张量使用backward()方法来初始化的，这个张量是由一个损失函数的求值得到的。向后传递为参与向前传递的张量对象计算梯度值。</p>
<p>一般来说，梯度是一个值，它表示函数输出相对于函数输入的斜率。在计算图形设置中，模型中的每个参数都存在梯度，可以认为是该参数对误差信号的贡献。在PyTorch中，可以使用.grad成员变量访问计算图中节点的梯度。优化器使用.grad变量更新参数的值。</p>
<p>到目前为止，我们一直在CPU内存上分配张量。在做线性代数运算时，如果你有一个GPU，那么利用它可能是有意义的。要利用GPU，首先需要分配GPU内存上的张量。对gpu的访问是通过一个名为CUDA的专门API进行的。CUDA API是由NVIDIA创建的，并且仅限于在NVIDIA gpu上使用。PyTorch提供的CUDA张量对象在使用中与常规cpu绑定张量没有区别，除了内部分配的方式不同。</p>
<h3 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h3><p>PyTorch使创建这些CUDA张量变得非常容易(示例1-16)，它将张量从CPU传输到GPU，同时维护其底层类型。PyTorch中的首选方法是与设备无关，并编写在GPU或CPU上都能工作的代码。在下面的代码片段中，我们首先使用torch.cuda.is_available()检查GPU是否可用，然后使用torch.device检索设备名。然后，将实例化所有未来的张量，并使用.to(device)方法将其移动到目标设备。</p>
<p>Example 1-16. Creating CUDA tensors<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">print</span> (torch.cuda.is_available())</span><br><span class="line">Output[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line">Input[<span class="number">1</span>]:</span><br><span class="line"><span class="comment"># preferred method: device agnostic tensor instantiation</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"><span class="keyword">print</span> (device)</span><br><span class="line">Output[<span class="number">1</span>]:</span><br><span class="line">cuda</span><br><span class="line">Input[<span class="number">2</span>]:</span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">3</span>).to(device)</span><br><span class="line">describe(x)</span><br><span class="line">Output[<span class="number">2</span>]:</span><br><span class="line">Type: torch.cuda.FloatTensor</span><br><span class="line">Shape/size: torch.Size([<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">Values:</span><br><span class="line">tensor([[ <span class="number">0.9149</span>,  <span class="number">0.3993</span>,  <span class="number">0.1100</span>],</span><br><span class="line">        [ <span class="number">0.2541</span>,  <span class="number">0.4333</span>,  <span class="number">0.4451</span>],</span><br><span class="line">        [ <span class="number">0.4966</span>,  <span class="number">0.7865</span>,  <span class="number">0.6604</span>]], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure></p>
<p>要对CUDA和非CUDA对象进行操作，我们需要确保它们在同一设备上。如果我们不这样做，计算就会中断，如下面的代码片段所示。例如，在计算不属于计算图的监视指标时，就会出现这种情况。当操作两个张量对象时，确保它们在同一个设备上。例子1-17所示。</p>
<p>Example 1-17. Mixing CUDA tensors with CPU bound tensors<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]</span><br><span class="line">y = torch.rand(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x + y</span><br><span class="line">Output[<span class="number">0</span>]</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">RuntimeError                         Traceback (most recent call last)</span><br><span class="line">      <span class="number">1</span> y = torch.rand(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">----&gt; 2 x + y</span><br><span class="line"></span><br><span class="line">RuntimeError: Expected object of type torch.cuda.FloatTensor but found type torch.FloatTensor <span class="keyword">for</span> argument <span class="comment">#3 'other'</span></span><br><span class="line">Input[<span class="number">1</span>]</span><br><span class="line">cpu_device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">y = y.to(cpu_device)</span><br><span class="line">x = x.to(cpu_device)</span><br><span class="line">x + y</span><br><span class="line">Output[<span class="number">1</span>]</span><br><span class="line">tensor([[ <span class="number">0.7159</span>,  <span class="number">1.0685</span>,  <span class="number">1.3509</span>],</span><br><span class="line">        [ <span class="number">0.3912</span>,  <span class="number">0.2838</span>,  <span class="number">1.3202</span>],</span><br><span class="line">        [ <span class="number">0.2967</span>,  <span class="number">0.0420</span>,  <span class="number">0.6559</span>]])</span><br></pre></td></tr></table></figure></p>
<p>请记住，将数据从GPU来回移动是非常昂贵的。因此，典型的过程包括在GPU上执行许多并行计算，然后将最终结果传输回CPU。这将允许您充分利用gpu。如果您有几个CUDA-visible设备(即，最佳实践是在执行程序时使用CUDA_VISIBLE_DEVICES环境变量，如下图所示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3 python main.py</span><br></pre></td></tr></table></figure>
<p>在本书中我们不涉及并行性和多gpu训练，但是它们在缩放实验中是必不可少的，有时甚至在训练大型模型时也是如此。我们建议您参考PyTorch文档和讨论论坛，以获得关于这个主题的更多帮助和支持。</p>
<h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises:"></a>Exercises:</h1><p>掌握一个主题的最好方法是解决问题。这里有一些热身运动。许多问题将涉及到查阅官方文件[1]和寻找有用的功能。<br>.</p>
<ol>
<li><p>Create a 2D tensor and then add a dimension of size 1 inserted at dimension 0.</p>
</li>
<li><p>Remove the extra dimension you just added to the previous tensor.</p>
</li>
<li><p>Create a random tensor of shape 5x3 in the interval [3, 7)</p>
</li>
<li><p>Create a tensor with values from a normal distribution (mean=0, std=1).</p>
</li>
<li><p>Retrieve the indexes of all the nonzero elements in the tensor torch.Tensor([1, 1, 1, 0, 1]).</p>
</li>
<li><p>Create a random tensor of size (3,1) and then horizontally stack 4 copies together.</p>
</li>
<li><p>Return the batch matrix-matrix product of two 3-dimensional matrices (a=torch.rand(3,4,5), b=torch.rand(3,5,4)).</p>
</li>
<li><p>Return the batch matrix-matrix product of a 3D matrix and a 2D matrix (a=torch.rand(3,4,5), b=torch.rand(5,4)).</p>
<h1 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h1></li>
<li>a = torch.rand(3, 3)<br> a.unsqueeze(0)</li>
<li>a.squeeze(0)</li>
<li>3 + torch.rand(5, 3) * (7 - 3)</li>
<li>a = torch.rand(3, 3)<br>a.normal_()</li>
<li><p>a = torch.Tensor([1, 1, 1, 0, 1])<br>torch.nonzero(a)</p>
</li>
<li><p>a = torch.rand(3, 1)<br>a.expand(3, 4)</p>
</li>
<li><p>a = torch.rand(3, 4, 5)<br>b = torch.rand(3, 5, 4)<br>torch.bmm(a, b)</p>
</li>
<li><p>a = torch.rand(3, 4, 5)<br>b = torch.rand(5, 4)<br>torch.bmm(a, b.unsqueeze(0).expand(a.size(0), * b.size()))</p>
</li>
</ol>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>在这一章中，我们介绍了本书的目标——自然语言处理(NLP)和深度学习——并对监督学习范式进行了详细的理解。在本章的最后，您现在应该熟悉或至少了解各种术语，例如观察、目标、模型、参数、预测、损失函数、表示、学习/培训和推理。您还了解了如何使用单热编码对学习任务的输入(观察和目标)进行编码。我们还研究了基于计数的表示，如TF和TF- idf。我们首先了解了什么是计算图，静态和动态计算图，以及PyTorch张量操纵操作。在第二章中，我们对传统的NLP进行了概述。第二章，这一章应该为你奠定必要的基础，如果你对这本书的主题是新的，并为你的书的其余部分做准备。</p>
<p>重点是TF-IDF</p>
<script type="math/tex; mode=display">词频(TF)=\frac{某个词在文章中出现的次数}{文章中的总词数}</script><script type="math/tex; mode=display">逆文档频率(IDF)=log(\frac{语料库的文档总数}{包含该词的文档数+1})</script><p>TF应该很容易理解就是计算词频,IDF衡量词的常见程度.为了计算IDF我们需要事先准备一个语料库用来模拟语言的使用环境,如果一个词越是常见,那么式子中分母越大,逆文档频率越接近0.这里分母+1是为了避免分母为0的情况出现</p>
<script type="math/tex; mode=display">TF-IDF=词频(TF)×逆文档频率(IDF)</script><p>TF-IDF可以很好的实现提取文章中关键词的目的.</p>

      
    </div>
    
    
    

	<div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/">Natural-Language-Processing-with-PyTorch（一）</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 Yif Du 的个人博客">Yif Du</a></p>
  <p><span>发布时间:</span>2018年12月17日 - 09:12</p>
  <p><span>最后更新:</span>2019年02月16日 - 21:02</p>
  <p><span>原始链接:</span><a href="/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/" title="Natural-Language-Processing-with-PyTorch（一）">http://yifdu.github.io/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://yifdu.github.io/2018/12/17/Natural-Language-Processing-with-PyTorch（一）/" aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
      $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
        });
    });  
</script>

      
    </div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
            <a href="/tags/Pytorch/" rel="tag"># Pytorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/15/机器学习（四）——集成学习/" rel="next" title="机器学习（四）——集成学习">
                <i class="fa fa-chevron-left"></i> 机器学习（四）——集成学习
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/17/数字图像处理笔记（二）/" rel="prev" title="数字图像处理笔记（二）">
                数字图像处理笔记（二） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div id="gitalk-container"></div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/xuanyi.jpg" alt="Yif Du">
            
              <p class="site-author-name" itemprop="name">Yif Du</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">80</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">69</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yifdu" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="17210240004@fudan.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Preface"><span class="nav-number">1.</span> <span class="nav-text">Preface</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">Chapter 1.Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Supervised-Learning-Paradigm"><span class="nav-number">2.1.</span> <span class="nav-text">The Supervised Learning Paradigm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Observation-and-Target-Encoding"><span class="nav-number">2.2.</span> <span class="nav-text">Observation and Target Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#One-Hot-Representation"><span class="nav-number">2.2.1.</span> <span class="nav-text">One-Hot Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TF-Representation"><span class="nav-number">2.2.2.</span> <span class="nav-text">TF Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TF-IDF-Representation"><span class="nav-number">2.2.3.</span> <span class="nav-text">TF-IDF Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Target-Encoding"><span class="nav-number">2.2.4.</span> <span class="nav-text">Target Encoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Computational-Graphs"><span class="nav-number">2.3.</span> <span class="nav-text">Computational Graphs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PyTorch-Basics"><span class="nav-number">2.4.</span> <span class="nav-text">PyTorch Basics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Installing-PyTorch"><span class="nav-number">2.4.1.</span> <span class="nav-text">Installing PyTorch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Creating-Tensors"><span class="nav-number">2.4.2.</span> <span class="nav-text">Creating Tensors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-Types-and-Size"><span class="nav-number">2.4.3.</span> <span class="nav-text">Tensor Types and Size</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-Operations"><span class="nav-number">2.4.4.</span> <span class="nav-text">Tensor Operations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Indexing-slicing-and-joining"><span class="nav-number">2.4.5.</span> <span class="nav-text">Indexing, slicing, and joining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensors-and-Computational-Graphs"><span class="nav-number">2.4.6.</span> <span class="nav-text">Tensors and Computational Graphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA-Tensors"><span class="nav-number">2.4.7.</span> <span class="nav-text">CUDA Tensors</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Exercises"><span class="nav-number">3.</span> <span class="nav-text">Exercises:</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Solutions"><span class="nav-number">4.</span> <span class="nav-text">Solutions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary"><span class="nav-number">5.</span> <span class="nav-text">Summary</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="heart">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yif Du</span>

  
</div>





        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script src="/js/src/md5.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '7428ad62daef314bef06',
          clientSecret: '93cd3f4cd41cfc00c4760f65f8d895a66088ea5a',
          repo: 'Comments',
          owner: 'yifdu',
          admin: ['yifdu'],
          id: md5(location.pathname),
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>


  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  <style>
#selectionCopyright {
    position: absolute;
    display: none;
    background: rgba(244,67,54,.7);
    color: #fff;
    border-radius: 6px;
    box-shadow: none;
    border: none;
    font-size: 14px;
}
#selectionCopyright a{
    color:#fff;
    border-color: #fff;
}
#selectionCopyright::before {
    content: "";
    width: 0;
    height: 0;
    border-style: solid;
    border-width: 6px 8px 6px 0;
    border-color: transparent rgba(244,67,54,.7) transparent transparent;
    position: absolute;
    left: -8px;
    top:50%;
    transform:translateY(-50%);
}
</style>

<button id="selectionCopyright" disabled="disabled">本文发表于[<a href="http://yifdu.github.io">yifdu.github.io</a>]分享请注明来源！</button>

<script>
window.onload = function() {
    function selectText() {
        if (document.selection) { //IE浏览器下
            return document.selection.createRange().text; //返回选中的文字
        } else { //非IE浏览器下
            return window.getSelection().toString(); //返回选中的文字
        }
    }
    var content = document.getElementsByTagName("body")[0];
    var scTip = document.getElementById('selectionCopyright');

    content.onmouseup = function(ev) { //设定一个onmouseup事件
        var ev = ev || window.event;
        var left = ev.clientX;//获取鼠标相对浏览器可视区域左上角水平距离距离
        var top = ev.clientY;//获取鼠标相对浏览器可视区域左上角垂直距离距离
        var xScroll = Math.max(document.body.scrollLeft, document.documentElement.scrollLeft);//获取文档水平滚动距离
        var yScroll = Math.max(document.body.scrollTop, document.documentElement.scrollTop);//获取文档垂直滚动距离
        if (selectText().length > 0) {
            setTimeout(function() { //设定一个定时器
                scTip.style.display = 'inline-block';
                scTip.style.left = left + xScroll + 15 + 'px';//鼠标当前x值
                scTip.style.top = top + yScroll - 15 + 'px';//鼠标当前y值
            }, 100);
        } else {
            scTip.style.display = 'none';
        }
    };

    content.onclick = function(ev) {
        var ev = ev || window.event;
        ev.cancelBubble = true;
    };
    document.onclick = function() {
        scTip.style.display = 'none';
    };
};
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
