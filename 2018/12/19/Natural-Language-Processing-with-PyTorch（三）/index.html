<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<meta name="theme-color" content="#222">


  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "258f1ebb"
    });
  daovoice('update');
  </script>









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP,Pytorch,">










<meta name="description" content="Chapter 3. Foundational Components of Neural Networks本章通过介绍构建神经网络的基本思想，如激活函数、损失函数、优化器和监督训练设置，为后面的章节奠定了基础。我们从感知器开始，这是一个将不同概念联系在一起的一个单元的神经网络。感知器本身是更复杂的神经网络的组成部分。这是一种贯穿全书的常见模式，我们讨论的每个架构或网络都可以单独使用，也可以在其他复">
<meta name="keywords" content="NLP,Pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="Natural-Language-Processing-with-PyTorch（三）">
<meta property="og:url" content="http://yifdu.github.io/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/index.html">
<meta property="og:site_name" content="深度智障">
<meta property="og:description" content="Chapter 3. Foundational Components of Neural Networks本章通过介绍构建神经网络的基本思想，如激活函数、损失函数、优化器和监督训练设置，为后面的章节奠定了基础。我们从感知器开始，这是一个将不同概念联系在一起的一个单元的神经网络。感知器本身是更复杂的神经网络的组成部分。这是一种贯穿全书的常见模式，我们讨论的每个架构或网络都可以单独使用，也可以在其他复">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1545023935255&di=c2469985994fb00217b96200714f5f27&imgtype=0&src=http%3A%2F%2Fimage.tupian114.com%2F20120504%2F2012050420332255.jpg">
<meta property="og:updated_time" content="2018-12-20T04:43:17.340Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Natural-Language-Processing-with-PyTorch（三）">
<meta name="twitter:description" content="Chapter 3. Foundational Components of Neural Networks本章通过介绍构建神经网络的基本思想，如激活函数、损失函数、优化器和监督训练设置，为后面的章节奠定了基础。我们从感知器开始，这是一个将不同概念联系在一起的一个单元的神经网络。感知器本身是更复杂的神经网络的组成部分。这是一种贯穿全书的常见模式，我们讨论的每个架构或网络都可以单独使用，也可以在其他复">
<meta name="twitter:image" content="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1545023935255&di=c2469985994fb00217b96200714f5f27&imgtype=0&src=http%3A%2F%2Fimage.tupian114.com%2F20120504%2F2012050420332255.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yifdu.github.io/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/">





  <title>Natural-Language-Processing-with-PyTorch（三） | 深度智障</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">深度智障</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yifdu.github.io/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yif Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/xuanyi.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深度智障">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Natural-Language-Processing-with-PyTorch（三）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-19T14:58:50+08:00">
                2018-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读数
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  15.2k 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  59 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
        <div class="post-gallery" itemscope="" itemtype="http://schema.org/ImageGallery">
          
          
            <div class="post-gallery-row">
              <a class="post-gallery-img fancybox" href="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1545023935255&di=c2469985994fb00217b96200714f5f27&imgtype=0&src=http%3A%2F%2Fimage.tupian114.com%2F20120504%2F2012050420332255.jpg" rel="gallery_cjpxov2hr005q6owagyuu7gdf" itemscope="" itemtype="http://schema.org/ImageObject" itemprop="url">
                <img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1545023935255&di=c2469985994fb00217b96200714f5f27&imgtype=0&src=http%3A%2F%2Fimage.tupian114.com%2F20120504%2F2012050420332255.jpg" itemprop="contentUrl">
              </a>
            
          

          
          </div>
        </div>
      

      
        <h1 id="Chapter-3-Foundational-Components-of-Neural-Networks"><a href="#Chapter-3-Foundational-Components-of-Neural-Networks" class="headerlink" title="Chapter 3. Foundational Components of Neural Networks"></a>Chapter 3. Foundational Components of Neural Networks</h1><p>本章通过介绍构建神经网络的基本思想，如激活函数、损失函数、优化器和监督训练设置，为后面的章节奠定了基础。我们从感知器开始，这是一个将不同概念联系在一起的一个单元的神经网络。感知器本身是更复杂的神经网络的组成部分。这是一种贯穿全书的常见模式，我们讨论的每个架构或网络都可以单独使用，也可以在其他复杂的网络中组合使用。当我们讨论计算图形和本书的其余部分时，这种组合性将变得清晰起来。</p>
<h2 id="Perceptron-The-Simplest-Neural-Network"><a href="#Perceptron-The-Simplest-Neural-Network" class="headerlink" title="Perceptron: The Simplest Neural Network"></a>Perceptron: The Simplest Neural Network</h2><p>最简单的神经网络单元是感知器。感知器在历史上是非常松散地模仿生物神经元的。就像生物神经元一样，有输入和输出，“信号”从输入流向输出，如图3-1所示。<br><img src="/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/neural.png" alt="neural" title="图3-1. 具有输入（x）和输出（y）的感知机的计算图。权值（w）和偏差（b）构成模型的参数。"><br>每个感知器单元有一个输入(x),一个输出(y),和三个“旋钮”（knobs）:一组权重(w),偏量(b),和一个激活函数(f)。权重和偏量都从数据学习,激活函数是精心挑选的取决于网络的网络设计师的直觉和目标输出。数学上，我们可以这样表示:</p>
<script type="math/tex; mode=display">y=f(wx+b)</script><p>通常情况下感知器有不止一个输入。我们可以用向量表示这个一般情况;即，x和w是向量，w和x的乘积替换为点积:</p>
<script type="math/tex; mode=display">y=f(\vec{w}^T\vec{x}+b)</script><p>激活函数，这里用f表示，通常是一个非线性函数。示例3-1展示了PyTorch中的感知器实现，它接受任意数量的输入、执行仿射转换、应用激活函数并生成单个输出。</p>
<p>Example 3-1. Implementing a Perceptron using PyTorch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" A Perceptron is one Linear layer """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            input_dim (int): size of the input features</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(Perceptron, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        <span class="string">"""The forward pass of the Perceptron</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x_in (torch.Tensor): an input data tensor.</span></span><br><span class="line"><span class="string">                x_in.shape should be (batch, num_features)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the resulting tensor. tensor.shape should be (batch,)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.fc1(x_in)).squeeze()</span><br></pre></td></tr></table></figure></p>
<p>线性运算$\vec{w}^T\vec{x}+b$称为仿射变换。PyTorch方便地在torch中提供了一个Linear()类。nn模块，它做权值和偏差所需的簿记，并做所需的仿射变换。在“深入到有监督的训练”中，您将看到如何从数据中“学习”权重w和b的值。前面示例中使用的激活函数是sigmoid函数。在下一节中，我们将回顾一些常见的激活函数，包括sigmoid函数。</p>
<h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h2><p>激活函数是神经网络中引入的非线性函数，用于捕获数据中的复杂关系。在“深入到有监督的训练”和“多层感知器”中，我们深入研究了为什么学习中需要非线性，但首先，让我们看看一些常用的激活函数。</p>
<h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>sigmoid是神经网络历史上最早使用的激活函数之一。它取任何实值并将其压缩在0和1之间。数学上，sigmoid的表达式如下:</p>
<script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-x}}</script><p>从表达式中很容易看出，sigmoid是一个光滑的、可微的函数。Torch将sigmoid实现为Torch .sigmoid()，如示例3-2所示。<br>Example 3-2. Sigmoid activation<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = torch.range(<span class="number">-5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line">y = torch.sigmoid(x)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>从图中可以看出，sigmoid函数饱和(即，产生极值输出)非常快，对于大多数输入。这可能成为一个问题，因为它可能导致梯度变为零或发散到溢出的浮点值。这些现象分别被称为消失梯度问题和爆炸梯度问题。因此，在神经网络中，除了在输出端使用sigmoid单元外，很少看到其他使用sigmoid单元的情况，在输出端，压缩属性允许将输出解释为概率。</p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>如例3-3所示，tanh激活函数是sigmoid在外观上的不同变体。当你写下tanh的表达式时，这就变得很清楚了:</p>
<script type="math/tex; mode=display">f(x)=tanh x=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><p>通过一些争论(我们留作练习)，您可以确信tanh只是sigmoid的一个线性变换。当您为tanh()写下PyTorch代码并绘制曲线时，这一点也很明显。注意双曲正切,像sigmoid,也是一个“压缩”函数,除了它映射一个实值集合从(-∞,+∞)到(-1,+1)范围。</p>
<p>Example 3-3. Tanh activation<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = torch.range(<span class="number">-5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line">y = torch.tanh(x)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>ReLU(发音为ray-luh)代表线性整流单元。这可以说是最重要的激活函数。事实上，我们可以大胆地说，如果没有使用ReLU，许多最近在深度学习方面的创新都是不可能实现的。对于一些如此基础的东西来说，神经网络激活函数的出现也是令人惊讶的。它的形式也出奇的简单:<br>f(x)=max(0,x)<br>因此，ReLU单元所做的就是将负值裁剪为零，如示例3-4所示。<br>Example 3-4. ReLU activation<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">relu = torch.nn.ReLU()</span><br><span class="line">x = torch.range(<span class="number">-5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line">y = relu(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>ReLU的裁剪效果有助于消除梯度问题，随着时间的推移，网络中的某些输出可能会变成零，再也不会恢复。这就是所谓的“dying ReLU”问题。为了减轻这种影响，提出了Leaky ReLU或  Parametric ReLU (PReLU)等变体，其中泄漏系数a是一个可学习参数:<br>f(x)=max(x,ax)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">prelu = torch.nn.PReLU(num_parameters=<span class="number">1</span>)</span><br><span class="line">x = torch.range(<span class="number">-5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line">y = prelu(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>激活函数的另一个选择是softmax。与sigmoid函数类似，softmax函数将每个单元的输出压缩为0到1之间。然而，softmax操作还将每个输出除以所有输出的和，从而得到一个离散概率分布，除以k个可能的类。结果分布中的概率总和为1。这对于解释分类任务的输出非常有用，因此这种转换通常与概率训练目标配对，例如分类交叉熵，它在“深入研究监督训练”中介绍</p>
<script type="math/tex; mode=display">softmax(x_i)=\frac{e^{x_i}}{\sum_{j=1}^ke^{x_j}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">y_output = softmax(x_input)</span><br><span class="line">print(x_input)</span><br><span class="line">print(y_output)</span><br><span class="line">print(torch.sum(y_output, dim=<span class="number">1</span>))</span><br><span class="line">Output[<span class="number">0</span>]</span><br><span class="line">tensor([[ <span class="number">0.5836</span>, <span class="number">-1.3749</span>, <span class="number">-1.1229</span>]])</span><br><span class="line">tensor([[ <span class="number">0.7561</span>,  <span class="number">0.1067</span>,  <span class="number">0.1372</span>]])</span><br><span class="line">tensor([ <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<p>在本节中，我们研究了四个重要的激活函数:Sigmoid、Tanh、ReLU和softmax。这些只是你在构建神经网络时可能用到的四种激活方式。随着本书的深入，我们将会清楚地看到应该使用哪些激活函数以及在哪里使用，但是一般的指南只是简单地遵循过去的工作原理。</p>
<h2 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h2><p>在第1章中，我们看到了通用的监督机器学习架构，以及损失函数或目标函数如何通过查看数据来帮助指导训练算法选择正确的参数。回想一下,一个损失函数truth(y)和预测(ŷ)作为输入,产生一个实值的分数。这个分数越高，模型的预测就越差。PyTorch在它的nn包中实现了许多损失函数，这些函数太过全面，这里就不介绍了，但是我们将介绍一些常用的损失函数。</p>
<h3 id="Mean-Squared-Error-Loss"><a href="#Mean-Squared-Error-Loss" class="headerlink" title="Mean Squared Error Loss"></a>Mean Squared Error Loss</h3><p>回归问题的网络的输出(ŷ)和目标(y)是连续值,一个常用的损失函数的均方误差(MSE)。</p>
<script type="math/tex; mode=display">L_{MSE}(y,\hat{y})=\frac{1}{n}\sum_{i=1}^n(y-\hat{y})^2</script><p>MSE就是预测值与目标值之差的平方的平均值。还有一些其他的损失函数可以用于回归问题，例如平均绝对误差(MAE)和均方根误差(RMSE)，但是它们都涉及到计算输出和目标之间的实值距离。示例3-6展示了如何使用PyTorch实现MSE loss。</p>
<p>Example 3-6. MSE Loss<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">mse_loss = nn.MSELoss()</span><br><span class="line">outputs = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">targets = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">loss = mse_loss(outputs, targets)</span><br><span class="line">print(loss)</span><br><span class="line">Output[<span class="number">0</span>]</span><br><span class="line">tensor(<span class="number">3.8618</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="Categorical-Cross-Entropy-Loss"><a href="#Categorical-Cross-Entropy-Loss" class="headerlink" title="Categorical Cross-Entropy Loss"></a>Categorical Cross-Entropy Loss</h3><p>分类交叉熵损失(categorical cross-entropy loss)通常用于多类分类设置，其中输出被解释为类隶属度概率的预测。目标(y)是n个元素的向量，表示所有类的真正多项分布。如果只有一个类是正确的，那么这个向量就是one hot向量。网络的输出(ŷ)也是一个向量n个元素,但代表了网络的多项分布的预测。分类交叉熵将比较这两个向量(y,ŷ)来衡量损失:</p>
<script type="math/tex; mode=display">L_{cross-entropy}(y,\hat{y})=-\sum_{i}y_ilog(\hat{y_i})</script><p>交叉熵和它的表达式起源于信息论，但是为了本节的目的，把它看作一种计算两个分布有多不同的方法是有帮助的。我们希望正确的类的概率接近1，而其他类的概率接近0。</p>
<p>为了正确地使用PyTorch的交叉熵损失，一定程度上理解网络输出、损失函数的计算方法和来自真正表示浮点数的各种计算约束之间的关系是很重要的。具体来说，有四条信息决定了网络输出和损失函数之间微妙的关系。首先，一个数字的大小是有限制的。其次，如果softmax公式中使用的指数函数的输入是负数，则结果是一个指数小的数，如果是正数，则结果是一个指数大的数。接下来，假定网络的输出是应用softmax函数之前的向量。最后,对数函数是指数函数的倒数,和log(exp (x))就等于x。因这四个信息,数学简化假设<br>指数函数和log函数是为了更稳定的数值计算和避免很小或很大的数字。这些简化的结果是，不使用softmax函数的网络输出可以与PyTorch的交叉熵损失一起使用，从而优化概率分布。然后，当网络经过训练后，可以使用softmax函数创建概率分布，如例3-7所示。<br>Example 3-7. Cross-entropy loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">ce_loss = nn.CrossEntropyLoss()</span><br><span class="line">outputs = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int64)</span><br><span class="line">loss = ce_loss(outputs, targets)</span><br><span class="line">print(loss)</span><br><span class="line">Output[<span class="number">0</span>]</span><br><span class="line">tensor(<span class="number">2.7256</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Binary-Cross-Entropy"><a href="#Binary-Cross-Entropy" class="headerlink" title="Binary Cross-Entropy"></a>Binary Cross-Entropy</h3><p>我们在上一节看到的分类交叉熵损失函数在我们有多个类的分类问题中非常有用。有时，我们的任务包括区分两个类——也称为二元分类。在这种情况下，利用二元交叉熵损失是有效的。我们将在示例任务的“示例:对餐馆评论的情绪进行分类”中研究这个损失函数。</p>
<p>在示例3-8中，我们使用表示网络输出的随机向量上的sigmoid激活函数创建二进制概率输出向量。接下来，ground truth被实例化为一个0和1的向量。最后，利用二元概率向量和基真值向量计算二元交叉熵损失。</p>
<p>Example 3-8. Binary cross-entropy loss<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]</span><br><span class="line">bce_loss = nn.BCELoss()</span><br><span class="line">sigmoid = nn.Sigmoid()</span><br><span class="line">probabilities = sigmoid(torch.randn(<span class="number">4</span>, <span class="number">1</span>, requires_grad=<span class="keyword">True</span>))</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],  dtype=torch.float32).view(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">loss = bce_loss(probabilities, targets)</span><br><span class="line">print(probabilities)</span><br><span class="line">print(loss)</span><br><span class="line">Output[<span class="number">0</span>]</span><br><span class="line">tensor([[ <span class="number">0.1625</span>],</span><br><span class="line">        [ <span class="number">0.5546</span>],</span><br><span class="line">        [ <span class="number">0.6596</span>],</span><br><span class="line">        [ <span class="number">0.4284</span>]])</span><br><span class="line">tensor(<span class="number">0.9003</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="Diving-Deep-into-Supervised-Training"><a href="#Diving-Deep-into-Supervised-Training" class="headerlink" title="Diving Deep into Supervised Training"></a>Diving Deep into Supervised Training</h2><p>监督学习是学习如何将观察结果映射到特定目标的问题。在这一节中，我们将更详细地讨论。具体地说，我们明确地描述了如何使用模型预测和损失函数对模型参数进行基于梯度的优化。这是一个重要的部分，因为本书的其余部分都依赖于它，所以即使您对监督学习有些熟悉，也值得详细阅读它。</p>
<p>回顾第1章，有监督学习需要以下内容:模型、损失函数、训练数据和优化算法。监督学习的训练数据是观察和目标对，模型从观察中计算预测，损失衡量预测相对于目标的误差。训练的目的是利用基于梯度的优化算法来调整模型的参数，使损失尽可能小。</p>
<p>在本节的其余部分中，我们将讨论一个经典的玩具问题:将二维点划分为两个类中的一个。直观上，这意味着学习一条直线(称为决策边界或超平面)来区分类之间的点。我们一步一步地描述数据结构，选择模型，选择一个损失，建立优化算法，最后，一起运行它。</p>
<h3 id="Constructing-Toy-Data"><a href="#Constructing-Toy-Data" class="headerlink" title="Constructing Toy Data"></a>Constructing Toy Data</h3><p>在机器学习中，当试图理解一个算法时，创建具有易于理解的属性的合成数据是一种常见的实践。在本节中，我们使用“玩具”任务的合成数据——将二维点分类为两个类中的一个。为了构建数据，我们从xy平面的两个不同部分采样点，为模型创建了一个易于学习的环境。示例如图3-2所示。模型的目标是将星星(⋆)作为一个类,◯)作为另一个类。这可以在图的右边看到，线上面的东西和线下面的东西分类不同。生成数据的代码位于本章附带的Python notebook中名为get_toy_data()的函数中。<br><img src="/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/data.png" alt="data" title="图3-2. 创建一个线性可分的玩具数据集。数据集是从两个正态分布中抽取的样本，每个正态分布对应一个类。分类任务变成了区分一个数据点属于一个分布还是另一个分布的任务之一。"></p>
<h3 id="CHOOSING-A-MODEL"><a href="#CHOOSING-A-MODEL" class="headerlink" title="CHOOSING A MODEL"></a>CHOOSING A MODEL</h3><p>我们在这里使用的模型是在本章开头介绍的:感知器。感知器是灵活的，因为它允许任何大小的输入。在典型的建模情况下，输入大小由任务和数据决定。在这个玩具示例中，输入大小为2，因为我们显式地将数据构造为二维平面。对于这个两类问题，我们为类指定一个数字索引:0和1。字符串的映射标签⋆和◯类指数是任意的,只要它在数据预处理是一致的,培训,评估和测试。该模型的另一个重要属性是其输出的性质。由于感知器的激活函数是一个sigmoid，感知器的输出为数据点(x)为class 1的概率;即P(y = 1 | x)</p>
<p>CONVERTING THE PROBABILITIES TO DISCRETE CLASSES<br>对于二元分类问题,我们可以输出概率转换成两个离散类通过利用决策边界,δ。如果预测的概率P(y = 1 | x)&gt;δ,预测类是1,其它类是0。通常，这个决策边界被设置为0.5，但是在实践中，您可能需要优化这个超参数(使用一个评估数据集)，以便在分类中获得所需的精度。</p>
<p>CHOOSING A LOSS FUNCTION<br>在准备好数据并选择了模型体系结构之后，在有监督的培训中还可以选择另外两个重要组件:损失函数和优化器。在模型输出为概率的情况下，最合适的损失函数是基于熵的交叉损失。对于这个玩具数据示例，由于模型产生二进制结果，我们特别使用BCE损失。</p>
<p>CHOOSING AN OPTIMIZER<br>在这个简化的监督训练示例中，最后的选择点是优化器。当模型产生预测，损失函数测量预测和目标之间的误差时，优化器使用错误信号更新模型的权重。最简单的形式是，有一个超参数控制优化器的更新行为。这个超参数称为学习率，它控制错误信号对更新权重的影响。学习速率是一个关键的超参数，你应该尝试几种不同的学习速率并进行比较。较大的学习率会对参数产生较大的变化，并会影响收敛性。学习率过低会导致在训练过程中进展甚微。</p>
<p>PyTorch库为优化器提供了几种选择。随机梯度下降法(SGD)是一种经典的选择算法，但对于复杂的优化问题，SGD存在收敛性问题，往往导致模型较差。当前首选的替代方案是自适应优化器，例如Adagrad或Adam，它们使用关于更新的信息。在下面的例子中，我们使用Adam，但是它总是值得查看几个优化器。对于Adam，默认的学习率是0.001。对于学习率之类的超参数，总是建议首先使用默认值，除非您从论文中获得了需要特定值的秘诀。</p>
<p>Example 3-9. Instantiating the Adam optimizer<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">input_dim = <span class="number">2</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">perceptron = Perceptron(input_dim=input_dim)</span><br><span class="line">bce_loss = nn.BCELoss()</span><br><span class="line">optimizer = optim.Adam(params=perceptron.parameters(), lr=lr)</span><br></pre></td></tr></table></figure></p>
<h3 id="Putting-It-Together-Gradient-Based-Supervised-Learning"><a href="#Putting-It-Together-Gradient-Based-Supervised-Learning" class="headerlink" title="Putting It Together: Gradient-Based Supervised Learning"></a>Putting It Together: Gradient-Based Supervised Learning</h3><p>学习从计算损失开始;也就是说，模型预测离目标有多远。损失函数的梯度，反过来，是参数应该改变多少的信号。每个参数的梯度表示给定参数的损失值的瞬时变化率。实际上，这意味着您可以知道每个参数对损失函数的贡献有多大。直观上，这是一个斜率，你可以想象每个参数都站在它自己的山上，想要向上或向下移动一步。基于梯度的模型训练所涉及的最简单的形式就是迭代地更新每个参数，并使用与该参数相关的损失函数的梯度。</p>
<p>让我们看看这个梯度步进(gradient-steeping)算法是什么样子的。首先，使用名为zero_grad()的函数清除当前存储在模型(感知器)对象中的所有记帐信息，例如梯度。然后，模型计算给定输入数据(x_data)的输出(y_pred)。接下来，通过比较模型输出(y_pred)和预期目标(y_target)来计算损失。这正是有监督训练信号的有监督部分。PyTorch损失对象(criteria)具有一个名为bcakward()的函数，该函数迭代地通过计算图向后传播损失，并将其梯度通知每个参数。最后，优化器(opt)用一个名为step()的函数指示参数如何在知道梯度的情况下更新它们的值。</p>
<p>整个训练数据集被划分成多个批(batch)。在文献和本书中，术语minibatch也可以互换使用，而不是“batch”来强调每个batch都明显小于训练数据的大小;例如，训练数据可能有数百万个，而小批数据可能只有几百个。梯度步骤的每一次迭代都在一批数据上执行。名为batch_size的超参数指定批次的大小。由于训练数据集是固定的，增加批大小会减少批的数量。在多个批处理(通常是有限大小数据集中的批处理数量)之后，训练循环完成了一个epoch。epoch是一个完整的训练迭代。如果每个epoch的批数量与数据集中的批数量相同，那么epoch就是对数据集的完整迭代。模型是为一定数量的epoch而训练的。要训练的epoch的数量对于选择来说不是复杂的，但是有一些方法可以决定什么时候停止，我们稍后将讨论这些方法。如示例3-10所示，受监督的训练循环因此是一个嵌套循环:数据集或批处理集合上的内部循环，以及外部循环，后者在固定数量的epoches或其他终止条件上重复内部循环。</p>
<p>Example 3-10. A supervised training loop for a Perceptron and binary classification<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># each epoch is a complete pass over the training data</span></span><br><span class="line"><span class="keyword">for</span> epoch_i <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># the inner loop is over the batches in the dataset</span></span><br><span class="line">    <span class="keyword">for</span> batch_i <span class="keyword">in</span> range(n_batches):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 0: Get the data</span></span><br><span class="line">        x_data, y_target = get_toy_data(batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1: Clear the gradients</span></span><br><span class="line">        perceptron.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2: Compute the forward pass of the model</span></span><br><span class="line">        y_pred = perceptron(x_data, apply_sigmoid=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3: Compute the loss value that we wish to optimize</span></span><br><span class="line">        loss = bce_loss(y_pred, y_target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4: Propagate the loss signal backward</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 5: Trigger the optimizer to perform one update</span></span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure></p>
<h3 id="Auxiliary-Training-Concepts"><a href="#Auxiliary-Training-Concepts" class="headerlink" title="Auxiliary Training Concepts"></a>Auxiliary Training Concepts</h3><p>基于梯度监督学习的核心概念很简单:定义模型，计算输出，使用损失函数计算梯度，应用优化算法用梯度更新模型参数。然而，在训练过程中有几个重要但辅助的概念。我们将在本节介绍其中的一些。</p>
<h3 id="Correctly-Measuring-Model-Performance-Evaluation-Metrics"><a href="#Correctly-Measuring-Model-Performance-Evaluation-Metrics" class="headerlink" title="Correctly Measuring Model Performance: Evaluation Metrics"></a>Correctly Measuring Model Performance: Evaluation Metrics</h3><p>核心监督训练循环之外最重要的部分是使用模型从未训练过的数据来客观衡量性能。模型使用一个或多个评估指标进行评估。在自然语言处理(NLP)中，存在多种评价指标。最常见的，也是我们将在本章使用的，是准确性。准确性仅仅是在训练过程中未见的数据集上预测正确的部分。</p>
<h3 id="Correctly-Measuring-Model-Performance-Splitting-the-Dataset"><a href="#Correctly-Measuring-Model-Performance-Splitting-the-Dataset" class="headerlink" title="Correctly Measuring Model Performance: Splitting the Dataset"></a>Correctly Measuring Model Performance: Splitting the Dataset</h3><p>一定要记住，最终的目标是很好地概括数据的真实分布。这是什么意思?假设我们能够看到无限数量的数据(“真实/不可见的分布”)，那么存在一个全局的数据分布。显然，我们不能那样做。相反，我们用有限的样本作为训练数据。我们观察有限样本中的数据分布这是真实分布的近似或不完全图像。如果一个模型不仅减少了训练数据中样本的误差，而且减少了来自不可见分布的样本的误差，那么这个模型就比另一个模型具有更好的通用性。当模型致力于降低它在训练数据上的损失时，它可以过度适应并适应那些实际上不是真实数据分布一部分的特性。</p>
<p>要实现这一点，标准实践是将数据集分割为三个随机采样的分区，称为训练、验证和测试数据集，或者进行k-fold交叉验证。分成三个分区是两种方法中比较简单的一种，因为它只需要一次计算。您应该采取预防措施，确保在三个分支之间的类分布保持相同。换句话说，通过类标签聚合数据集，然后将每个由类标签分隔的集合随机拆分为训练、验证和测试数据集，这是一种很好的实践。一个常见的分割百分比是预留70%用于培训，15%用于验证，15%用于测试。不过，这不是一个硬编码的约定。</p>
<p>在某些情况下，可能存在预定义的训练、验证和测试分离;这在用于基准测试任务的数据集中很常见。在这种情况下，重要的是只使用训练数据更新模型参数，在每个epoch结束时使用验证数据测量模型性能，在所有的建模选择被探索并需要报告最终结果之后，只使用测试数据一次。这最后一部分是极其重要的,因为更多的机器学习工程师在玩模型的性能测试数据集,他们是偏向选择测试集上表现得更好。当这种情况发生时,它是不可能知道该模型性能上看不见的数据没有收集更多的数据。</p>
<p>使用k-fold交叉验证的模型评估与使用预定义分割的评估非常相似，但是在此之前还有一个额外的步骤，将整个数据集分割为k个大小相同的fold。其中一个fold保留用于评估，剩下的k-1fold用于训练。通过交换出计算中的哪些fold，可以重复执行此操作。因为有k个fold，每一个fold都有机会成为一个评价fold，并产生一个特定于fold的精度，从而产生k个精度值。最终报告的准确性只是具有标准差的平均值。k-fold评估在计算上是昂贵的，但是对于较小的数据集来说是非常必要的，对于较小的数据集来说，错误的分割可能导致过于乐观(因为测试数据太容易了)或过于悲观(因为测试数据太困难了)。</p>
<h3 id="Knowing-When-to-Stop-Training"><a href="#Knowing-When-to-Stop-Training" class="headerlink" title="Knowing When to Stop Training"></a>Knowing When to Stop Training</h3><p>之前的例子训练了固定次数的模型。虽然这是最简单的方法，但它是任意的和不必要的。正确度量模型性能的一个关键功能是使用该度量来知道何时应该停止训练。最常用的方法是使用启发式方法，称为早期停止(early stopping)。早期停止通过跟踪验证数据集上从一个epoch到另一个epoch的性能并注意性能何时不再改进来的工作。然后，如果业绩继续没有改善，训练将终止。在结束训练之前需要等待的时间称为耐心。一般来说，模型停止改进某些数据集的时间点称为模型收敛的时间点。在实际应用中，我们很少等待模型完全收敛，因为收敛是耗时的，而且会导致过拟合。</p>
<h3 id="Finding-the-Right-Hyperparameters"><a href="#Finding-the-Right-Hyperparameters" class="headerlink" title="Finding the Right Hyperparameters"></a>Finding the Right Hyperparameters</h3><p>我们在前面了解到，参数(或权重)采用优化器针对称为minibatch的固定训练数据子集调整的实际值。超参数是影响模型中参数数量和参数所取值的任何模型设置。有许多不同的选择来决定如何训练模型。这些选择包括选择一个损失函数;优化器;优化器的学习率，如层大小(在第4章中介绍);有等到早停止(early stopping)的耐心;和各种正规化决策(也在第4章讨论)。需要注意的是，这些决策会对模型是否收敛及其性能产生很大影响，你应该系统地探索各种选择点。</p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>深度学习(以及机器学习)中最重要的概念之一是正则化。正则化的概念来源于数值优化理论。回想一下，大多数机器学习算法都在优化损失函数，以找到最可能解释观测结果(即，产生的损失最少)。对于大多数数据集和任务，这个优化问题可能有多个解决方案(可能的模型)。那么我们(或优化器)应该选择哪一个呢?为了形成直观的理解，请考虑图3-3通过一组点拟合曲线的任务。<br><img src="/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/regularize.png" alt="regularize" title="图3-3. 两条曲线都“fit”这些点，但其中一条似乎比另一条更合理。正则化帮助我们选择更合理的解释。(图片来源:维基百科)"></p>
<p>两条曲线都“fit”这些点，但哪一条是不太可能的解释呢?通过求助于奥卡姆剃刀，我们凭直觉知道一个简单的解释比复杂的解释更好。这种机器学习中的平滑约束称为L2正则化。在PyTorch中，您可以通过在优化器中设置weight_decay参数来控制这一点。weight_decay值越大，优化器选择的解释就越流畅;也就是说，L2正则化越强。</p>
<p>除了L2，另一种流行的正则化是L1正则化。L1通常用来鼓励稀疏解;换句话说，大多数模型参数值都接近于零。在第4章中，您将看到一种结构正则化技术，称为“dropout”。模型正则化是一个活跃的研究领域，PyTorch是实现自定义正则化的灵活框架。</p>
<h2 id="Example-Classifying-Sentiment-of-Restaurant-Reviews"><a href="#Example-Classifying-Sentiment-of-Restaurant-Reviews" class="headerlink" title="Example: Classifying Sentiment of Restaurant Reviews"></a>Example: Classifying Sentiment of Restaurant Reviews</h2><p>在上一节中，我们通过一个玩具示例深入研究了有监督的训练，并阐述了许多基本概念。在本节中，我们将重复上述练习，但这次使用的是一个真实的任务和数据集:使用感知器和监督培训对Yelp上的餐馆评论进行分类，判断它们是正面的还是负面的。因为这是本书中第一个完整的NLP示例，所以我们将极其详细地描述辅助数据结构和训练例程。后面几章中的示例将遵循非常相似的模式，因此我们鼓励您仔细遵循本节，并在需要复习时参考它。</p>
<p>在本书的每个示例的开头，我们将描述正在使用的数据集和任务。在这个例子中，我们使用Yelp数据集，它将评论与它们的情感标签(正面或负面)配对。此外，我们还描述了一些数据集操作步骤，这些步骤用于清理数据集并将其划分为训练、验证和测试集。</p>
<p>在理解数据集之后，您将看到定义三个辅助类的模式，这三个类在本书中反复出现，用于将文本数据转换为向量化的形式:词汇表(the Vocabulary)、向量化器(Vectorizer)和PyTorch的DataLoader。词汇表协调我们在“观察和目标编码”中讨论的整数到令牌(token)映射。我们使用一个词汇表将文本标记(text tokens)映射到整数，并将类标签映射到整数。接下来，矢量化器(vectorizer)封装词汇表，并负责接收字符串数据，如审阅文本，并将其转换为将在训练例程中使用的数字向量。我们使用最后一个辅助类，PyTorch的DataLoader，将单个向量化数据点分组并整理成minibatches。</p>
<p>在描述了构成文本向量化小批处理管道(text-to-vectorized-minibatch pipeline)的数据集和辅助类之后，概述了感知器分类器及其训练例程。需要注意的重要一点是，本书中的每个示例的训练例程基本保持不变。我们会在这个例子中更详细地讨论它，因此，我们再次鼓励您使用这个例子作为未来训练例程的参考。我们通过讨论结果来总结这个例子，并深入了解模型学习到了什么。</p>
<h3 id="The-Yelp-Review-Dataset"><a href="#The-Yelp-Review-Dataset" class="headerlink" title="The Yelp Review Dataset"></a>The Yelp Review Dataset</h3><p>2015年，Yelp举办了一场竞赛，要求参与者根据点评预测一家餐厅的评级。同年，Zhang, Zhao，和Lecun(2015)将1星和2星评级转换为“消极”情绪类，将3星和4星评级转换为“积极”情绪类，从而简化了数据集。该数据集分为56万个训练样本和3.8万个测试样本。在这个数据集部分的其余部分中，我们将描述最小化清理数据并导出最终数据集的过程。然后，我们概述了利用PyTorch的数据集类的实现。</p>
<p>在这个例子中，我们使用了简化的Yelp数据集，但是有两个细微的区别。第一个区别是我们使用数据集的“轻量级”版本，它是通过选择10%的训练样本作为完整数据集而派生出来的。这有两个结果:首先，使用一个小数据集可以使训练测试循环快速，因此我们可以快速地进行实验。其次，它生成的模型精度低于使用所有数据。这种低精度通常不是主要问题，因为您可以使用从较小数据集子集中获得的知识对整个数据集进行重新训练。在训练深度学习模型时，这是一个非常有用的技巧，因为在许多情况下，训练数据的数量是巨大的。</p>
<p>从这个较小的子集中，我们将数据集分成三个分区:一个用于训练，一个用于验证，一个用于测试。虽然原始数据集只有两个部分，但是有一个验证集是很重要的。在机器学习中，您经常在数据集的训练部分上训练模型，并且需要一个held-out部分来评估模型的性能。如果模型决策基于held-out部分，那么模型现在不可避免地偏向于更好地执行held-out部分。因为度量增量进度是至关重要的，所以这个问题的解决方案是使用第三个部分，它尽可能少地用于评估。</p>
<p>综上所述，您应该使用数据集的训练部分来派生模型参数，使用数据集的验证部分在超参数之间进行选择(进行建模决策)，使用数据集的测试分区进行最终评估和报告。在例3-11中，我们展示了如何分割数据集。注意，随机种子被设置为一个静态数字，我们首先通过类标签聚合以确保类分布保持不变。</p>
<p>Example 3-11. Creating training, validation, and testing splits<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splitting the subset by rating to create new train, val, and test splits</span></span><br><span class="line">by_rating = collections.defaultdict(list)</span><br><span class="line"><span class="keyword">for</span> _, row <span class="keyword">in</span> review_subset.iterrows():</span><br><span class="line">    by_rating[row.rating].append(row.to_dict())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create split data</span></span><br><span class="line">final_list = []</span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, item_list <span class="keyword">in</span> sorted(by_rating.items()):</span><br><span class="line">    np.random.shuffle(item_list)</span><br><span class="line"></span><br><span class="line">    n_total = len(item_list)</span><br><span class="line">    n_train = int(args.train_proportion * n_total)</span><br><span class="line">    n_val = int(args.val_proportion * n_total)</span><br><span class="line">    n_test = int(args.test_proportion * n_total)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Give data point a split attribute</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> item_list[:n_train]:</span><br><span class="line">        item[<span class="string">'split'</span>] = <span class="string">'train'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> item_list[n_train:n_train+n_val]:</span><br><span class="line">        item[<span class="string">'split'</span>] = <span class="string">'val'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> item_list[n_train+n_val:n_train+n_val+n_test]:</span><br><span class="line">        item[<span class="string">'split'</span>] = <span class="string">'test'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add to final list</span></span><br><span class="line">    final_list.extend(item_list)</span><br><span class="line"></span><br><span class="line">final_reviews = pd.DataFrame(final_list)</span><br></pre></td></tr></table></figure></p>
<p>除了创建一个子集，该子集有三个分区用于训练、验证和测试之外，我们还通过在标点符号周围添加空格和删除并非所有分割都使用标点符号的无关符号来最低限度地清理数据，如示例3-12所示。</p>
<p>Example 3-12. Minimally cleaning the data<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_text</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = text.lower()</span><br><span class="line">    text = re.sub(<span class="string">r"([.,!?])"</span>, <span class="string">r" \1 "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"[^a-zA-Z.,!?]+"</span>, <span class="string">r" "</span>, text)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">final_reviews.review = final_reviews.review.apply(preprocess_text)</span><br></pre></td></tr></table></figure></p>
<h3 id="Understanding-PyTorch’s-Dataset-Representation"><a href="#Understanding-PyTorch’s-Dataset-Representation" class="headerlink" title="Understanding PyTorch’s Dataset Representation"></a>Understanding PyTorch’s Dataset Representation</h3><p>示例3-13中给出的ReviewDataset类假设数据集已被最少地清理并分割为三个部分。特别是，数据集假定它可以基于空白分隔评论，以便获得评论中的令牌(tokens)列表。此外，它假定数据有一个注释，该注释将数据拆分为它所属的部分。需要注意的是，我们使用Python的类方法为这个数据集类指定了入口点方法。我们在整本书中都遵循这个模式。</p>
<p>PyTorch通过提供数据集类为数据集提供了一个抽象。数据集类是一个抽象迭代器。在对新数据集使用PyTorch时，必须首先从数据集类继承子类(或继承)，并实现<strong>getitem</strong>和<strong>len</strong>方法。对于这个例子，我们创建了一个ReviewDataset类，它继承自PyTorch的Dataset类，并实现了两个方法:__getitem<strong>和</strong>len__。通过实现这两种方法，有一个概念上的约定，允许各种PyTorch实用程序使用我们的数据集。在下一节中，我们将介绍其中一个实用程序，特别是DataLoader。下面的实现严重依赖于一个名为ReviewVectorizer的类。在下一节中，我们将描述ReviewVectorizer，但是您可以直观地将其描述为处理从评审文本到表示评审的数字向量的转换的类。神经网络只有通过一定的向量化步骤才能与文本数据进行交互。总体设计模式是实现一个数据集类，它处理一个数据点的向量化逻辑。然后，PyTorch的DataLoader(下一节也将介绍)将通过对数据集进行采样和整理来创建小批数据(minibatch)。</p>
<p>Example 3-13. The ReviewDataset class<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReviewDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, review_df, vectorizer)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            review_df (pandas.DataFrame): the dataset</span></span><br><span class="line"><span class="string">            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.review_df = review_df</span><br><span class="line">        self._vectorizer = vectorizer</span><br><span class="line"></span><br><span class="line">        self.train_df = self.review_df[self.review_df.split==<span class="string">'train'</span>]</span><br><span class="line">        self.train_size = len(self.train_df)</span><br><span class="line"></span><br><span class="line">        self.val_df = self.review_df[self.review_df.split==<span class="string">'val'</span>]</span><br><span class="line">        self.validation_size = len(self.val_df)</span><br><span class="line"></span><br><span class="line">        self.test_df = self.review_df[self.review_df.split==<span class="string">'test'</span>]</span><br><span class="line">        self.test_size = len(self.test_df)</span><br><span class="line"></span><br><span class="line">        self._lookup_dict = &#123;<span class="string">'train'</span>: (self.train_df, self.train_size),</span><br><span class="line">                             <span class="string">'val'</span>: (self.val_df, self.validation_size),</span><br><span class="line">                             <span class="string">'test'</span>: (self.test_df, self.test_size)&#125;</span><br><span class="line"></span><br><span class="line">        self.set_split(<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset_and_make_vectorizer</span><span class="params">(cls, review_csv)</span>:</span></span><br><span class="line">        <span class="string">"""Load dataset and make a new vectorizer from scratch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            review_csv (str): location of the dataset</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            an instance of ReviewDataset</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        review_df = pd.read_csv(review_csv)</span><br><span class="line">        <span class="keyword">return</span> cls(review_df, ReviewVectorizer.from_dataframe(review_df))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vectorizer</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">""" returns the vectorizer """</span></span><br><span class="line">        <span class="keyword">return</span> self._vectorizer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_split</span><span class="params">(self, split=<span class="string">"train"</span>)</span>:</span></span><br><span class="line">        <span class="string">""" selects the splits in the dataset using a column in the dataframe</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            split (str): one of "train", "val", or "test"</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._target_split = split</span><br><span class="line">        self._target_df, self._target_size = self._lookup_dict[split]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._target_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""the primary entry point method for PyTorch datasets</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            index (int): the index to the data point</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            a dict of the data point's features (x_data) and label (y_target)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        row = self._target_df.iloc[index]</span><br><span class="line"></span><br><span class="line">        review_vector = \</span><br><span class="line">            self._vectorizer.vectorize(row.review)</span><br><span class="line"></span><br><span class="line">        rating_index = \</span><br><span class="line">            self._vectorizer.rating_vocab.lookup_token(row.rating)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'x_data'</span>: review_vector,</span><br><span class="line">                <span class="string">'y_target'</span>: rating_index&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_num_batches</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="string">"""Given a batch size, return the number of batches in the dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            batch_size (int)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            number of batches in the dataset</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> len(self) // batch_size</span><br></pre></td></tr></table></figure></p>
<h3 id="The-Vocabulary-the-Vectorizer-and-the-DataLoader"><a href="#The-Vocabulary-the-Vectorizer-and-the-DataLoader" class="headerlink" title="The Vocabulary, the Vectorizer, and the DataLoader"></a>The Vocabulary, the Vectorizer, and the DataLoader</h3><p>词汇表、向量化器和DataLoader是三个类，我们几乎在本书的每个示例中都使用它们来执行一个关键的管道:将文本输入转换为向量化的小批(minibatch)。管道从预处理文本开始;每个数据点都是令牌的集合。在本例中，令牌碰巧是单词，但是正如您将在第4章和第6章中看到的，令牌也可以是字符。以下小节中提供的三个类负责将每个令牌映射到一个整数，将此映射应用到每个数据点，以创建一个向量化表单，然后将向量化数据点分组到模型的一个小批处理中。</p>
<p>VOCABULARY<br>从文本到向量化的minibatch处理的第一步是将每个令牌(tokens)映射到其自身的数字版本。标准的方法是在令牌(tokens)和整数之间有一个双向映射(可以反向映射)。在Python中，这只是两个字典。我们将这个词封装到词汇表类中，如示例3-14所示。词汇表类不仅管理这个bijection—允许用户添加新的令牌并使索引自动递增—而且还处理一个名为UNK.UNK的特殊令牌，它代表“未知”令牌。通过使用UNK令牌，我们可以在测试期间处理训练中从未见过的令牌;例如，您可能会遇到一个以前从未见过的单词。正如我们将在接下来的矢量化器中看到的，我们甚至将显式地限制词汇表中不经常出现的令牌，以便在我们的训练例程中有UNK令牌。这对于限制词汇表类使用的内存非常重要。预期的行为是调用add_token向词汇表中添加新的令牌，检索令牌索引时调用lookup_token，检索特定索引对应的令牌时调用lookup_index。</p>
<p>Example 3-14. The Vocabulary class<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocabulary</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Class to process text and extract vocabulary for mapping"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, token_to_idx=None, add_unk=True, unk_token=<span class="string">"&lt;UNK&gt;"</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token_to_idx (dict): a pre-existing map of tokens to indices</span></span><br><span class="line"><span class="string">            add_unk (bool): a flag that indicates whether to add the UNK token</span></span><br><span class="line"><span class="string">            unk_token (str): the UNK token to add into the Vocabulary</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_to_idx <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">&gt;           token_to_idx = &#123;&#125;</span><br><span class="line">        self._token_to_idx = token_to_idx</span><br><span class="line"></span><br><span class="line">        self._idx_to_token = &#123;idx: token</span><br><span class="line">                              <span class="keyword">for</span> token, idx <span class="keyword">in</span> self._token_to_idx.items()&#125;</span><br><span class="line"></span><br><span class="line">        self._add_unk = add_unk</span><br><span class="line">        self._unk_token = unk_token</span><br><span class="line"></span><br><span class="line">        self.unk_index = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">if</span> add_unk:</span><br><span class="line">            self.unk_index = self.add_token(unk_token)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_serializable</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">""" returns a dictionary that can be serialized """</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'token_to_idx'</span>: self._token_to_idx,</span><br><span class="line">                <span class="string">'add_unk'</span>: self._add_unk,</span><br><span class="line">                <span class="string">'unk_token'</span>: self._unk_token&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_serializable</span><span class="params">(cls, contents)</span>:</span></span><br><span class="line">        <span class="string">""" instantiates the Vocabulary from a serialized dictionary """</span></span><br><span class="line">        <span class="keyword">return</span> cls(**contents)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_token</span><span class="params">(self, token)</span>:</span></span><br><span class="line">        <span class="string">"""Update mapping dicts based on the token.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token (str): the item to add into the Vocabulary</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            index (int): the integer corresponding to the token</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">in</span> self._token_to_idx:</span><br><span class="line">            index = self._token_to_idx[token]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            index = len(self._token_to_idx)</span><br><span class="line">            self._token_to_idx[token] = index</span><br><span class="line">            self._idx_to_token[index] = token</span><br><span class="line">        <span class="keyword">return</span> index</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lookup_token</span><span class="params">(self, token)</span>:</span></span><br><span class="line">        <span class="string">"""Retrieve the index associated with the token</span></span><br><span class="line"><span class="string">          or the UNK index if token isn't present.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token (str): the token to look up</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            index (int): the index corresponding to the token</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            `unk_index` needs to be &gt;=0 (having been added into the Vocabulary)</span></span><br><span class="line"><span class="string">              for the UNK functionality</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.add_unk:</span><br><span class="line">            <span class="keyword">return</span> self._token_to_idx.get(token, self.unk_index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._token_to_idx[token]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lookup_index</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""Return the token associated with the index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            index (int): the index to look up</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            token (str): the token corresponding to the index</span></span><br><span class="line"><span class="string">        Raises:</span></span><br><span class="line"><span class="string">            KeyError: if the index is not in the Vocabulary</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> index <span class="keyword">not</span> <span class="keyword">in</span> self._idx_to_token:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">"the index (%d) is not in the Vocabulary"</span> % index)</span><br><span class="line">        <span class="keyword">return</span> self._idx_to_token[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"&lt;Vocabulary(size=%d)&gt;"</span> % len(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._token_to_idx)</span><br></pre></td></tr></table></figure></p>
<p>VECTORIZER<br>从文本数据集到向量化的小批处理的第二个阶段是迭代输入数据点的令牌，并将每个令牌转换为其整数形式。这个迭代的结果应该是一个向量。由于这个向量将与来自其他数据点的向量组合，因此有一个约束条件，即由矢量化器生成的向量应该始终具有相同的长度。</p>
<p>为了实现这些目标，Vectorizer类封装了评审词汇表，它将评审中的单词映射到整数。在示例3-15中，矢量化器为from_dataframe方法使用Python的classmethod装饰器来指示实例化矢量化器的入口点。from_dataframe方法在panda dataframe的行上迭代，有两个目标。第一个目标是计算数据集中出现的所有令牌的频率。第二个目标是创建一个词汇表，该词汇表只使用与方法截止提供的关键字参数一样频繁的令牌。有效地，这种方法是找到所有至少出现截止时间的单词，并将它们添加到词汇表中。由于还将UNK令牌添加到词汇表中，因此在调用词汇表的lookup_令牌方法时，未添加的任何单词都将具有unk_index。</p>
<p>方法向量化封装了向量化器的核心功能。它以表示评审的字符串作为参数，并返回评审的向量化表示。在这个例子中，我们使用在第1章中介绍的折叠的onehot表示。这种表示方式创建了一个二进制向量——一个包含1和0的向量——它的长度等于词汇表的大小。二进制向量在与复习中的单词对应的位置有1。注意，这种表示有一些限制。首先，它是稀疏的——复习中惟一单词的数量总是远远少于词汇表中惟一单词的数量。第二，它抛弃了单词在评论中出现的顺序(“bag of words”)。在后面的章节中，您将看到其他没有这些限制的方法。</p>
<p>Example 3-15. The Vectorizer class<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReviewDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, review_df, vectorizer)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            review_df (pandas.DataFrame): the dataset</span></span><br><span class="line"><span class="string">            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.review_df = review_df</span><br><span class="line">        self._vectorizer = vectorizer</span><br><span class="line"></span><br><span class="line">        self.train_df = self.review_df[self.review_df.split==<span class="string">'train'</span>]</span><br><span class="line">        self.train_size = len(self.train_df)</span><br><span class="line"></span><br><span class="line">        self.val_df = self.review_df[self.review_df.split==<span class="string">'val'</span>]</span><br><span class="line">        self.validation_size = len(self.val_df)</span><br><span class="line"></span><br><span class="line">        self.test_df = self.review_df[self.review_df.split==<span class="string">'test'</span>]</span><br><span class="line">        self.test_size = len(self.test_df)</span><br><span class="line"></span><br><span class="line">        self._lookup_dict = &#123;<span class="string">'train'</span>: (self.train_df, self.train_size),</span><br><span class="line">                             <span class="string">'val'</span>: (self.val_df, self.validation_size),</span><br><span class="line">                             <span class="string">'test'</span>: (self.test_df, self.test_size)&#125;</span><br><span class="line"></span><br><span class="line">        self.set_split(<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset_and_make_vectorizer</span><span class="params">(cls, review_csv)</span>:</span></span><br><span class="line">        <span class="string">"""Load dataset and make a new vectorizer from scratch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            review_csv (str): location of the dataset</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            an instance of ReviewDataset</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        review_df = pd.read_csv(review_csv)</span><br><span class="line">        train_review_df = review_df[review_df.split==<span class="string">'train'</span>]</span><br><span class="line">        <span class="keyword">return</span> cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset_and_load_vectorizer</span><span class="params">(cls, review_csv, vectorizer_filepath)</span>:</span></span><br><span class="line">        <span class="string">"""Load dataset and the corresponding vectorizer.</span></span><br><span class="line"><span class="string">        Used in the case in the vectorizer has been cached for re-use</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            review_csv (str): location of the dataset</span></span><br><span class="line"><span class="string">            vectorizer_filepath (str): location of the saved vectorizer</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            an instance of ReviewDataset</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        review_df = pd.read_csv(review_csv)</span><br><span class="line">        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)</span><br><span class="line">        <span class="keyword">return</span> cls(review_df, vectorizer)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_vectorizer_only</span><span class="params">(vectorizer_filepath)</span>:</span></span><br><span class="line">        <span class="string">"""a static method for loading the vectorizer from file</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vectorizer_filepath (str): the location of the serialized vectorizer</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            an instance of ReviewVectorizer</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">with</span> open(vectorizer_filepath) <span class="keyword">as</span> fp:</span><br><span class="line">            <span class="keyword">return</span> ReviewVectorizer.from_serializable(json.load(fp))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_vectorizer</span><span class="params">(self, vectorizer_filepath)</span>:</span></span><br><span class="line">        <span class="string">"""saves the vectorizer to disk using json</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vectorizer_filepath (str): the location to save the vectorizer</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">with</span> open(vectorizer_filepath, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            json.dump(self._vectorizer.to_serializable(), fp)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vectorizer</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">""" returns the vectorizer """</span></span><br><span class="line">        <span class="keyword">return</span> self._vectorizer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_split</span><span class="params">(self, split=<span class="string">"train"</span>)</span>:</span></span><br><span class="line">        <span class="string">"""selects the splits in the dataset using a column in the dataframe"""</span></span><br><span class="line">        self._target_split = split</span><br><span class="line">        self._target_df, self._target_size = self._lookup_dict[split]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._target_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""the primary entry point method for PyTorch datasets</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            index (int): the index to the data point</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            a dict of the data point's features (x_data) and label (y_target)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        row = self._target_df.iloc[index]</span><br><span class="line"></span><br><span class="line">        review_vector = \</span><br><span class="line">            self._vectorizer.vectorize(row.review)</span><br><span class="line"></span><br><span class="line">        rating_index = \</span><br><span class="line">            self._vectorizer.rating_vocab.lookup_token(row.rating)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'x_data'</span>: review_vector,</span><br><span class="line">                <span class="string">'y_target'</span>: rating_index&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_num_batches</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="string">"""Given a batch size, return the number of batches in the dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            batch_size (int)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            number of batches in the dataset</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> len(self) // batch_size</span><br></pre></td></tr></table></figure></p>
<p>DATALOADER</p>
<p>文本向矢量化的小批处理的最后一个阶段是对向矢量化的数据点进行分组。因为分组成小批是训练神经网络的重要部分，所以PyTorch提供了一个名为DataLoader的内置类来协调这个过程。DataLoader类通过提供一个PyTorch数据集(例如为本例定义的ReviewDataset)、一个batch_size和一些其他关键字参数来实例化。得到的对象是一个Python迭代器，它对数据集.19中提供的数据点进行分组和整理。在示例3-16中，我们将DataLoader包装在generate_batch()函数中，该函数是一个生成器，用于方便地在CPU和GPU之间切换数据。</p>
<p>Example 3-16. The generate_batches function<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batches</span><span class="params">(dataset, batch_size, shuffle=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                     drop_last=True, device=<span class="string">"cpu"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A generator function which wraps the PyTorch DataLoader. It will</span></span><br><span class="line"><span class="string">      ensure each tensor is on the write device location.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,</span><br><span class="line">                            shuffle=shuffle, drop_last=drop_last)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data_dict <span class="keyword">in</span> dataloader:</span><br><span class="line">        out_data_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> name, tensor <span class="keyword">in</span> data_dict.items():</span><br><span class="line">            out_data_dict[name] = data_dict[name].to(device)</span><br><span class="line">        <span class="keyword">yield</span> out_data_dict</span><br></pre></td></tr></table></figure></p>
<h3 id="A-Perceptron-Classifier"><a href="#A-Perceptron-Classifier" class="headerlink" title="A Perceptron Classifier"></a>A Perceptron Classifier</h3><p>我们在这里使用的模型是我们在本章开头展示的感知器的重新实现。ReviewClassifier继承自PyTorch的模块，并创建具有单个输出的单个线性层。因为这是一个二元分类设置(消极或积极的审查)，所以这是一个适当的设置。最终的非线性函数为sigmoid函数。</p>
<p>我们对forward方法进行参数化，以允许可选地应用sigmoid函数。要理解其中的原因，首先需要指出的是，在二元分类任务中，二元交叉熵损失(torch.nn.BCELoss)是最合适的损失函数。它是用数学公式表示二进制概率的。然而，应用一个乙状结肠然后使用这个损失函数存在数值稳定性问题。为了给用户提供更稳定的快捷方式，PyTorch提供了BCEWithLogitsLoss。要使用这个损失函数，输出不应该应用sigmoid函数。因此，在默认情况下，我们不应用sigmoid。但是，如果分类器的用户希望得到一个概率值，则需要使用sigmoid，并将其作为选项保留。在示例3-17的结果部分中，我们看到了以这种方式使用它的示例。</p>
<p>Example 3-17. The Perceptron classifier<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReviewClassifier</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" a simple perceptron based classifier """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            num_features (int): the size of the input feature vector</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ReviewClassifier, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(in_features=num_features,</span><br><span class="line">                             out_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in, apply_sigmoid=False)</span>:</span></span><br><span class="line">        <span class="string">"""The forward pass of the classifier</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x_in (torch.Tensor): an input data tensor.</span></span><br><span class="line"><span class="string">                x_in.shape should be (batch, num_features)</span></span><br><span class="line"><span class="string">            apply_sigmoid (bool): a flag for the sigmoid activation</span></span><br><span class="line"><span class="string">                should be false if used with the Cross Entropy losses</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the resulting tensor. tensor.shape should be (batch,)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        y_out = self.fc1(x_in).squeeze()</span><br><span class="line">        <span class="keyword">if</span> apply_sigmoid:</span><br><span class="line">            y_out = F.sigmoid(y_out)</span><br><span class="line">        <span class="keyword">return</span> y_out</span><br></pre></td></tr></table></figure></p>
<h3 id="The-Training-Routine"><a href="#The-Training-Routine" class="headerlink" title="The Training Routine"></a>The Training Routine</h3><p>在本节中，我们将概述训练例程的组件，以及它们如何与数据集和模型结合来调整模型参数并提高其性能。在其核心，训练例程负责实例化模型，在数据集上迭代，在给定数据作为输入时计算模型的输出，计算损失(模型的错误程度)，并根据损失比例更新模型。虽然这可能看起来有很多细节需要管理，但是改变培训常规的地方并不多，因此，在您的深度学习开发过程中，这将成为一种习惯。为了帮助管理高层决策，我们使用args对象集中协调所有决策点，您可以在示例3-18中看到。</p>
<p>Example 3-18. The args for classifying yelp reviews<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> Namespace</span><br><span class="line"></span><br><span class="line">args = Namespace(</span><br><span class="line">    <span class="comment"># Data and Path information</span></span><br><span class="line">    frequency_cutoff=<span class="number">25</span>,</span><br><span class="line">    model_state_file=<span class="string">'model.pth'</span>,</span><br><span class="line">    review_csv=<span class="string">'data/yelp/reviews_with_splits_lite.csv'</span>,</span><br><span class="line">    save_dir=<span class="string">'model_storage/ch3/yelp/'</span>,</span><br><span class="line">    vectorizer_file=<span class="string">'vectorizer.json'</span>,</span><br><span class="line">    <span class="comment"># No Model hyper parameters</span></span><br><span class="line">    <span class="comment"># Training hyper parameters</span></span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    early_stopping_criteria=<span class="number">5</span>,</span><br><span class="line">    learning_rate=<span class="number">0.001</span>,</span><br><span class="line">    num_epochs=<span class="number">100</span>,</span><br><span class="line">    seed=<span class="number">1337</span>,</span><br><span class="line">    <span class="comment"># ...  runtime options omitted for space</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>在本节的其余部分中，我们首先描述训练状态，这是一个用于跟踪关于训练过程的信息的小字典。当您跟踪关于训练例程的更多细节时，这个字典将会增长，如果您选择这样做，您可以系统化它，但是在下一个示例中给出的字典是您将在模型训练期间跟踪的基本信息集。在描述了训练状态之后，我们将概述为要执行的模型训练实例化的对象集。这包括模型本身、数据集、优化器和损失函数。在其他示例和补充材料中，我们包含了其他组件，但为了简单起见，我们不在文本中列出它们。最后，我们用训练循环本身结束本节，并演示标准PyTorch优化模式。</p>
<p>SETTING THE STAGE FOR THE TRAINING TO BEGIN<br>示例3-19展示了我们为这个示例实例化的训练组件。第一项是初始训练状态。该函数接受args对象作为参数，以便训练状态能够处理复杂的信息，但是在本书的文本中，我们没有展示这些复杂性。我们建议您参考补充材料，看看您在培训状态下还可以使用哪些额外的东西。这里显示的最小集包括训练损失、训练精度、验证损失和验证精度的epoch索引和列表。它还包括测试损失和测试精度两个字段。</p>
<p>接下来要实例化的两个项目是数据集和模型。在本例中，以及本书其余部分的示例中，我们将数据集设计为负责实例化向量化器。在补充材料中，数据集实例化嵌套在一个if语句中，该if语句允许加载以前实例化的矢量化器，或者一个新的实例化，该实例化器也将保存到磁盘。重要的是，通过协调用户的意愿(通过args.cuda)和检查GPU设备是否确实可用的条件，将模型移动到正确的设备。目标设备用于核心训练循环中的generate_batch函数调用，以便数据和模型将位于相同的设备位置。</p>
<p>初始实例化中的最后两项是loss函数和优化器。本例中使用的损失函数是bcewithlogits损失。要更详细地解释为什么使用这种损失，请参考“感知器分类器”，它描述了模型。简而言之,最合适的损失函数的二元分类是二进制交叉熵(BCE)损失和更数值稳定对BCEWithLogitsLoss的模型不适用乙状结肠函数输出比一对BCELoss模型,并应用sigmoid函数的输出。我们使用的优化器是Adam优化器。一般来说，Adam与其他优化器相比具有很强的竞争力，在撰写本文时，还没有令人信服的证据表明可以使用任何其他优化器来替代Adam。我们鼓励您通过尝试其他优化器并注意性能来验证这一点。</p>
<p>Example 3-19. Instantiating the dataset, model, loss, optimizer, and training state<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_train_state</span><span class="params">(args)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'epoch_index'</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">'train_loss'</span>: [],</span><br><span class="line">            <span class="string">'train_acc'</span>: [],</span><br><span class="line">            <span class="string">'val_loss'</span>: [],</span><br><span class="line">            <span class="string">'val_acc'</span>: [],</span><br><span class="line">            <span class="string">'test_loss'</span>: <span class="number">-1</span>,</span><br><span class="line">            <span class="string">'test_acc'</span>: <span class="number">-1</span>&#125;</span><br><span class="line">train_state = make_train_state(args)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> torch.cuda.is_available():</span><br><span class="line">    args.cuda = <span class="keyword">False</span></span><br><span class="line">args.device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> args.cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset and vectorizer</span></span><br><span class="line">dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)</span><br><span class="line">vectorizer = dataset.get_vectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))</span><br><span class="line">classifier = classifier.to(args.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss and optimizer</span></span><br><span class="line">loss_func = nn.BCEWithLogitsLoss()</span><br><span class="line">optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)</span><br></pre></td></tr></table></figure></p>
<p>THE TRAINING LOOP<br>训练循环使用来自初始实例化的对象来更新模型参数，以便随着时间的推移进行改进。更具体地说，训练循环由两个循环组成:一个内循环覆盖数据集中的小批处理，另一个外循环重复内循环若干次。在内部循环中，计算每个小批处理的损失，并使用优化器更新模型参数。</p>
<p>为了更全面地介绍所发生的事情，让我们从下面的代码片段顶部开始。在第一行中，我们使用for循环，它的范围跨越各个epochs。epochs的数量是一个可以设置的超参数。它控制训练例程应该对数据集进行多少次传递。在实践中，您应该使用类似于早期停止标准的东西来在循环结束之前终止它。在补充资料中，我们向您展示了如何做到这一点。</p>
<p>在for循环的顶部，有几个例程定义和实例化。首先设置训练状态的epoch索引，然后设置数据集的分割(首先是“train”，然后是“val”，当我们想在epoch结束时测量模型性能，最后是“test”，当我们想评估模型的最终性能)。考虑到我们是如何构造数据集的，应该总是在调用generate_batch()之前设置拆分。创建batch_generator之后，将实例化两个浮动，以跟踪批处理之间的损失和准确性。有关这里使用的“运行平均公式”的更多细节，请参阅Wikipedia的“moving average”页面。最后，调用分类器的.train()方法，表示模型处于“训练模式”，模型参数是可变的。这也支持像drop这样的正则化机制。</p>
<p>训练循环的下一部分是迭代batch_generator中的训练批，并执行更新模型参数的基本操作。在每个批处理迭代中，首先使用optimizer.zero_grad()方法重置优化器的梯度。然后，从模型中计算输出。接下来，loss函数用于计算模型输出与监督目标(真正的类标签)之间的损失。在此之后，对loss对象(而不是loss function对象)调用loss.backward()方法，导致梯度传播到每个参数。最后，优化器使用这些传播的梯度来使用optimizer.step()方法执行参数更新。这五个步骤是梯度下降的基本步骤。除此之外，还有一些用于记帐和跟踪的额外操作。具体来说，损失和精度值(作为常规Python变量存储)被计算出来，然后用于更新运行损失和运行精度变量。</p>
<p>在训练分割批处理的内部循环之后，有两个簿记和实例化操作。具体来说，首先用最终的损失和精度值更新训练状态。然后，创建一个新的批处理生成器、运行损失和运行精度。验证数据的循环几乎与培训数据相同，因此重用相同的变量。有一个主要的区别:调用分类器的.eval()方法，它执行与分类器的.train()方法相反的操作。eval()方法使模型参数不可变，且不可丢失。eval模式还禁止计算梯度的损失并将其传播回参数。这很重要，因为我们不希望模型根据验证数据调整参数。相反，我们希望这些数据作为模型执行情况的度量。如果其测量性能之间存在着很大的差异在训练数据和验证数据的测量性能,很可能模型过度拟合训练数据,你应该调整模型或培训程序(比如设置阻止早期,我们使用补充笔记本对于这个例子)。</p>
<p>在对验证数据进行迭代并保存由此产生的验证损失和精度值之后，外部for循环就完成了。我们在本书中实现的每个训练例程都将遵循非常相似的设计模式。事实上，所有梯度下降算法都遵循相似的设计模式。在您习惯了从头开始编写这个循环之后，您将会学会如何使用它执行梯度下降!示例3-20给出了代码。</p>
<p>Example 3-20. A bare-bones training loop<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch_index <span class="keyword">in</span> range(args.num_epochs):</span><br><span class="line">    train_state[<span class="string">'epoch_index'</span>] = epoch_index</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate over training dataset</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># setup: batch generator, set loss and acc to 0, set train mode on</span></span><br><span class="line">    dataset.set_split(<span class="string">'train'</span>)</span><br><span class="line">    batch_generator = generate_batches(dataset,</span><br><span class="line">                                       batch_size=args.batch_size,</span><br><span class="line">                                       device=args.device)</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    running_acc = <span class="number">0.0</span></span><br><span class="line">    classifier.train()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_index, batch_dict <span class="keyword">in</span> enumerate(batch_generator):</span><br><span class="line">        <span class="comment"># the training routine is 5 steps:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 1. zero the gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 2. compute the output</span></span><br><span class="line">        y_pred = classifier(x_in=batch_dict[<span class="string">'x_data'</span>].float())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 3. compute the loss</span></span><br><span class="line">        loss = loss_func(y_pred, batch_dict[<span class="string">'y_target'</span>].float())</span><br><span class="line">        loss_batch = loss.item()</span><br><span class="line">        running_loss += (loss_batch - running_loss) / (batch_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 4. use loss to produce gradients</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 5. use optimizer to take gradient step</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -----------------------------------------</span></span><br><span class="line">        <span class="comment"># compute the accuracy</span></span><br><span class="line">        acc_batch = compute_accuracy(y_pred, batch_dict[<span class="string">'y_target'</span>])</span><br><span class="line">        running_acc += (acc_batch - running_acc) / (batch_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    train_state[<span class="string">'train_loss'</span>].append(running_loss)</span><br><span class="line">    train_state[<span class="string">'train_acc'</span>].append(running_acc)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate over val dataset</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># setup: batch generator, set loss and acc to 0; set eval mode on</span></span><br><span class="line">    dataset.set_split(<span class="string">'val'</span>)</span><br><span class="line">    batch_generator = generate_batches(dataset,</span><br><span class="line">                                       batch_size=args.batch_size,</span><br><span class="line">                                       device=args.device)</span><br><span class="line">    running_loss = <span class="number">0.</span></span><br><span class="line">    running_acc = <span class="number">0.</span></span><br><span class="line">    classifier.eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_index, batch_dict <span class="keyword">in</span> enumerate(batch_generator):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 1. compute the output</span></span><br><span class="line">        y_pred = classifier(x_in=batch_dict[<span class="string">'x_data'</span>].float())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 2. compute the loss</span></span><br><span class="line">        loss = loss_func(y_pred, batch_dict[<span class="string">'y_target'</span>].float())</span><br><span class="line">        loss_batch = loss.item()</span><br><span class="line">        running_loss += (loss_batch - running_loss) / (batch_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 3. compute the accuracy</span></span><br><span class="line">        acc_batch = compute_accuracy(y_pred, batch_dict[<span class="string">'y_target'</span>])</span><br><span class="line">        running_acc += (acc_batch - running_acc) / (batch_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    train_state[<span class="string">'val_loss'</span>].append(running_loss)</span><br><span class="line">    train_state[<span class="string">'val_acc'</span>].append(running_acc)</span><br></pre></td></tr></table></figure></p>
<h3 id="Evaluation-Inference-and-Inspection"><a href="#Evaluation-Inference-and-Inspection" class="headerlink" title="Evaluation, Inference, and Inspection"></a>Evaluation, Inference, and Inspection</h3><p>在您有了一个经过训练的模型之后，接下来的步骤是要么评估它是如何处理一些保留下来的数据的，要么使用它对新数据进行推断，要么检查模型的权重，看看它学到了什么。在本节中，我们将向您展示所有三个步骤。</p>
<p>EVALUATING ON TEST DATA<br>为了评估外置测试集上的数据，代码与我们在上一个示例中看到的训练例程中的验证循环完全相同，但有一个细微的区别:分割设置为“test”而不是“val”。数据集的两个分区之间的区别在于，测试集应该尽可能少地运行。每次您在测试集上运行一个训练过的模型，做出一个新的模型决策(例如改变层的大小)，并在测试集上重新测量新的再训练模型时，您都是在向测试数据倾斜您的建模决策。换句话说，如果您足够频繁地重复这个过程，那么测试集作为真正交付数据的精确度量将变得毫无意义。示例3-21对此进行了更深入的研究。</p>
<p>Example 3-21. Test set evaluation<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]</span><br><span class="line">dataset.set_split(<span class="string">'test'</span>)</span><br><span class="line">batch_generator = generate_batches(dataset,</span><br><span class="line">                                   batch_size=args.batch_size,</span><br><span class="line">                                   device=args.device)</span><br><span class="line">running_loss = <span class="number">0.</span></span><br><span class="line">running_acc = <span class="number">0.</span></span><br><span class="line">classifier.eval()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_index, batch_dict <span class="keyword">in</span> enumerate(batch_generator):</span><br><span class="line">    <span class="comment"># compute the output</span></span><br><span class="line">    y_pred = classifier(x_in=batch_dict[<span class="string">'x_data'</span>].float())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the loss</span></span><br><span class="line">    loss = loss_func(y_pred, batch_dict[<span class="string">'y_target'</span>].float())</span><br><span class="line">    loss_batch = loss.item()</span><br><span class="line">    running_loss += (loss_batch - running_loss) / (batch_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the accuracy</span></span><br><span class="line">    acc_batch = compute_accuracy(y_pred, batch_dict[<span class="string">'y_target'</span>])</span><br><span class="line">    running_acc += (acc_batch - running_acc) / (batch_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">train_state[<span class="string">'test_loss'</span>] = running_loss</span><br><span class="line">train_state[<span class="string">'test_acc'</span>] = running_acc</span><br><span class="line">Input[<span class="number">1</span>]</span><br><span class="line">print(<span class="string">"Test loss: &#123;:.3f&#125;"</span>.format(train_state[<span class="string">'test_loss'</span>]))</span><br><span class="line">print(<span class="string">"Test Accuracy: &#123;:.2f&#125;"</span>.format(train_state[<span class="string">'test_acc'</span>]))</span><br><span class="line">Output[<span class="number">1</span>]</span><br><span class="line">Test loss: <span class="number">0.297</span></span><br><span class="line">Test Accuracy: <span class="number">90.55</span></span><br></pre></td></tr></table></figure></p>
<p>INFERENCE AND CLASSIFYING NEW DATA POINTS<br>评价模型的另一种方法是对新数据进行推断，并对模型是否有效进行定性判断。我们可以在示例3-22中看到这一点。</p>
<p>Example 3-22. Printing the prediction for a sample review<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Input[<span class="number">0</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rating</span><span class="params">(review, classifier, vectorizer,</span></span></span><br><span class="line"><span class="function"><span class="params">                   decision_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Predict the rating of a review</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        review (str): the text of the review</span></span><br><span class="line"><span class="string">        classifier (ReviewClassifier): the trained model</span></span><br><span class="line"><span class="string">        vectorizer (ReviewVectorizer): the corresponding vectorizer</span></span><br><span class="line"><span class="string">        decision_threshold (float): The numerical boundary which</span></span><br><span class="line"><span class="string">            separates the rating classes</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    review = preprocess_text(review)</span><br><span class="line">    vectorized_review = torch.tensor(vectorizer.vectorize(review))</span><br><span class="line">    result = classifier(vectorized_review.view(<span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">    probability_value = F.sigmoid(result).item()</span><br><span class="line"></span><br><span class="line">    index =  <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> probability_value &lt; decision_threshold:</span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vectorizer.rating_vocab.lookup_index(index)</span><br><span class="line"></span><br><span class="line">test_review = <span class="string">"this is a pretty awesome book"</span></span><br><span class="line">prediction = predict_rating(test_review, classifier, vectorizer)</span><br><span class="line">print(<span class="string">"&#123;&#125; -&gt; &#123;&#125;"</span>.format(test_review, prediction)</span><br><span class="line">Output[<span class="number">0</span>]</span><br><span class="line">this is a pretty awesome book -&gt; positive</span><br></pre></td></tr></table></figure></p>
<p>INSPECTING MODEL WEIGHTS<br>最后，了解模型在完成训练后是否表现良好的最后一种方法是检查权重，并对权重是否正确做出定性判断。如示例3-23所示，使用感知器和压缩的onehot编码，这是一种相当简单的方法，因为每个模型的权重与词汇表中的单词完全对应。</p>
<p>Example 3-23. Print words corresponding to the classifier’s weights<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">Input[0]</span><br><span class="line"># Sort weights</span><br><span class="line">fc1_weights = classifier.fc1.weight.detach()[0]</span><br><span class="line">_, indices = torch.sort(fc1_weights, dim=0, descending=True)</span><br><span class="line">indices = indices.numpy().tolist()</span><br><span class="line"></span><br><span class="line"># Top 20 words</span><br><span class="line">print(&quot;Influential words in Positive Reviews:&quot;)</span><br><span class="line">print(&quot;--------------------------------------&quot;)</span><br><span class="line">for i in range(20):</span><br><span class="line">    print(vectorizer.review_vocab.lookup_index(indices[i]))</span><br><span class="line">Output[0]</span><br><span class="line">Influential words in Positive Reviews:</span><br><span class="line">--------------------------------------</span><br><span class="line">great</span><br><span class="line">awesome</span><br><span class="line">amazing</span><br><span class="line">love</span><br><span class="line">friendly</span><br><span class="line">delicious</span><br><span class="line">best</span><br><span class="line">excellent</span><br><span class="line">definitely</span><br><span class="line">perfect</span><br><span class="line">fantastic</span><br><span class="line">wonderful</span><br><span class="line">vegas</span><br><span class="line">favorite</span><br><span class="line">loved</span><br><span class="line">yummy</span><br><span class="line">fresh</span><br><span class="line">reasonable</span><br><span class="line">always</span><br><span class="line">recommend</span><br><span class="line">Input[1]</span><br><span class="line"># Top 20 negative words</span><br><span class="line">print(&quot;Influential words in Negative Reviews:&quot;)</span><br><span class="line">print(&quot;--------------------------------------&quot;)</span><br><span class="line">indices.reverse()</span><br><span class="line">for i in range(20):</span><br><span class="line">    print(vectorizer.review_vocab.lookup_index(indices[i]))</span><br><span class="line">Output[1]</span><br><span class="line">Influential words in Negative Reviews:</span><br><span class="line">--------------------------------------</span><br><span class="line">worst</span><br><span class="line">horrible</span><br><span class="line">mediocre</span><br><span class="line">terrible</span><br><span class="line">not</span><br><span class="line">rude</span><br><span class="line">bland</span><br><span class="line">disgusting</span><br><span class="line">dirty</span><br><span class="line">awful</span><br><span class="line">poor</span><br><span class="line">disappointing</span><br><span class="line">ok</span><br><span class="line">no</span><br><span class="line">overpriced</span><br><span class="line">sorry</span><br><span class="line">nothing</span><br><span class="line">meh</span><br><span class="line">manager</span><br><span class="line">gross</span><br></pre></td></tr></table></figure></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在这一章中，你学习了监督神经网络训练的一些基本概念:</p>
<ol>
<li>最简单的神经网络模型，感知器</li>
<li>基本概念如激活函数、损失函数及其不同种类</li>
<li>在一个玩具示例的上下文中，训练循环、批大小和时间</li>
<li>泛化是什么意思，以及使用训练/测试/验证分割来衡量泛化性能的良好实践</li>
<li>早期停止等准则来确定训练算法的端点或收敛性<br>什么是超参数和他们的一些例子，如批大小，学习率等等</li>
<li>如何使用PyTorch实现的感知器模型对英文Yelp餐厅评论进行分类，如何通过检验权重来解释该模型</li>
</ol>
<p>在第4章中，我们介绍了前馈网络，首先在不起眼的感知器模型的基础上，通过纵向和横向叠加来建立前馈网络，从而得到多层感知器模型。我们还研究了一种新的基于卷积运算的前馈网络来捕获语言子结构。</p>

      
    </div>
    
    
    

	<div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/">Natural-Language-Processing-with-PyTorch（三）</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 Yif Du 的个人博客">Yif Du</a></p>
  <p><span>发布时间:</span>2018年12月19日 - 14:12</p>
  <p><span>最后更新:</span>2018年12月20日 - 12:12</p>
  <p><span>原始链接:</span><a href="/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/" title="Natural-Language-Processing-with-PyTorch（三）">http://yifdu.github.io/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://yifdu.github.io/2018/12/19/Natural-Language-Processing-with-PyTorch（三）/" aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
      $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
        });
    });  
</script>

      
    </div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
            <a href="/tags/Pytorch/" rel="tag"># Pytorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/18/Natural-Language-Processing-with-PyTorch（二）/" rel="next" title="Natural-Language-Processing-with-PyTorch（二）">
                <i class="fa fa-chevron-left"></i> Natural-Language-Processing-with-PyTorch（二）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/20/Natural-Language-Processing-with-PyTorch（四）/" rel="prev" title="Natural-Language-Processing-with-PyTorch（四）">
                Natural-Language-Processing-with-PyTorch（四） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div id="gitalk-container"></div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/xuanyi.jpg" alt="Yif Du">
            
              <p class="site-author-name" itemprop="name">Yif Du</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yifdu" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="17210240004@fudan.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-3-Foundational-Components-of-Neural-Networks"><span class="nav-number">1.</span> <span class="nav-text">Chapter 3. Foundational Components of Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Perceptron-The-Simplest-Neural-Network"><span class="nav-number">1.1.</span> <span class="nav-text">Perceptron: The Simplest Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Activation-Functions"><span class="nav-number">1.2.</span> <span class="nav-text">Activation Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sigmoid"><span class="nav-number">1.2.1.</span> <span class="nav-text">Sigmoid</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tanh"><span class="nav-number">1.2.2.</span> <span class="nav-text">Tanh</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ReLU"><span class="nav-number">1.2.3.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax"><span class="nav-number">1.2.4.</span> <span class="nav-text">Softmax</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-Functions"><span class="nav-number">1.3.</span> <span class="nav-text">Loss Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mean-Squared-Error-Loss"><span class="nav-number">1.3.1.</span> <span class="nav-text">Mean Squared Error Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Categorical-Cross-Entropy-Loss"><span class="nav-number">1.3.2.</span> <span class="nav-text">Categorical Cross-Entropy Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Binary-Cross-Entropy"><span class="nav-number">1.3.3.</span> <span class="nav-text">Binary Cross-Entropy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Diving-Deep-into-Supervised-Training"><span class="nav-number">1.4.</span> <span class="nav-text">Diving Deep into Supervised Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Constructing-Toy-Data"><span class="nav-number">1.4.1.</span> <span class="nav-text">Constructing Toy Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CHOOSING-A-MODEL"><span class="nav-number">1.4.2.</span> <span class="nav-text">CHOOSING A MODEL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Putting-It-Together-Gradient-Based-Supervised-Learning"><span class="nav-number">1.4.3.</span> <span class="nav-text">Putting It Together: Gradient-Based Supervised Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Auxiliary-Training-Concepts"><span class="nav-number">1.4.4.</span> <span class="nav-text">Auxiliary Training Concepts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Correctly-Measuring-Model-Performance-Evaluation-Metrics"><span class="nav-number">1.4.5.</span> <span class="nav-text">Correctly Measuring Model Performance: Evaluation Metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Correctly-Measuring-Model-Performance-Splitting-the-Dataset"><span class="nav-number">1.4.6.</span> <span class="nav-text">Correctly Measuring Model Performance: Splitting the Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Knowing-When-to-Stop-Training"><span class="nav-number">1.4.7.</span> <span class="nav-text">Knowing When to Stop Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Finding-the-Right-Hyperparameters"><span class="nav-number">1.4.8.</span> <span class="nav-text">Finding the Right Hyperparameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">1.4.9.</span> <span class="nav-text">Regularization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-Classifying-Sentiment-of-Restaurant-Reviews"><span class="nav-number">1.5.</span> <span class="nav-text">Example: Classifying Sentiment of Restaurant Reviews</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Yelp-Review-Dataset"><span class="nav-number">1.5.1.</span> <span class="nav-text">The Yelp Review Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-PyTorch’s-Dataset-Representation"><span class="nav-number">1.5.2.</span> <span class="nav-text">Understanding PyTorch’s Dataset Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Vocabulary-the-Vectorizer-and-the-DataLoader"><span class="nav-number">1.5.3.</span> <span class="nav-text">The Vocabulary, the Vectorizer, and the DataLoader</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Perceptron-Classifier"><span class="nav-number">1.5.4.</span> <span class="nav-text">A Perceptron Classifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Training-Routine"><span class="nav-number">1.5.5.</span> <span class="nav-text">The Training Routine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation-Inference-and-Inspection"><span class="nav-number">1.5.6.</span> <span class="nav-text">Evaluation, Inference, and Inspection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">1.6.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="heart">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yif Du</span>

  
</div>





        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script src="/js/src/md5.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '7428ad62daef314bef06',
          clientSecret: '93cd3f4cd41cfc00c4760f65f8d895a66088ea5a',
          repo: 'Comments',
          owner: 'yifdu',
          admin: ['yifdu'],
          id: md5(location.pathname),
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>


  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  <style>
#selectionCopyright {
    position: absolute;
    display: none;
    background: rgba(244,67,54,.7);
    color: #fff;
    border-radius: 6px;
    box-shadow: none;
    border: none;
    font-size: 14px;
}
#selectionCopyright a{
    color:#fff;
    border-color: #fff;
}
#selectionCopyright::before {
    content: "";
    width: 0;
    height: 0;
    border-style: solid;
    border-width: 6px 8px 6px 0;
    border-color: transparent rgba(244,67,54,.7) transparent transparent;
    position: absolute;
    left: -8px;
    top:50%;
    transform:translateY(-50%);
}
</style>

<button id="selectionCopyright" disabled="disabled">本文发表于[<a href="http://yifdu.github.io">yifdu.github.io</a>]分享请注明来源！</button>

<script>
window.onload = function() {
    function selectText() {
        if (document.selection) { //IE浏览器下
            return document.selection.createRange().text; //返回选中的文字
        } else { //非IE浏览器下
            return window.getSelection().toString(); //返回选中的文字
        }
    }
    var content = document.getElementsByTagName("body")[0];
    var scTip = document.getElementById('selectionCopyright');

    content.onmouseup = function(ev) { //设定一个onmouseup事件
        var ev = ev || window.event;
        var left = ev.clientX;//获取鼠标相对浏览器可视区域左上角水平距离距离
        var top = ev.clientY;//获取鼠标相对浏览器可视区域左上角垂直距离距离
        var xScroll = Math.max(document.body.scrollLeft, document.documentElement.scrollLeft);//获取文档水平滚动距离
        var yScroll = Math.max(document.body.scrollTop, document.documentElement.scrollTop);//获取文档垂直滚动距离
        if (selectText().length > 0) {
            setTimeout(function() { //设定一个定时器
                scTip.style.display = 'inline-block';
                scTip.style.left = left + xScroll + 15 + 'px';//鼠标当前x值
                scTip.style.top = top + yScroll - 15 + 'px';//鼠标当前y值
            }, 100);
        } else {
            scTip.style.display = 'none';
        }
    };

    content.onclick = function(ev) {
        var ev = ev || window.event;
        ev.cancelBubble = true;
    };
    document.onclick = function() {
        scTip.style.display = 'none';
    };
};
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
