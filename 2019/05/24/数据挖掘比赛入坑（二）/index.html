<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<meta name="theme-color" content="#222">
<script>
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！请问博主爸爸');
                history.back();
            }
        }
    })();
</script>


  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "258f1ebb"
    });
  daovoice('update');
  </script>









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="数据挖掘,Kaggle比赛,">










<meta name="description" content="Feature Engineering“More data beats clever algorithms,but better data beats more data.” 分类特征 往往需要处理的 高基数能产生更稀疏的数据 填充缺失比较困难  方法一：Onehot encoding 方法二：Hash encoding特点：  避免极度稀疏数据 可能会引起碰撞 可以重复使用不同的哈希函数和袋的结">
<meta name="keywords" content="数据挖掘,Kaggle比赛">
<meta property="og:type" content="article">
<meta property="og:title" content="数据挖掘比赛入坑（二）">
<meta property="og:url" content="http://yifdu.github.io/2019/05/24/数据挖掘比赛入坑（二）/index.html">
<meta property="og:site_name" content="深度菜鸟">
<meta property="og:description" content="Feature Engineering“More data beats clever algorithms,but better data beats more data.” 分类特征 往往需要处理的 高基数能产生更稀疏的数据 填充缺失比较困难  方法一：Onehot encoding 方法二：Hash encoding特点：  避免极度稀疏数据 可能会引起碰撞 可以重复使用不同的哈希函数和袋的结">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1559300206&di=5df0274745df8711154689ab60bc2640&imgtype=jpg&er=1&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201708%2F29%2F20170829110230_X3x2z.thumb.700_0.jpeg">
<meta property="og:updated_time" content="2019-05-25T11:02:18.478Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据挖掘比赛入坑（二）">
<meta name="twitter:description" content="Feature Engineering“More data beats clever algorithms,but better data beats more data.” 分类特征 往往需要处理的 高基数能产生更稀疏的数据 填充缺失比较困难  方法一：Onehot encoding 方法二：Hash encoding特点：  避免极度稀疏数据 可能会引起碰撞 可以重复使用不同的哈希函数和袋的结">
<meta name="twitter:image" content="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1559300206&di=5df0274745df8711154689ab60bc2640&imgtype=jpg&er=1&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201708%2F29%2F20170829110230_X3x2z.thumb.700_0.jpeg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yifdu.github.io/2019/05/24/数据挖掘比赛入坑（二）/">





  <title>数据挖掘比赛入坑（二） | 深度菜鸟</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">深度菜鸟</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-resume">
          <a href="/resume/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            简历
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yifdu.github.io/2019/05/24/数据挖掘比赛入坑（二）/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yif Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/xuanyi.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深度菜鸟">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">数据挖掘比赛入坑（二）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-24T18:54:53+08:00">
                2019-05-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/数据挖掘比赛/" itemprop="url" rel="index">
                    <span itemprop="name">数据挖掘比赛</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读数
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  10k 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  37 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
        <div class="post-gallery" itemscope="" itemtype="http://schema.org/ImageGallery">
          
          
            <div class="post-gallery-row">
              <a class="post-gallery-img fancybox" href="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1559300206&di=5df0274745df8711154689ab60bc2640&imgtype=jpg&er=1&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201708%2F29%2F20170829110230_X3x2z.thumb.700_0.jpeg" rel="gallery_ck9453bed00nxu0wakrzo9lwx" itemscope="" itemtype="http://schema.org/ImageObject" itemprop="url">
                <img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1559300206&di=5df0274745df8711154689ab60bc2640&imgtype=jpg&er=1&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201708%2F29%2F20170829110230_X3x2z.thumb.700_0.jpeg" itemprop="contentUrl">
              </a>
            
          

          
          </div>
        </div>
      

      
        <h1 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h1><p>“More data beats clever algorithms,but better data beats more data.”</p>
<h2 id="分类特征"><a href="#分类特征" class="headerlink" title="分类特征"></a>分类特征</h2><ol>
<li>往往需要处理的</li>
<li>高基数能产生更稀疏的数据</li>
<li>填充缺失比较困难</li>
</ol>
<h3 id="方法一：Onehot-encoding"><a href="#方法一：Onehot-encoding" class="headerlink" title="方法一：Onehot encoding"></a>方法一：Onehot encoding</h3><p><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic1.png" alt="pic1"></p>
<h3 id="方法二：Hash-encoding"><a href="#方法二：Hash-encoding" class="headerlink" title="方法二：Hash encoding"></a>方法二：Hash encoding</h3><p>特点：</p>
<ol>
<li>避免极度稀疏数据</li>
<li>可能会引起碰撞</li>
<li>可以重复使用不同的哈希函数和袋的结果，以实现小碰撞的准确性</li>
<li>碰撞通常会降低结果，但可能会提高结果</li>
<li>优雅地处理新变量(eg:new user-agents)<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic2.png" alt="pic2"><h3 id="方法三-Label-encoding"><a href="#方法三-Label-encoding" class="headerlink" title="方法三:Label encoding"></a>方法三:Label encoding</h3></li>
<li>给每个分类变量一个唯一的数值ID</li>
<li>对基于树的非线性算法有用</li>
<li>不增加维数</li>
<li>随机化 cat_var-&gt; num_id的映射并重新训练,平均<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic3.png" alt="pic3"><h3 id="方法四-Count-encoding"><a href="#方法四-Count-encoding" class="headerlink" title="方法四:Count encoding"></a>方法四:Count encoding</h3></li>
<li>用分类变量在训练集中的数目来替代分类变量</li>
<li>对线性和非线性都有用</li>
<li>对离散值敏感</li>
<li>可以增加log变换,让count的效果更好</li>
<li>用‘1’代替没有见过的变量</li>
<li>可以给碰撞:相同编码,不同变量<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic4.png" alt="pic4"><br>LabelCount encoding<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic5.png" alt="pic5"><h3 id="方法五-Target-encoding"><a href="#方法五-Target-encoding" class="headerlink" title="方法五:Target encoding"></a>方法五:Target encoding</h3></li>
<li>按目标比率编码分类变量(二分类或回归)</li>
<li>要当心过拟合</li>
<li>叠加形式:单变量模型，输出平均目标</li>
<li>在交叉熵形式下做</li>
<li>增加平滑来避免变量编码被置为0</li>
<li>增加随机噪声以与过拟合斗争</li>
<li>如果应用合适：将是对线性和非线性都是最好的编码<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic6.png" alt="pic6"><h3 id="方法六-Catgory-embedding"><a href="#方法六-Catgory-embedding" class="headerlink" title="方法六:Catgory embedding"></a>方法六:Catgory embedding</h3></li>
<li>用神经网络来对分类变量创建稠密的embeddings</li>
<li>用一个近似函数映射分类变量到欧氏空间</li>
<li>更快的模型训练</li>
<li>更少的内从开销</li>
<li>能产生更好的one-hot编码<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic7.png" alt="pic7"><h3 id="NaN编码"><a href="#NaN编码" class="headerlink" title="NaN编码"></a>NaN编码</h3></li>
<li>给NaN值一个明确的编码来代替忽视他</li>
<li>NaN值也含有信息</li>
<li>当心过拟合</li>
<li>仅当训练和测试集中的nan-值由相同的原因引起时，或者当本地验证证明它含有信号时才使用<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic8.png" alt="pic8"><h3 id="Polynomial-encoding"><a href="#Polynomial-encoding" class="headerlink" title="Polynomial encoding"></a>Polynomial encoding</h3></li>
<li>编码分类变量间的交互</li>
<li>没有交互的线性算法不能解决XOR问题</li>
<li>一个多项式核能解决XOR问题</li>
<li>特征空间爆炸：使用FS,hash 和/或VM<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic9.png" alt="pic9"><h3 id="Expansion-encoding"><a href="#Expansion-encoding" class="headerlink" title="Expansion encoding"></a>Expansion encoding</h3></li>
<li>根据一个单变量产生多个分类变量</li>
<li>一些高基数特征,像user-agents，拥有更多的信息在其中：</li>
</ol>
<ul>
<li>is_mobile?</li>
<li>is_latest_version?</li>
<li>Operation_system</li>
<li>Browser_build</li>
<li>Etc<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic10.png" alt="pic10"><h3 id="Consolidation-encoding"><a href="#Consolidation-encoding" class="headerlink" title="Consolidation encoding"></a>Consolidation encoding</h3></li>
</ul>
<ol>
<li>映射不同的分类变量到相同的变量<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic11.png" alt="pic11"></li>
</ol>
<p>Coming up with feature is difficult,time-consuming,requires expert knowledge. Applied machine learning is basically feature engineering.</p>
<h2 id="数值特征"><a href="#数值特征" class="headerlink" title="数值特征"></a>数值特征</h2><ol>
<li>能直接喂给算法</li>
<li>能构成floats,counts,number</li>
<li>容易填充缺失数据<h3 id="方法一-Rounding"><a href="#方法一-Rounding" class="headerlink" title="方法一:Rounding"></a>方法一:Rounding</h3></li>
<li>近似数值变量</li>
<li>有损压缩形式:保留数据最重要的特性</li>
<li>有时候太精确只是噪声</li>
<li>能在rounding前用log变换<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic12.png" alt="pic12"><h3 id="方法二-Binning"><a href="#方法二-Binning" class="headerlink" title="方法二:Binning"></a>方法二:Binning</h3></li>
<li>将数值变量放入一个分箱中并且用bin-ID编码它</li>
<li>Binning可以实用地设置，也可以通过分位数、均匀设置，或者使用模型来寻找最优的垃圾桶</li>
<li>可以优雅地处理训练集合中看到的范围之外的变量<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic13.png" alt="pic13"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic14.png" alt="pic14"><h3 id="方法三-Scaling"><a href="#方法三-Scaling" class="headerlink" title="方法三:Scaling"></a>方法三:Scaling</h3></li>
<li>缩放数值变量到特定范围内</li>
</ol>
<ul>
<li>Standard(Z) Scaling</li>
<li>MinMax Scaling</li>
<li>Root Scaling</li>
<li>Log Scaling<h3 id="方法四-Imputation"><a href="#方法四-Imputation" class="headerlink" title="方法四:Imputation"></a>方法四:Imputation</h3></li>
</ul>
<ol>
<li>填充缺失值</li>
<li>硬编码可以与Imputation相结合</li>
</ol>
<ul>
<li>Mean:Very basic</li>
<li>Median:对离群点更鲁棒</li>
<li>Ignoring: 只是延缓了这个问题</li>
<li>用模型:能展现算法偏差<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic15.png" alt="pic15"><h3 id="方法五-Interactions"><a href="#方法五-Interactions" class="headerlink" title="方法五:Interactions"></a>方法五:Interactions</h3></li>
</ul>
<ol>
<li>对交互的数值变量进行特定的编码</li>
</ol>
<ul>
<li>尝试:减，加，乘,除</li>
<li>使用:特征选择通过统计学测试或训练模型的feature importances</li>
<li>忽视:人工直觉;怪异的整合可能会带来极大的改进</li>
</ul>
<p>“…. some machine learning projects succeed and some fail. What make the difference? Easily the most important factor is the features used.”</p>
<h3 id="方法六-线性算法的非线性编码"><a href="#方法六-线性算法的非线性编码" class="headerlink" title="方法六:线性算法的非线性编码"></a>方法六:线性算法的非线性编码</h3><ol>
<li>硬编码非线性能改善线性算法</li>
</ol>
<ul>
<li>Polynomial kernel</li>
<li>Leafcoding(随机森林embedding)</li>
<li>Genetic algorithms</li>
<li>Locally Linear Embedding,Spectral Embedding,t-SNE<h3 id="方法七-Row-statistics"><a href="#方法七-Row-statistics" class="headerlink" title="方法七:Row statistics"></a>方法七:Row statistics</h3></li>
</ul>
<ol>
<li>Create statistics on a row of data</li>
</ol>
<ul>
<li>NaN的数目</li>
<li>0的数目</li>
<li>负数的数目</li>
<li>Mean,Max,Min,skewness,等</li>
</ul>
<p>“The algorithms we used are very standard for Kagglers.[…]We spent most of our efforts in feature engineering.[….] We were also very careful to discard features likely to expose us to the risk of over-fitting our model.”</p>
<h2 id="时间变量"><a href="#时间变量" class="headerlink" title="时间变量"></a>时间变量</h2><ol>
<li>时间变量,像日期,需要局部的验证方式(像backtesting)</li>
<li>这里容易犯错</li>
<li>有许多机会造成极大的改进<h3 id="方法一-Projecting-to-a-circle"><a href="#方法一-Projecting-to-a-circle" class="headerlink" title="方法一:Projecting to a circle"></a>方法一:Projecting to a circle</h3></li>
<li>转换单一特征,像day_of_week,转化为一个圆上的两个坐标上</li>
<li>Ensures that distance between max and min is the same as min and min +1（不懂）</li>
<li>对day_of_week,day_of month,hour_of_day等用<h3 id="方法二-trendlines"><a href="#方法二-trendlines" class="headerlink" title="方法二:trendlines"></a>方法二:trendlines</h3></li>
<li>代替编码总的花费,编码像:上一周的花费,上一个月的花费,上一年的花费</li>
<li>该算法给出了一个趋势:两个消费相同的客户，可能会有截然不同的行为——一个客户可能开始消费更多，而另一个客户则开始减少支出。</li>
</ol>
<h3 id="方法三-Closeness-to-major-events"><a href="#方法三-Closeness-to-major-events" class="headerlink" title="方法三:Closeness to major events"></a>方法三:Closeness to major events</h3><ul>
<li>硬编码分类特征像:date_3_days_before_holidays:1</li>
<li>尝试:国家假期,主要的运动事件,周末,月的第一个周六等</li>
<li>这些因素能有主要影响在消费行为上。</li>
</ul>
<p>“feature engineering is another topic which doesn’t seem to merit any review papers or books, or even chapters in books,but it is absolutely vital to ML success.[…] Much of the success of machine learning is actually success in engineering features that a learner can understand.”</p>
<h2 id="空间变量"><a href="#空间变量" class="headerlink" title="空间变量"></a>空间变量</h2><ol>
<li>空间变量是能编码空间位置的变量</li>
<li>例子包括:GPS坐标,城市,国家,地址<h3 id="方法一-Categorizing-location"><a href="#方法一-Categorizing-location" class="headerlink" title="方法一:Categorizing location"></a>方法一:Categorizing location</h3></li>
<li>Kriging</li>
<li>K-means clustering</li>
<li>Raw latitude longitude</li>
<li>转化城市为经纬度</li>
<li>在街道名称中添加邮政编码<h3 id="方法二-Closeness-to-hubs"><a href="#方法二-Closeness-to-hubs" class="headerlink" title="方法二:Closeness to hubs"></a>方法二:Closeness to hubs</h3></li>
<li>Find closeness between a location to a major hub</li>
<li>小镇会继承附近大城市的文化/context</li>
<li>手机位置能银蛇到附近的商业和supermarkets<h3 id="方法三-Spatial-fraudulent-behavior"><a href="#方法三-Spatial-fraudulent-behavior" class="headerlink" title="方法三:Spatial fraudulent behavior"></a>方法三:Spatial fraudulent behavior</h3></li>
<li>位置事件数据能表示可疑行为</li>
<li>不可能的旅行速度:在不同的国家同时进行多个交易</li>
<li>spending in different town than home or shipping address</li>
<li>Never spending at the same location</li>
</ol>
<p>“You have to turn your inputs into things the algorithm can understand”</p>
<h2 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h2><ol>
<li>数据探索额能找到数据的健康问题,离群点,噪声,特征工程ideas，特征清理ideas</li>
<li>能使用:Console,Notebook,pandas</li>
<li>简单统计尝试:Min,max</li>
<li>Incorporate the target so find correlation between signal<h3 id="方法一-Iteration-Debugging"><a href="#方法一-Iteration-Debugging" class="headerlink" title="方法一:Iteration/Debugging"></a>方法一:Iteration/Debugging</h3></li>
<li>特征工程是一个迭代过程:使你的pipelines稳定来更快的迭代</li>
<li>Use sub-linear debugging:output intermediate information on the process,do spurious logging(使用子线性调试:输出进程的中间信息，做伪日志记录)</li>
<li>失败的点子比成功的点子多<h3 id="方法二-Label-Engineering"><a href="#方法二-Label-Engineering" class="headerlink" title="方法二:Label Engineering"></a>方法二:Label Engineering</h3></li>
<li>能创建一个label/target/dependent 变量作为数据的一个特征</li>
</ol>
<ul>
<li>log变换:y-&gt;log(y+1)| exp(y_pred)-1</li>
<li>square变换</li>
<li>Box-Cox变换</li>
<li>创建一个分数,来用回归转换二值目标</li>
<li>训练回归器来预测测试集中不可用的特性</li>
</ul>
<p>“Developing good models requires iterating many times on your initial ideas,up until the deadline; you can always improve your models further. Your final models will typically share little in common with the solutions you envisioned when first approaching the problem,because a-priori plans basically never survive confrontation with experimental reality.”</p>
<h2 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h2><ol>
<li>能使用相同的ideas从categorical features里借鉴</li>
<li>深度学习(自动特性工程)正在逐渐蚕食这个领域，但是带有良好工程特性的浅层学习仍然具有竞争力</li>
<li>数据中的高稀疏将引起维度灾难</li>
<li>特征工程里的许多机会：<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic16.png" alt="pic16"><h3 id="方法一-清洗-cleaning"><a href="#方法一-清洗-cleaning" class="headerlink" title="方法一:清洗(cleaning)"></a>方法一:清洗(cleaning)</h3></li>
<li>字母小写化(Lowercasing)</li>
<li>unidecode:将重音字符转换为对应的ascii字符</li>
<li>移除非字母数字</li>
<li>修复(repairing):修复编码问题或修剪令牌间空间<h3 id="方法二-Tokenizing"><a href="#方法二-Tokenizing" class="headerlink" title="方法二:Tokenizing"></a>方法二:Tokenizing</h3></li>
<li>编码标点符号</li>
<li>符号化(tokenize)</li>
<li>N-grams(编码连续token为tokens)</li>
<li>skip-gram</li>
<li>char-grams</li>
<li>Affixes<h3 id="方法三-Removing"><a href="#方法三-Removing" class="headerlink" title="方法三:Removing"></a>方法三:Removing</h3></li>
<li>stopswords:移除出现在停用词里的words/tokens</li>
<li>rare words:移除只在训练集中出现很少次的单词</li>
<li>Common words:移除那些没在停用词中但极其常见的单词<h3 id="方法四-Roots"><a href="#方法四-Roots" class="headerlink" title="方法四:Roots"></a>方法四:Roots</h3></li>
<li>spelling correction：改变tokens为正确的拼写</li>
<li>chop:只取一个单词的前n(8)个字符</li>
<li>stem:减少一个单词为它的词根形式</li>
<li>Lemmatize:找到语义根:”never be late”——&gt;”never are late”<h3 id="方法五-Enrich"><a href="#方法五-Enrich" class="headerlink" title="方法五:Enrich"></a>方法五:Enrich</h3></li>
<li>文档特征:空格,tabs,newlines,字符,tokens等的数目</li>
<li>实体插入:增加更多一般具体到文本中”Microsoft releases Windows”——&gt;”Microsoft(company) release Windows(application)”</li>
<li>Parse Trees: Parse a sentence into logic form: “Alice hits Bill”——&gt;Alice/Noun_subject hits/Verb Bill/Noun_object.</li>
<li>Reading level:Compute the reading level of a document<h3 id="方法六-Similarities"><a href="#方法六-Similarities" class="headerlink" title="方法六:Similarities"></a>方法六:Similarities</h3></li>
<li>Token similarity</li>
<li>Compression distance</li>
<li>Levenshtein/Hamming/Jaccard Distance</li>
<li>Word2Vec/Glove<h3 id="方法七-TF-IDF"><a href="#方法七-TF-IDF" class="headerlink" title="方法七:TF-IDF"></a>方法七:TF-IDF</h3></li>
</ol>
<ul>
<li>Term Frequency</li>
<li>Inverse Document Frequency</li>
<li>TF-IDF<h3 id="方法八-降维"><a href="#方法八-降维" class="headerlink" title="方法八:降维"></a>方法八:降维</h3></li>
</ul>
<ol>
<li>PCA</li>
<li>SVD</li>
<li>LDA</li>
<li>LSA<h3 id="方法九-外部模型"><a href="#方法九-外部模型" class="headerlink" title="方法九:外部模型"></a>方法九:外部模型</h3></li>
<li>情感分析</li>
<li>主题模型</li>
</ol>
<p>“So many papers:feature engineering is hard and time consuming. instead here’s 8 pages in which we have to design a new weird neural net to do the same thing.”</p>
<h2 id="Neural-Networks-amp-Deep-Learning"><a href="#Neural-Networks-amp-Deep-Learning" class="headerlink" title="Neural Networks&amp;Deep Learning"></a>Neural Networks&amp;Deep Learning</h2><ol>
<li>神经网络申明是端到端的自动特征工程</li>
<li>特征工程垂死领域</li>
<li>不！只是转移注意力到了结构工程中去了<h3 id="Leakage-Golden-Features"><a href="#Leakage-Golden-Features" class="headerlink" title="Leakage/Golden Features"></a>Leakage/Golden Features</h3></li>
<li>特征工程能帮助弥补缺陷</li>
<li>反向工程:</li>
</ol>
<ul>
<li>Reverse MD5 hash with rainbow tables.</li>
<li>Reverse TF-IDF back to Term Frequency</li>
<li>Encode order of samples data set</li>
<li>Encode file creation dates</li>
</ul>
<ol>
<li>规则挖掘:</li>
</ol>
<ul>
<li>找到简单的规则(并且编码这些)来帮助你的模型</li>
</ul>
<h1 id="COS424-Feature-engineering"><a href="#COS424-Feature-engineering" class="headerlink" title="COS424 Feature engineering"></a>COS424 Feature engineering</h1><h2 id="The-importance-of-features"><a href="#The-importance-of-features" class="headerlink" title="The importance of features"></a>The importance of features</h2><h3 id="简单的线性模型"><a href="#简单的线性模型" class="headerlink" title="简单的线性模型"></a>简单的线性模型</h3><p>People like simple linear models with convex loss functions</p>
<ul>
<li>Training has a unique solution.</li>
<li>Easy to analyze and easy to debug.<br>Which basis functions Φ?</li>
<li>Also called the features<br>Many basis function Φ?</li>
<li>Poor testing performance.<br>Few basis functions</li>
<li>Poor training performance, in general.</li>
<li>Good training performance if we pick the right ones.</li>
<li>The testing performance is then good as well.<h3 id="Explainable-models"><a href="#Explainable-models" class="headerlink" title="Explainable models"></a>Explainable models</h3>Modelling for prediction</li>
<li>Sometimes one builds a model for its predictions.</li>
<li>The model is the operational system.</li>
<li>Better prediction =⇒ $$$.<br>Modelling for explanations</li>
<li>Sometimes one builds a model for interpreting its structure.</li>
<li>The human acquires knowledge from the model.</li>
<li>The human then design the operational system.<br>(we need humans because our modelling technology is insufficient.)<br>Selecting the important features</li>
<li>More compact models are usually easier to interpret.</li>
<li>A model optimized for explanability is not optimized for accuracy.</li>
<li>Identification problem vs. emulation problem.<h3 id="Feature-explosion"><a href="#Feature-explosion" class="headerlink" title="Feature explosion"></a>Feature explosion</h3>Initial features</li>
<li>The initial pick of feature is always an expression of prior knowledge.<br>images −→ pixels, contours, textures, etc.<br>signal −→ samples, spectrograms, etc.<br>time series −→ ticks, trends, reversals, etc.<br>biological data −→ dna, marker sequences, genes, etc.<br>text data −→ words, grammatical classes and relations, etc.<br>Combining features</li>
<li>Combinations that linear system cannot represent:<br>polynomial combinations, logical conjunctions, decision trees.</li>
<li>Total number of features then grows very quickly.<br>Solutions</li>
<li>Kernels (with caveats, see later)</li>
<li>Feature selection (but why should it work at all?)</li>
</ul>
<h2 id="Feature-relevance"><a href="#Feature-relevance" class="headerlink" title="Feature relevance"></a>Feature relevance</h2><p>Assume we know distribution p (X, Y ).<br>Y : output<br>X : input, all features<br>Xi: one feature<br>Ri = X  Xi: all features but Xi</p>
<h3 id="Probabilistic-feature-relevance"><a href="#Probabilistic-feature-relevance" class="headerlink" title="Probabilistic feature relevance"></a>Probabilistic feature relevance</h3><ol>
<li>Strongly relevant feature<br>– Definition: Xi 不相关 Y | Ri<br>Feature Xi brings information that no other feature contains.</li>
<li>Weakly relevant feature<br>– Definition: Xi 不相关 Y | S for some strict subset S of Ri<br>.<br>Feature Xi brings information that also exists in other features.<br>Feature Xi brings information in conjunction with other features.</li>
<li>Irrelevant feature<br>– Definition: neither strongly relevant nor weakly relevant.<br>Stronger than Xi ⊥⊥ Y . See the XOR example.</li>
<li>Relevant feature<br>– Definition: not irrelevant.<h3 id="Interesting-example"><a href="#Interesting-example" class="headerlink" title="Interesting example"></a>Interesting example</h3><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic17.png" alt="pic17"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic18.png" alt="pic18"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic19.png" alt="pic19"><h2 id="Selecting-features"><a href="#Selecting-features" class="headerlink" title="Selecting features"></a>Selecting features</h2>How to select relevant features<br>when p(x, y) is unknown<br>but data is available?</li>
</ol>
<h3 id="Selecting-features-from-data"><a href="#Selecting-features-from-data" class="headerlink" title="Selecting features from data"></a>Selecting features from data</h3><p>Training data is limited</p>
<ul>
<li>Restricting the number of features is a capactity control mechanism.</li>
<li>We may want to use only a subset of the relevant features.<br>Notable approaches</li>
<li>Feature selection using regularization.</li>
<li>Feature selection using wrappers.</li>
<li>Feature selection using greedy algorithms.<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic20.png" alt="pic20"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic21.png" alt="pic21"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic22.png" alt="pic22"><h3 id="Wrapper-approaches"><a href="#Wrapper-approaches" class="headerlink" title="Wrapper approaches"></a>Wrapper approaches</h3>Wrappers</li>
<li>Assume we have chosen a learning system and algorithm.</li>
<li>Navigate feature subsets by adding/removing features.</li>
<li>Evaluate on the validation set.<br>Backward selection wrapper</li>
<li>Start with all features.</li>
<li>Try removing each feature and measure validation set impact.</li>
<li>Remove the feature that causes the least harm.</li>
<li>Repeat.<br>Notes</li>
<li>There are many variants (forward, backtracking, etc.)</li>
<li>Risk of overfitting the validation set.</li>
<li>Computationally expensive.</li>
<li>Quite effective in practice.</li>
</ul>
<h3 id="Greedy-methods"><a href="#Greedy-methods" class="headerlink" title="Greedy methods"></a>Greedy methods</h3><p>Algorithms that incorporate features one by one.<br>Decision trees</p>
<ul>
<li>Each decision can be seen as a feature.</li>
<li>Pruning the decision tree prunes the features<br>Ensembles</li>
<li>Ensembles of classifiers involving few features.</li>
<li>Random forests.</li>
<li>Boosting.</li>
</ul>
<h2 id="Learning-features"><a href="#Learning-features" class="headerlink" title="Learning features"></a>Learning features</h2><h3 id="Feature-learning-in-one-slide"><a href="#Feature-learning-in-one-slide" class="headerlink" title="Feature learning in one slide"></a>Feature learning in one slide</h3><p>Suppose we have weight on a feature X.<br>Suppose we prefer a closely related feature $X+\epsilon$<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic23.png" alt="pic23"></p>
<h3 id="Feature-learning-and-multilayer-models"><a href="#Feature-learning-and-multilayer-models" class="headerlink" title="Feature learning and multilayer models"></a>Feature learning and multilayer models</h3><p><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic24.png" alt="pic24"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic25.png" alt="pic25"></p>
<h1 id="Feature-Engineering-Knowledge-Discovery-and-Data-Mining"><a href="#Feature-Engineering-Knowledge-Discovery-and-Data-Mining" class="headerlink" title="Feature Engineering: Knowledge Discovery and Data Mining"></a>Feature Engineering: Knowledge Discovery and Data Mining</h1><p><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic26.png" alt="pic26"></p>
<h2 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h2><p>熵<br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic27.png" alt="pic27"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic28.png" alt="pic28"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic29.png" alt="pic29"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic30.png" alt="pic30"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic31.png" alt="pic31"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic32.png" alt="pic32"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic33.png" alt="pic33"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic34.png" alt="pic34"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic35.png" alt="pic35"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic36.png" alt="pic36"></p>
<h2 id="What-are-features-amp-feature-engineering"><a href="#What-are-features-amp-feature-engineering" class="headerlink" title="What are features &amp; feature engineering"></a>What are features &amp; feature engineering</h2><p><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic37.png" alt="pic37"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic38.png" alt="pic38"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic39.png" alt="pic39"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic40.png" alt="pic40"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic41.png" alt="pic41"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic42.png" alt="pic42"></p>
<h2 id="Feature-Value-Processing"><a href="#Feature-Value-Processing" class="headerlink" title="Feature Value Processing"></a>Feature Value Processing</h2><p><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic43.png" alt="pic43"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic44.png" alt="pic44"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic45.png" alt="pic45"></p>
<h2 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h2><p><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic46.png" alt="pic46"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic47.png" alt="pic47"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic48.png" alt="pic48"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic49.png" alt="pic49"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic50.png" alt="pic50"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic51.png" alt="pic51"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic52.png" alt="pic52"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic53.png" alt="pic53"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic54.png" alt="pic54"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic55.png" alt="pic55"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic56.png" alt="pic56"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic57.png" alt="pic57"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic58.png" alt="pic58"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic59.png" alt="pic59"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic60.png" alt="pic60"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic61.png" alt="pic61"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic62.png" alt="pic62"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic63.png" alt="pic63"></p>
<h1 id="发现特征工程，如何设计特征以及如何获得它"><a href="#发现特征工程，如何设计特征以及如何获得它" class="headerlink" title="发现特征工程，如何设计特征以及如何获得它"></a>发现特征工程，如何设计特征以及如何获得它</h1><h2 id="特征工程解决的问题"><a href="#特征工程解决的问题" class="headerlink" title="特征工程解决的问题"></a>特征工程解决的问题</h2><p>当您的目标是从预测模型中获得最佳结果时，您需要从所拥有的内容中获得最大收益。</p>
<p>这包括从您使用的算法中获得最佳结果。它还涉及充分利用您的算法使用的数据。</p>
<p><strong>如何充分利用数据进行预测建模？</strong><br>这是特征工程的过程和实践解决的问题.</p>
<h2 id="特征工程的重要性"><a href="#特征工程的重要性" class="headerlink" title="特征工程的重要性"></a>特征工程的重要性</h2><p>数据中的功能将直接影响您使用的预测模型和您可以实现的结果。</p>
<p>您可以这样说：您准备和选择的功能越多，您获得的结果就越好。这是事实，但也有误导性。</p>
<p>您获得的结果是您选择的模型，您可用的数据和您准备的功能的一个因素。即使你对问题的框架和你用来估计准确性的客观措施起了作用。您的结果取决于许多相互依赖的属性。</p>
<p>您需要很好的功能来描述数据中固有的结构。<br><strong>更好的功能意味着灵活性。</strong><br>您可以选择“错误的模型”（低于最佳模型）并仍然可以获得良好的结果。大多数模型可以在数据中获得良好的结构。良好功能的灵活性允许您使用运行速度更快，更易于理解且更易于维护的不太复杂的模型。这是非常理想的。<br><strong>更好的功能，意味着简单的模型。</strong><br>凭借精心设计的功能，您可以选择“错误的参数”（低于最佳值）并仍然获得良好的结果，原因大致相同。您不需要努力选择正确的模型和最优化的参数。<br>通过良好的功能，您可以更接近底层问题，并可以表示您可用的所有数据，并可用于最好地表征潜在问题。</p>
<p><strong>更好的功能，意味着更好的结果</strong></p>
<h2 id="什么是特征工程"><a href="#什么是特征工程" class="headerlink" title="什么是特征工程"></a>什么是特征工程</h2><p>以下是我定义特征工程的方法：<br><strong>特征工程是将原始数据转换为更能代表预测模型的基础问题的特征的过程，从而提高了对看不见的数据的模型准确性。</strong><br>您可以在此定义中查看依赖项：</p>
<p>您选择的绩效指标（RMSE？AUC？）<br>问题的框架（分类？回归？）<br>你正在使用的预测模型（SVM？）<br>您选择和准备的原始数据（样本？格式化？清洁？）</p>
<p><strong>特征工程是手动设计输入x应该是什么</strong></p>
<h3 id="特征工程是一个表示问题"><a href="#特征工程是一个表示问题" class="headerlink" title="特征工程是一个表示问题"></a>特征工程是一个表示问题</h3><p>机器学习算法从样本数据中学习问题的解决方案。</p>
<p>在此上下文中，特征工程要求：学习问题解决方案的样本数据的最佳表示是什么？</p>
<p>它太深了。在机器学习方面做得很好，即使在人工智能方面，也可以回到表示问题。知道最佳表示形式，先验可能是不可知的（或者至多是难以处理的）。</p>
<p>你必须把你的输入变成算法可以理解的东西</p>
<h3 id="特征工程是一门艺术"><a href="#特征工程是一门艺术" class="headerlink" title="特征工程是一门艺术"></a>特征工程是一门艺术</h3><p>这是一门艺术，就像工程学是一门艺术，就像编程是一门艺术，就像医学是一门艺术。</p>
<p>有明确定义的程序是有条理的，可证明的和理解的。</p>
<p>对特征工程的掌握伴随着实践，并且研究其他正在做得好的人正在练习。</p>
<h2 id="特征工程的子问题"><a href="#特征工程的子问题" class="headerlink" title="特征工程的子问题"></a>特征工程的子问题</h2><p>将特征工程视为一件事是很常见的。</p>
<p>例如，对我来说很长一段时间，特征工程是特征构造。</p>
<p>我想我自己“ 我现在正在进行特征工程 ”，我会追问“ 如何分解或聚合原始数据以更好地描述潜在问题？“目标是正确的，但这种方法很多。</p>
<p>在本节中，我们将介绍这些方法以及它们要解决的具体子问题。每个都可以是他们自己的深度文章，因为它们是实践和研究的重要和重要领域。</p>
<h3 id="功能：对建模任务有用的属性"><a href="#功能：对建模任务有用的属性" class="headerlink" title="功能：对建模任务有用的属性"></a>功能：对建模任务有用的属性</h3><p>让我们从数据开始，什么是功能。</p>
<p>表格数据是根据观察或由变量或属性（列）组成的实例（行）来描述的。属性可以是一个功能。</p>
<p>与属性分离的特征的概念在问题的上下文中更有意义。功能是对您的问题有用或有意义的属性。它是了解正在建模的问题结构的观察的重要部分。</p>
<p>我使用“ 有意义 ”来区分属性和属性。有些属性可能没有。我认为没有一个没有意义的功能。如果某个功能对问题没有影响，则不是问题的一部分。</p>
<p>在计算机视觉中，图像是观察，但是特征可以是图像中的线。在自然语言处理中，文档或推文可以是观察，并且短语或单词计数可以是特征。在语音识别中，话语可以是观察，但是特征可以是单个单词或音素。</p>
<h3 id="特征重要性：对特征有用性的估计"><a href="#特征重要性：对特征有用性的估计" class="headerlink" title="特征重要性：对特征有用性的估计"></a>特征重要性：对特征有用性的估计</h3><p>您可以客观地估计要素的有用性。</p>
<p>这可以作为选择要素的前提。功能分配分数，然后可以按其分数排名。可以选择具有最高分数的那些特征以包括在训练数据集中，而可以忽略那些剩余的特征。</p>
<p>功能重要性分数还可以为您提供可用于提取或构建新功能的信息，这些功能与估计有用的功能类似但不同。</p>
<p>如果特征与因变量（被预测的事物）高度相关，则该特征可能是重要的。相关系数和其他单变量（每个属性被独立考虑）方法是常用方法。</p>
<p>更复杂的预测建模算法在构建模型时在内部执行特征重要性和选择。一些例子包括MARS，Random Forest和Gradient Boosted Machines。这些模型还可以报告在模型准备过程中确定的变量重要性。</p>
<h3 id="特征提取-从原始数据自动构建新特征"><a href="#特征提取-从原始数据自动构建新特征" class="headerlink" title="特征提取:从原始数据自动构建新特征"></a>特征提取:从原始数据自动构建新特征</h3><p>一些观察结果在其原始状态下过于庞大，无法直接通过预测建模算法进行建模。</p>
<p>常见示例包括图像，音频和文本数据，但可以轻松地包含具有数百万属性的表格数据。</p>
<p>特征提取是将这些类型的观察的维度自动降低为可以建模的更小集合的过程。</p>
<p>对于表格数据，这可能包括主成分分析和无监督聚类方法等投影方法。对于图像数据，这可能包括线或边缘检测。根据域，图像，视频和音频观察结果适用于许多相同类型的DSP方法。</p>
<p>特征提取的关键是方法是自动的（尽管可能需要通过简单的方法设计和构造）并解决难以管理的高维数据的问题，最常用于以数字格式存储的模拟观测。</p>
<h3 id="功能选择：从许多功能到一些有用的功能"><a href="#功能选择：从许多功能到一些有用的功能" class="headerlink" title="功能选择：从许多功能到一些有用的功能"></a>功能选择：从许多功能到一些有用的功能</h3><p>并非所有功能都是平等的。</p>
<p>需要删除与问题无关的那些属性。对于模型的准确性，将会有一些功能比其他功能更重要。在其他功能的上下文中还将存在多余的功能。</p>
<p>功能选择通过自动选择对问题最有用的子集来解决这些问题。</p>
<p>特征选择算法可以使用评分方法来排序和选择特征，例如相关性或其他特征重要性方法。</p>
<p>更高级的方法可以通过反复试验来搜索特征子集，自动创建和评估模型以追求客观上最具预测性的子特征组。</p>
<p>还有一些方法可以烘焙特征选择或将其作为模型的副作用。逐步回归是一种算法的示例，该算法自动执行特征选择，作为模型构建过程的一部分。</p>
<p>像LASSO和岭回归这样的正则化方法也可以被认为是具有特征选择的算法，因为它们积极地寻求去除或折扣特征的贡献作为模型构建过程的一部分。</p>
<p>在帖子中阅读更多内容：功能选择简介。</p>
<h3 id="特征构造：从原始数据手动构建新特征"><a href="#特征构造：从原始数据手动构建新特征" class="headerlink" title="特征构造：从原始数据手动构建新特征"></a>特征构造：从原始数据手动构建新特征</h3><p>最好的结果归结于你，从业者，制作功能。</p>
<p>功能重要性和选择可以告诉您功能的客观效用，但这些功能必须来自某个地方。</p>
<p>您需要手动创建它们。这需要花费大量时间来处理实际的样本数据（而不是聚合），并考虑问题的基本形式，数据结构以及如何最好地将它们暴露给预测建模算法。</p>
<p>对于表格数据，它通常意味着聚合或组合功能以创建新功能，分解或拆分功能以创建新功能。</p>
<p>对于文本数据，它通常意味着设计与问题相关的文档或上下文特定指标。对于图像数据，它通常意味着大量的时间规定自动过滤器来挑选相关结构。</p>
<p>这是特征工程的一部分，经常被作为一种艺术形式进行讨论，这一部分归功于重要性，并标志着它是竞争机器学习的差异化因素。</p>
<p>它是手动的，它很慢，它需要大量的人脑力量，它会产生很大的不同。</p>
<p><strong>特征工程和特征选择不是互斥的。它们都很有用。我会说功能工程更重要，特别是因为你不能真正自动化它。</strong></p>
<h3 id="特征学习：自动识别和使用原始数据中的特征"><a href="#特征学习：自动识别和使用原始数据中的特征" class="headerlink" title="特征学习：自动识别和使用原始数据中的特征"></a>特征学习：自动识别和使用原始数据中的特征</h3><p>我们是否可以避免手动加载规定如何从原始数据构造或提取特征？</p>
<p>表征学习或特色学习是朝着这个目标努力的方向。</p>
<p>现代深度学习方法在该领域取得了一些成功，例如自动编码器和受限制的玻尔兹曼机器。它们已经被证明是自动的，并且以无监督或半监督的方式，学习特征的抽象表示（压缩形式），这反过来又支持了诸如语音识别，图像分类等领域的最新结果。物体识别和其他领域。</p>
<p>我们没有自动特征提取或构造，但我们可能永远不会有自动特征工程。</p>
<p>抽象表示是自动准备的，但除了以黑盒方式之外，您无法理解和利用所学知识。他们不能（但是，或者很容易）告知您和过程如何创建更多相似和不同的功能，例如那些在特定问题或未来类似问题上表现良好的功能。获得的技能被困。</p>
<p>然而，它是令人着迷，令人兴奋的，也是功能工程的一个重要而现代的部分。</p>
<h2 id="特征工程过程"><a href="#特征工程过程" class="headerlink" title="特征工程过程"></a>特征工程过程</h2><p>在更广泛的应用机器学习过程中最好地理解特征工程。</p>
<p>你需要这个背景。</p>
<h3 id="机器学习的过程"><a href="#机器学习的过程" class="headerlink" title="机器学习的过程"></a>机器学习的过程</h3><p>应用机器学习的过程（缺乏更好的名称）在广泛的刷子意​​义上涉及许多活动。前面是问题定义，接下来是数据选择和准备，中间是模型准备，评估和调整，最后是结果的呈现。</p>
<p>数据挖掘和KDD等流程描述有助于更好地理解任务和子任务。您可以按照自己喜欢的方式挑选和选择流程。我之前谈过这个问题很多。</p>
<p>与我们关于特征工程的讨论相关的图片是此过程的前端。它可能类似于以下内容：</p>
<ol>
<li>此前的任务……）</li>
<li>选择数据：集成数据，将其去标准化为数据集，将其收集在一起。</li>
<li>预处理数据：对其进行格式化，清理，对其进行采样，以便您可以使用它。</li>
<li>转换数据：功能工程师在这里发生。</li>
<li>模型数据：创建模型，评估模型并对其进行调整。</li>
<li>（此后的任务……）</li>
</ol>
<p>传统的“ 将数据转换 ”从原始状态转变为适合建模的状态是特征工程适用的地方。转换数据和特征工程实际上可能是同义词。</p>
<p>这张照片在某些方面有所帮助。</p>
<p>您可以看到，在特征工程之前，我们正在将数据转换为我们甚至可以查看的格式，就在此之前，我们正在将数据从数据库整理和非规范化为某种中心图像。</p>
<p>当我们确定数据的新观点时，我们可以而且应该回过头来完成这些步骤。</p>
<p>例如，我们可能有一个属性，即聚合字段，如总和。我们可能决定创建功能来描述按时间间隔（例如季节）的数量，而不是一笔总和。我们需要通过预处理，甚至选择数据来向后退一步，以获取对“真实原始数据”的访问并创建此功能。</p>
<p>我们可以看到特征工程之后是建模。</p>
<p>它表明了与建模的强烈互动，提醒我们设计功能的相互作用，并根据我们的测试工具和最终性能测量的表面进行测试。</p>
<p>这也表明我们可能需要以适合所选建模算法的形式保留数据，例如将特征标准化或标准化作为最后一步。这听起来像是一个预处理步骤，它可能是，但它有助于我们在有效建模之前考虑数据需要哪些类型的最后润色。</p>
<h3 id="特征工程的迭代过程"><a href="#特征工程的迭代过程" class="headerlink" title="特征工程的迭代过程"></a>特征工程的迭代过程</h3><p>解特征工程在应用机器学习过程的上下文中的位置突出表明它不是独立的。</p>
<p>这是一个迭代过程，一次又一次地与数据选择和模型评估相互作用，直到我们的问题耗尽时间。</p>
<p>该过程可能如下所示：</p>
<ol>
<li>头脑风暴功能：真正进入问题，查看大量数据，研究其他问题的特征工程，看看你可以窃取什么。</li>
<li>设计功能：取决于您的问题，但您可以使用自动特征提取，手动功能构建和两者的混合。</li>
<li>选择要素：使用不同的要素重要性评分和要素选择方法为模型准备一个或多个“视图”以进行操作。</li>
<li>评估模型：使用所选特征估算未见数据的模型准确性。<br>您需要一个明确定义的问题，以便您知道何时停止此过程并继续尝试其他模型，其他模型配置，模型集合等。一旦你对想法或准确度增加达到稳定水平，那么后来在管道中就会有所收获。</li>
</ol>
<p>您需要一个经过深思熟虑和设计的测试工具，以客观地估计看不见的数据的模型技能。这将是您对功能工程流程的唯一衡量标准，您必须相信它不要浪费您的时间。</p>
<h2 id="特征工程的一般例子"><a href="#特征工程的一般例子" class="headerlink" title="特征工程的一般例子"></a>特征工程的一般例子</h2><p>让我们使特征工程的概念更具体。</p>
<p>在本节中，我们将考虑您可能在Excel电子表格中使用的表格数据。我们将看一些您可能想要考虑自己的问题的手动功能构建示例。</p>
<p>当我听到“ 特征工程非常重要 ”时，这就是我想到的特征工程类型。这是我熟悉和实践的最常见的形式。</p>
<p>哪个最好？你事前无法知道。您必须尝试它们并评估结果以实现您的算法和性能测量。</p>
<h3 id="分解分类属性"><a href="#分解分类属性" class="headerlink" title="分解分类属性"></a>分解分类属性</h3><p>想象一下，你有一个分类属性，比如“ Item_Color ”，可以是红色，蓝色或未知。</p>
<p>未知可能是特殊的，但对于模型，它看起来只是另一种颜色选择。更好地公开这些信息可能是有益的。</p>
<p>您可以创建一个名为“ Has_Color ” 的新二进制功能，并在项目具有颜色时为其指定值“ 1 ”，并在颜色未知时为其指定“ 0 ”。</p>
<p>更进一步，您可以为Item_Color具有的每个值创建二进制功能。这将是三个二进制属性：Is_Red，Is_Blue和Is_Unknown。</p>
<p>可以使用这些附加功能代替Item_Color功能（如果您想尝试更简单的线性模型）或者除此之外（如果您想从决策树中获得更多功能）</p>
<h3 id="分解日期时间"><a href="#分解日期时间" class="headerlink" title="分解日期时间"></a>分解日期时间</h3><p>日期时间包含很多信息，这些信息很难让模型以其原生形式利用，例如ISO 8601（即2014-09-20T20：45：40Z）。</p>
<p>如果您怀疑时间与其他属性之间存在关系，则可以将日期时间分解为可能允许模型发现和利用这些关系的组成部分。</p>
<p>例如，您可能怀疑一天中的时间与其他属性之间存在关系。</p>
<p>您可以创建一个名为Hour_of_Day的新数字功能，该功能可能有助于回归模型。</p>
<p>您可以创建一个名为Part_Of_Day的新序数功能，其中包含4个值Morning，Midday，Afternoon，Night以及您认为相关的任何小时边界。这可能对决策树有用。</p>
<p>您可以使用类似的方法来选择一周中的时间关系，一个月的时间关系以及一年中各种季节性结构。</p>
<p>日期时间结构丰富，如果您怀疑数据存在时间依赖性，请花点时间将其取出。</p>
<h3 id="重构数值量"><a href="#重构数值量" class="headerlink" title="重构数值量"></a>重构数值量</h3><p>您的数据很可能包含数量，可以重新定义以更好地暴露相关结构。这可以是转换为新单元或将速率分解为时间和数量的组件。</p>
<p>您可能拥有重量，距离或时间等数量。线性变换对于回归和其他依赖于尺度的方法可能是有用的。</p>
<p>例如，您可以使用以克为单位的Item_Weight，其值为6289.您可以创建一个新的要素，其中此数量（公斤）为6.289或舍入公斤（如6）。如果域名是运输数据，则可能千克数足够或更有用（更少噪音）Item_Weight的精度。</p>
<p>的Item_Weight可以被分成两个功能：Item_Weight_Kilograms和Item_Weight_Remainder_Grams，分别为6和289的示例值。</p>
<p>可能存在领域知识，即权重大于4的项目会产生更高的税率。该魔术域号可用于创建一个新的二进制特征Item_Above_4kg，其值为“ 1 ”，例如6289克。</p>
<p>您还可以将数量存储为间隔的费率或总数量。例如，Num_Customer_Purchases汇总了一年。</p>
<p>在这种情况下，您可能希望返回数据收集步骤并创建除此聚合之外的新功能，并尝试在购买中公开更多时间结构，例如季节性。例如，可以创建以下新的二进制功能：Purchases_Summer，Purchases_Fall，Purchases_Winter和Purchases_Spring。</p>
<h2 id="特征工程的具体例子"><a href="#特征工程的具体例子" class="headerlink" title="特征工程的具体例子"></a>特征工程的具体例子</h2><p>研究特征工程示例的一个好地方是竞争机器学习的结果。</p>
<p>比赛通常使用来自现实世界问题领域的数据。在比赛结束时需要记录方法和方法。这些文章为有效的现实世界机器学习过程和方法提供了宝贵的见解。</p>
<p>在本节中，我们将介绍几个关注特征工程的有趣且值得注意的赛后写作示例。</p>
<h3 id="预测2010年KDD杯的学生考试成绩"><a href="#预测2010年KDD杯的学生考试成绩" class="headerlink" title="预测2010年KDD杯的学生考试成绩"></a>预测2010年KDD杯的学生考试成绩</h3><p>KDD杯是每年的知识发现和数据挖掘会议中，ACM特殊利益集团的参加者举办了学习机的竞争。</p>
<p>2010年，比赛的重点是对学生的学习方式进行建模。提供了一个关于代数问题的学生语料库，用于预测学生未来的表现。</p>
<p>比赛的获胜者是国立台湾大学的一群学生和学者。他们的方法在“ 2010年KDD杯的特征工程和分类器集合 ”一文中有所描述。</p>
<p>本文将特色工程作为获胜的关键方法。特征工程以创建数百万个二进制特征为代价简化了问题的结构。简单的结构使团队能够使用高性能但非常简单的线性方法来实现获胜的预测模型。</p>
<p>本文详细介绍了问题结构中特定的时间和其他非线性如何简化为简单的复合二元指标。</p>
<p>这是简单属性分解可能实现的极端和有益的例子。</p>
<h3 id="预测遗产健康奖的患者准入"><a href="#预测遗产健康奖的患者准入" class="headerlink" title="预测遗产健康奖的患者准入"></a>预测遗产健康奖的患者准入</h3><p>该遗产卫生奖是颁发给球队谁能够最好地预测哪些患者将在未来一年内入院300万美元的奖金。</p>
<p>该奖项每年都会获得里程碑奖，其中顶级团队将获得奖项，其流程和方法将公开。</p>
<p>我记得读过三个里程碑中的第一个发布的论文，并对所涉及的特征工程的数量印象深刻。</p>
<p>具体来说，Phil Brierley，David Vogel和Randy Axelrod撰写的论文“ 第1轮里程碑奖：我们如何做到 - 团队市场创造者 ”。大多数竞赛都涉及大量的特征工程，但令我印象深刻的是这篇论文的重点。</p>
<p>本文提供了构造属性所需的属性和SQL表。</p>
<p>本文通过简单分解给出了一些很好的现实世界的特征工程实例。有很多计数，分钟，最大值，大量二进制属性和离散化的数字属性。非常简单的方法用于很好的效果。</p>
<h2 id="更多关于特征工程的资源"><a href="#更多关于特征工程的资源" class="headerlink" title="更多关于特征工程的资源"></a>更多关于特征工程的资源</h2><p>我们在本文中介绍了很多内容，并希望您对功能工程是什么，它适合哪些以及如何实现这一功能有了更多的了解。</p>
<p>这真的是你旅程的开始。您需要练习特征工程，并且需要学习优秀的特征工程从业者。</p>
<p>本节提供了一些可能对您的旅程有所帮助的资源。</p>
<h3 id="图书"><a href="#图书" class="headerlink" title="图书"></a>图书</h3><ol>
<li>Feature Extraction, Construction and Selection: A Data Mining Perspective</li>
<li>Feature Extraction: Foundations and Applications (I like this book)</li>
<li>Feature Extraction &amp; Image Processing for Computer Vision, Third Edition</li>
<li><a href="https://books.google.co.uk/books?id=Ofp4h_oXsZ4C&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;hl=zh-CN#v=onepage&amp;q&amp;f=false" target="_blank" rel="noopener">https://books.google.co.uk/books?id=Ofp4h_oXsZ4C&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;hl=zh-CN#v=onepage&amp;q&amp;f=false</a></li>
</ol>
<h3 id="论文和ppt"><a href="#论文和ppt" class="headerlink" title="论文和ppt"></a>论文和ppt</h3><ol>
<li><a href="http://www.columbia.edu/~rsb2162/FES2013/materials.html" target="_blank" rel="noopener">Feature Engineering Studio</a></li>
<li><a href="https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/" target="_blank" rel="noopener">Feature selection</a></li>
</ol>
<h1 id="七月在线特征工程"><a href="#七月在线特征工程" class="headerlink" title="七月在线特征工程"></a>七月在线特征工程</h1><p>特征=&gt;数据中抽取出来的对结果预测有用的信息<br>特征工程是使用专业背景知识和技巧处理数据,使得特征能在机器学习算法熵发挥更好的作用的过程</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><ol>
<li>更好的特征意味着更强的灵活度</li>
<li>更好的特征意味着只需用简单模型</li>
<li>更好的特征意味着更好的结果</li>
</ol>
<h2 id="数据与特征处理"><a href="#数据与特征处理" class="headerlink" title="数据与特征处理"></a>数据与特征处理</h2><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><ol>
<li>哪些数据对最后的结果预测有帮助？</li>
<li>数据我们能够采集到么？</li>
<li>线上实时计算的时候获取是否快捷?</li>
</ol>
<p>举例:现在要预测用户对商品的下单情况,或要给用户做商品推荐,那我需要采集什么信息?(这往往是对工业界来说的,如果是比赛的话,主办方会将采集好的数据提供给你)</p>
<h3 id="数据格式化"><a href="#数据格式化" class="headerlink" title="数据格式化"></a>数据格式化</h3><ol>
<li>数据格式化:<br>a)时间可以用年月日or 时间戳 or 第几天<br>b)单个动作记录 or一天行为聚合<br>c)….</li>
<li>大多数情况下,需要关联非常非常多的hive表和hdfs文件<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3></li>
<li>garbage in, garbage out</li>
<li>算法大多数时候就是一个加工机器,至于最后的产品(成品)如何,取决于原材料的好坏</li>
<li>实际这个过程会花掉一大部分时间,而且它会使得你对于业务的理解非常透彻</li>
<li>数据清洗做的事情=&gt;去掉脏数据</li>
</ol>
<h3 id="数据采样"><a href="#数据采样" class="headerlink" title="数据采样"></a>数据采样</h3><ol>
<li>很多情况下,正负样本是不均衡的<br>a)电商的用户点/买过的商品和没有行为的商品<br>b)某些疾病的患者与健康人<br>c)…</li>
<li>多数模型对正负样本比是敏感的(比如LR)</li>
<li>随机采样和分层抽样</li>
</ol>
<h3 id="正负样本不平衡处理办法"><a href="#正负样本不平衡处理办法" class="headerlink" title="正负样本不平衡处理办法"></a>正负样本不平衡处理办法</h3><ol>
<li>正样本&gt;&gt;负样本,且量都挺大=&gt;downsampling</li>
<li>正样本&gt;&gt;负样本,量不大=&gt;<br>a)采集更多的数据<br>b)上采样/oversamling<br>c)修改损失函数<h2 id="特征处理中不同类型的特征的处理"><a href="#特征处理中不同类型的特征的处理" class="headerlink" title="特征处理中不同类型的特征的处理"></a>特征处理中不同类型的特征的处理</h2><h3 id="数值型"><a href="#数值型" class="headerlink" title="数值型"></a>数值型</h3></li>
<li>幅度调整/归一化</li>
<li>统计值max,min,mean,std</li>
<li>离散化</li>
<li>Hash分桶</li>
<li>每个类别下对应的变量统计值histogram(分布状况)</li>
<li>试试 数值型=&gt;类别型<h3 id="类别型"><a href="#类别型" class="headerlink" title="类别型"></a>类别型</h3></li>
<li>Onehot编码(哑变量)</li>
<li>Hash与聚类处理</li>
<li>Hash与聚类处理</li>
<li>小技巧:统计每个类别变量下各个target比例,转成数值型<h3 id="类别型特征Python处理-hash技巧"><a href="#类别型特征Python处理-hash技巧" class="headerlink" title="类别型特征Python处理:hash技巧"></a>类别型特征Python处理:hash技巧</h3><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic64.png" alt="pic64"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic65.png" alt="pic65"><h3 id="日期型"><a href="#日期型" class="headerlink" title="日期型"></a>日期型</h3>既可以看作连续值,也可以看作离散型</li>
<li>连续值:<br>a)持续时间(单页浏览时长)<br>b)间隔时间(上次购买/点击离现在的时间)</li>
<li>离散值<br>a)一天中哪个时间段(hour_0-23)<br>b)一周中星期几(week_monday…)<br>c)一年中哪个星期<br>d)一年中哪个季度<br>e)工作日/周末<h3 id="文本型"><a href="#文本型" class="headerlink" title="文本型"></a>文本型</h3><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic66.png" alt="pic66"><h3 id="统计型"><a href="#统计型" class="headerlink" title="统计型"></a>统计型</h3>历届的Kaggle/天池比赛,天猫/京东排序和推荐业务线里模型用到的特征</li>
<li>加减平均:商品价格高于平均价格多少,用户在某个品类下消费超过平均用户多少,用户连续登陆天数超过平均多少</li>
<li>分位线:商品属于售出商品价格得到多少分位线处</li>
<li>次序型:排在第几位</li>
<li>比例类:电商中,好/中/差评比例,你已超过全国百分之…同学</li>
</ol>
<p><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic67.png" alt="pic67"><br><img src="/2019/05/24/数据挖掘比赛入坑（二）/pic68.png" alt="pic68"></p>
<h3 id="组合特征"><a href="#组合特征" class="headerlink" title="组合特征"></a>组合特征</h3><ol>
<li>简单组合特征:拼接型</li>
<li>模型特征组合<br>a)用GBDT产出特征组合路径<br>b)组合特征和原始特征一起放进LR训练<br>c)最早Facebook使用的方式,多家互联网公司在用</li>
</ol>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="原因"><a href="#原因" class="headerlink" title="原因:"></a>原因:</h3><ol>
<li>冗余:部分特征的相关度太高了,消耗计算性能</li>
<li>噪声:部分特征是对预测结果有负影响<h3 id="特征选择VS降维"><a href="#特征选择VS降维" class="headerlink" title="特征选择VS降维"></a>特征选择VS降维</h3></li>
<li>前者只踢掉原本特征里和结果预测关系不大的,后者做特征的计算组合构成新特征</li>
<li>SVD或者PCA缺失也能解决一定的高纬度问题</li>
</ol>
<h3 id="常见特征选择方式之一-过滤型"><a href="#常见特征选择方式之一-过滤型" class="headerlink" title="常见特征选择方式之一:过滤型"></a>常见特征选择方式之一:过滤型</h3><ol>
<li>评估单个特征和结果值之间的相关程度,排序留下Top相关的特征部分</li>
<li>Pearson相关系数,互信息,距离相关度</li>
<li>缺点:没有考虑到特征之间的关联作用,可能把有用的关联特征误踢掉</li>
<li>过滤型方式简单粗暴<h3 id="常见特征选择方式之包裹型"><a href="#常见特征选择方式之包裹型" class="headerlink" title="常见特征选择方式之包裹型"></a>常见特征选择方式之包裹型</h3></li>
<li>把特征选择看作一个特征子集搜索问题,筛选各种特征子集,用模型评估效果</li>
<li>典型的包裹型算法为”递归特征删除算法”(Recursive feature elimination algorithm)</li>
<li>比如用逻辑回归,怎么做这个事情呢?<br>a)全量特征跑一个模型<br>b)根据线性模型的系数(体现相关性),删掉5-10%的弱特征,观察准确率/auc的变化<br>c)逐步进行,直至准确率/auc出现大的下滑停止</li>
</ol>
<h3 id="常见特征选择方式之嵌入型"><a href="#常见特征选择方式之嵌入型" class="headerlink" title="常见特征选择方式之嵌入型"></a>常见特征选择方式之嵌入型</h3><ol>
<li>根据模型来分析特征的重要性(有别于上面的方式,是从生产的模型权重等)</li>
<li>最常见的方式为正则化方式来做特征选择</li>
<li>举个例子,最早在电商用LR做CTR预估,在3-5亿维的系数特征上用L1正则化的LR模型.剩余2-3千万的feature,意味着其他的feature重要度不够。</li>
</ol>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li><a href="https://www.slideshare.net/HJvanVeen/feature-engineering-72376750(干货" target="_blank" rel="noopener">https://www.slideshare.net/HJvanVeen/feature-engineering-72376750(干货</a>)</li>
<li><a href="http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf" target="_blank" rel="noopener">http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf</a></li>
<li><a href="http://kti.tugraz.at/staff/denis/courses/kddm1/featureengineering.pdf" target="_blank" rel="noopener">http://kti.tugraz.at/staff/denis/courses/kddm1/featureengineering.pdf</a></li>
<li><a href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/" target="_blank" rel="noopener">https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/</a></li>
</ol>

      
    </div>
    
    
    

	<div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      
    </div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/数据挖掘/" rel="tag"># 数据挖掘</a>
          
            <a href="/tags/Kaggle比赛/" rel="tag"># Kaggle比赛</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/05/23/数据挖掘比赛入坑（一）/" rel="next" title="数据挖掘比赛入坑（一）">
                <i class="fa fa-chevron-left"></i> 数据挖掘比赛入坑（一）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/05/25/最大熵、逻辑回归的关系/" rel="prev" title="最大熵、逻辑回归的关系">
                最大熵、逻辑回归的关系 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div id="gitalk-container"></div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/xuanyi.jpg" alt="Yif Du">
            
              <p class="site-author-name" itemprop="name">Yif Du</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">172</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">135</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yifdu" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="17210240004@fudan.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Feature-Engineering"><span class="nav-number">1.</span> <span class="nav-text">Feature Engineering</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#分类特征"><span class="nav-number">1.1.</span> <span class="nav-text">分类特征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方法一：Onehot-encoding"><span class="nav-number">1.1.1.</span> <span class="nav-text">方法一：Onehot encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法二：Hash-encoding"><span class="nav-number">1.1.2.</span> <span class="nav-text">方法二：Hash encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法三-Label-encoding"><span class="nav-number">1.1.3.</span> <span class="nav-text">方法三:Label encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法四-Count-encoding"><span class="nav-number">1.1.4.</span> <span class="nav-text">方法四:Count encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法五-Target-encoding"><span class="nav-number">1.1.5.</span> <span class="nav-text">方法五:Target encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法六-Catgory-embedding"><span class="nav-number">1.1.6.</span> <span class="nav-text">方法六:Catgory embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NaN编码"><span class="nav-number">1.1.7.</span> <span class="nav-text">NaN编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Polynomial-encoding"><span class="nav-number">1.1.8.</span> <span class="nav-text">Polynomial encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Expansion-encoding"><span class="nav-number">1.1.9.</span> <span class="nav-text">Expansion encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Consolidation-encoding"><span class="nav-number">1.1.10.</span> <span class="nav-text">Consolidation encoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数值特征"><span class="nav-number">1.2.</span> <span class="nav-text">数值特征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方法一-Rounding"><span class="nav-number">1.2.1.</span> <span class="nav-text">方法一:Rounding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法二-Binning"><span class="nav-number">1.2.2.</span> <span class="nav-text">方法二:Binning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法三-Scaling"><span class="nav-number">1.2.3.</span> <span class="nav-text">方法三:Scaling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法四-Imputation"><span class="nav-number">1.2.4.</span> <span class="nav-text">方法四:Imputation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法五-Interactions"><span class="nav-number">1.2.5.</span> <span class="nav-text">方法五:Interactions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法六-线性算法的非线性编码"><span class="nav-number">1.2.6.</span> <span class="nav-text">方法六:线性算法的非线性编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法七-Row-statistics"><span class="nav-number">1.2.7.</span> <span class="nav-text">方法七:Row statistics</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#时间变量"><span class="nav-number">1.3.</span> <span class="nav-text">时间变量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方法一-Projecting-to-a-circle"><span class="nav-number">1.3.1.</span> <span class="nav-text">方法一:Projecting to a circle</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法二-trendlines"><span class="nav-number">1.3.2.</span> <span class="nav-text">方法二:trendlines</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法三-Closeness-to-major-events"><span class="nav-number">1.3.3.</span> <span class="nav-text">方法三:Closeness to major events</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#空间变量"><span class="nav-number">1.4.</span> <span class="nav-text">空间变量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方法一-Categorizing-location"><span class="nav-number">1.4.1.</span> <span class="nav-text">方法一:Categorizing location</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法二-Closeness-to-hubs"><span class="nav-number">1.4.2.</span> <span class="nav-text">方法二:Closeness to hubs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法三-Spatial-fraudulent-behavior"><span class="nav-number">1.4.3.</span> <span class="nav-text">方法三:Spatial fraudulent behavior</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Exploration"><span class="nav-number">1.5.</span> <span class="nav-text">Exploration</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方法一-Iteration-Debugging"><span class="nav-number">1.5.1.</span> <span class="nav-text">方法一:Iteration/Debugging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法二-Label-Engineering"><span class="nav-number">1.5.2.</span> <span class="nav-text">方法二:Label Engineering</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Natural-Language-Processing"><span class="nav-number">1.6.</span> <span class="nav-text">Natural Language Processing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方法一-清洗-cleaning"><span class="nav-number">1.6.1.</span> <span class="nav-text">方法一:清洗(cleaning)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法二-Tokenizing"><span class="nav-number">1.6.2.</span> <span class="nav-text">方法二:Tokenizing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法三-Removing"><span class="nav-number">1.6.3.</span> <span class="nav-text">方法三:Removing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法四-Roots"><span class="nav-number">1.6.4.</span> <span class="nav-text">方法四:Roots</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法五-Enrich"><span class="nav-number">1.6.5.</span> <span class="nav-text">方法五:Enrich</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法六-Similarities"><span class="nav-number">1.6.6.</span> <span class="nav-text">方法六:Similarities</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法七-TF-IDF"><span class="nav-number">1.6.7.</span> <span class="nav-text">方法七:TF-IDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法八-降维"><span class="nav-number">1.6.8.</span> <span class="nav-text">方法八:降维</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法九-外部模型"><span class="nav-number">1.6.9.</span> <span class="nav-text">方法九:外部模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Networks-amp-Deep-Learning"><span class="nav-number">1.7.</span> <span class="nav-text">Neural Networks&amp;Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Leakage-Golden-Features"><span class="nav-number">1.7.1.</span> <span class="nav-text">Leakage/Golden Features</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#COS424-Feature-engineering"><span class="nav-number">2.</span> <span class="nav-text">COS424 Feature engineering</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-importance-of-features"><span class="nav-number">2.1.</span> <span class="nav-text">The importance of features</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简单的线性模型"><span class="nav-number">2.1.1.</span> <span class="nav-text">简单的线性模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Explainable-models"><span class="nav-number">2.1.2.</span> <span class="nav-text">Explainable models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-explosion"><span class="nav-number">2.1.3.</span> <span class="nav-text">Feature explosion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-relevance"><span class="nav-number">2.2.</span> <span class="nav-text">Feature relevance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Probabilistic-feature-relevance"><span class="nav-number">2.2.1.</span> <span class="nav-text">Probabilistic feature relevance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Interesting-example"><span class="nav-number">2.2.2.</span> <span class="nav-text">Interesting example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Selecting-features"><span class="nav-number">2.3.</span> <span class="nav-text">Selecting features</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Selecting-features-from-data"><span class="nav-number">2.3.1.</span> <span class="nav-text">Selecting features from data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wrapper-approaches"><span class="nav-number">2.3.2.</span> <span class="nav-text">Wrapper approaches</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-methods"><span class="nav-number">2.3.3.</span> <span class="nav-text">Greedy methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-features"><span class="nav-number">2.4.</span> <span class="nav-text">Learning features</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-learning-in-one-slide"><span class="nav-number">2.4.1.</span> <span class="nav-text">Feature learning in one slide</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-learning-and-multilayer-models"><span class="nav-number">2.4.2.</span> <span class="nav-text">Feature learning and multilayer models</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Feature-Engineering-Knowledge-Discovery-and-Data-Mining"><span class="nav-number">3.</span> <span class="nav-text">Feature Engineering: Knowledge Discovery and Data Mining</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#信息论"><span class="nav-number">3.1.</span> <span class="nav-text">信息论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-are-features-amp-feature-engineering"><span class="nav-number">3.2.</span> <span class="nav-text">What are features &amp; feature engineering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Value-Processing"><span class="nav-number">3.3.</span> <span class="nav-text">Feature Value Processing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Selection"><span class="nav-number">3.4.</span> <span class="nav-text">Feature Selection</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#发现特征工程，如何设计特征以及如何获得它"><span class="nav-number">4.</span> <span class="nav-text">发现特征工程，如何设计特征以及如何获得它</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程解决的问题"><span class="nav-number">4.1.</span> <span class="nav-text">特征工程解决的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程的重要性"><span class="nav-number">4.2.</span> <span class="nav-text">特征工程的重要性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是特征工程"><span class="nav-number">4.3.</span> <span class="nav-text">什么是特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#特征工程是一个表示问题"><span class="nav-number">4.3.1.</span> <span class="nav-text">特征工程是一个表示问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征工程是一门艺术"><span class="nav-number">4.3.2.</span> <span class="nav-text">特征工程是一门艺术</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程的子问题"><span class="nav-number">4.4.</span> <span class="nav-text">特征工程的子问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#功能：对建模任务有用的属性"><span class="nav-number">4.4.1.</span> <span class="nav-text">功能：对建模任务有用的属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征重要性：对特征有用性的估计"><span class="nav-number">4.4.2.</span> <span class="nav-text">特征重要性：对特征有用性的估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征提取-从原始数据自动构建新特征"><span class="nav-number">4.4.3.</span> <span class="nav-text">特征提取:从原始数据自动构建新特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#功能选择：从许多功能到一些有用的功能"><span class="nav-number">4.4.4.</span> <span class="nav-text">功能选择：从许多功能到一些有用的功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征构造：从原始数据手动构建新特征"><span class="nav-number">4.4.5.</span> <span class="nav-text">特征构造：从原始数据手动构建新特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征学习：自动识别和使用原始数据中的特征"><span class="nav-number">4.4.6.</span> <span class="nav-text">特征学习：自动识别和使用原始数据中的特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程过程"><span class="nav-number">4.5.</span> <span class="nav-text">特征工程过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习的过程"><span class="nav-number">4.5.1.</span> <span class="nav-text">机器学习的过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征工程的迭代过程"><span class="nav-number">4.5.2.</span> <span class="nav-text">特征工程的迭代过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程的一般例子"><span class="nav-number">4.6.</span> <span class="nav-text">特征工程的一般例子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分解分类属性"><span class="nav-number">4.6.1.</span> <span class="nav-text">分解分类属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分解日期时间"><span class="nav-number">4.6.2.</span> <span class="nav-text">分解日期时间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#重构数值量"><span class="nav-number">4.6.3.</span> <span class="nav-text">重构数值量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程的具体例子"><span class="nav-number">4.7.</span> <span class="nav-text">特征工程的具体例子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#预测2010年KDD杯的学生考试成绩"><span class="nav-number">4.7.1.</span> <span class="nav-text">预测2010年KDD杯的学生考试成绩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预测遗产健康奖的患者准入"><span class="nav-number">4.7.2.</span> <span class="nav-text">预测遗产健康奖的患者准入</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#更多关于特征工程的资源"><span class="nav-number">4.8.</span> <span class="nav-text">更多关于特征工程的资源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#图书"><span class="nav-number">4.8.1.</span> <span class="nav-text">图书</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#论文和ppt"><span class="nav-number">4.8.2.</span> <span class="nav-text">论文和ppt</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#七月在线特征工程"><span class="nav-number">5.</span> <span class="nav-text">七月在线特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#意义"><span class="nav-number">5.1.</span> <span class="nav-text">意义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据与特征处理"><span class="nav-number">5.2.</span> <span class="nav-text">数据与特征处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据采集"><span class="nav-number">5.2.1.</span> <span class="nav-text">数据采集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据格式化"><span class="nav-number">5.2.2.</span> <span class="nav-text">数据格式化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据清洗"><span class="nav-number">5.2.3.</span> <span class="nav-text">数据清洗</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据采样"><span class="nav-number">5.2.4.</span> <span class="nav-text">数据采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正负样本不平衡处理办法"><span class="nav-number">5.2.5.</span> <span class="nav-text">正负样本不平衡处理办法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征处理中不同类型的特征的处理"><span class="nav-number">5.3.</span> <span class="nav-text">特征处理中不同类型的特征的处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数值型"><span class="nav-number">5.3.1.</span> <span class="nav-text">数值型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#类别型"><span class="nav-number">5.3.2.</span> <span class="nav-text">类别型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#类别型特征Python处理-hash技巧"><span class="nav-number">5.3.3.</span> <span class="nav-text">类别型特征Python处理:hash技巧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#日期型"><span class="nav-number">5.3.4.</span> <span class="nav-text">日期型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文本型"><span class="nav-number">5.3.5.</span> <span class="nav-text">文本型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#统计型"><span class="nav-number">5.3.6.</span> <span class="nav-text">统计型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#组合特征"><span class="nav-number">5.3.7.</span> <span class="nav-text">组合特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征选择"><span class="nav-number">5.4.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原因"><span class="nav-number">5.4.1.</span> <span class="nav-text">原因:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征选择VS降维"><span class="nav-number">5.4.2.</span> <span class="nav-text">特征选择VS降维</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见特征选择方式之一-过滤型"><span class="nav-number">5.4.3.</span> <span class="nav-text">常见特征选择方式之一:过滤型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见特征选择方式之包裹型"><span class="nav-number">5.4.4.</span> <span class="nav-text">常见特征选择方式之包裹型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见特征选择方式之嵌入型"><span class="nav-number">5.4.5.</span> <span class="nav-text">常见特征选择方式之嵌入型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            

			
          </div>
        </section>
      <!--/noindex-->
      

      
	 

    </div>
		  
	  
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="heart">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yif Du</span>

  
</div>





        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script src="/js/src/md5.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '7428ad62daef314bef06',
          clientSecret: '93cd3f4cd41cfc00c4760f65f8d895a66088ea5a',
          repo: 'Comments',
          owner: 'yifdu',
          admin: ['yifdu'],
          id: md5(location.pathname),
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>


  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  <style>
#selectionCopyright {
    position: absolute;
    display: none;
    background: rgba(244,67,54,.7);
    color: #fff;
    border-radius: 6px;
    box-shadow: none;
    border: none;
    font-size: 14px;
}
#selectionCopyright a{
    color:#fff;
    border-color: #fff;
}
#selectionCopyright::before {
    content: "";
    width: 0;
    height: 0;
    border-style: solid;
    border-width: 6px 8px 6px 0;
    border-color: transparent rgba(244,67,54,.7) transparent transparent;
    position: absolute;
    left: -8px;
    top:50%;
    transform:translateY(-50%);
}
</style>

<button id="selectionCopyright" disabled="disabled">本文发表于[<a href="http://yifdu.github.io">yifdu.github.io</a>]分享请注明来源！</button>

<script>
window.onload = function() {
    function selectText() {
        if (document.selection) { //IE浏览器下
            return document.selection.createRange().text; //返回选中的文字
        } else { //非IE浏览器下
            return window.getSelection().toString(); //返回选中的文字
        }
    }
    var content = document.getElementsByTagName("body")[0];
    var scTip = document.getElementById('selectionCopyright');

    content.onmouseup = function(ev) { //设定一个onmouseup事件
        var ev = ev || window.event;
        var left = ev.clientX;//获取鼠标相对浏览器可视区域左上角水平距离距离
        var top = ev.clientY;//获取鼠标相对浏览器可视区域左上角垂直距离距离
        var xScroll = Math.max(document.body.scrollLeft, document.documentElement.scrollLeft);//获取文档水平滚动距离
        var yScroll = Math.max(document.body.scrollTop, document.documentElement.scrollTop);//获取文档垂直滚动距离
        if (selectText().length > 0) {
            setTimeout(function() { //设定一个定时器
                scTip.style.display = 'inline-block';
                scTip.style.left = left + xScroll + 15 + 'px';//鼠标当前x值
                scTip.style.top = top + yScroll - 15 + 'px';//鼠标当前y值
            }, 100);
        } else {
            scTip.style.display = 'none';
        }
    };

    content.onclick = function(ev) {
        var ev = ev || window.event;
        ev.cancelBubble = true;
    };
    document.onclick = function() {
        scTip.style.display = 'none';
    };
};
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"live2d-widget-model-miku"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
